{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BRaP_x0eoI1"
   },
   "source": [
    "# NLP. Tarea 5: Modelo del Lenguaje Neuronal.\n",
    "\n",
    "**Diego Moreno**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Modelo Neuronal a nivel de caracter.\n",
    "\n",
    "Importamos librer칤as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5WWvA-6sets1"
   },
   "outputs": [],
   "source": [
    "# Tools\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from typing import Tuple\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "# Preprocesing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import  TweetTokenizer\n",
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Pytorch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Scikitlearn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bqN6c3B-f7_h"
   },
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Yb4ys6nJgPYJ"
   },
   "outputs": [],
   "source": [
    "pth = ''\n",
    "X_train = pd.read_csv(pth+'mex_train.txt', sep='\\r\\n',  engine='python', header=None).loc[:,0].values.tolist()\n",
    "X_val = pd.read_csv(pth+'mex_val.txt', sep='\\r\\n',  engine='python', header=None).loc[:,0].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nivel de caracter, tenemos que fijarnos en una ventana de 6 o m치s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Jj3gCUEXhlGu"
   },
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.N = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase de N-gramas se quedar치 igual pues la estrategia ser치 solamente cambiar el tokenizador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2T9J65oaiILS"
   },
   "outputs": [],
   "source": [
    "class NgramData():\n",
    "    def __init__(self, N: int, vocab_max: int=5000, tokenizer=None, embedding_model=None):\n",
    "        self.tokenizer = tokenizer if tokenizer else self.default_tokenizer\n",
    "        self.punct = set(['.',',',';',':','-','^','춺','췉','\"','!','춰','?','','\\'','...','<url>','*','@usuario'])\n",
    "        self.N = N\n",
    "        self.vocab_max = vocab_max\n",
    "        self.UNK = '<unk>'\n",
    "        self.SOS = '<s>'\n",
    "        self.EOS = '</s>'\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "    def default_tokenizer(self, doc: str) -> list:\n",
    "        return doc.split(' ')\n",
    "\n",
    "    def get_vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def remove_word(self, word: str) -> bool:\n",
    "        word = word.lower()\n",
    "        is_punct = True if word in self.punct else False\n",
    "        is_digit = word.isnumeric()\n",
    "        return is_punct or is_digit\n",
    "\n",
    "    def get_vocab(self, corpus: list) -> set:\n",
    "        freq_dist = FreqDist([w.lower() for sent in corpus \\\n",
    "                              for w in self.tokenizer(sent) \\\n",
    "                              if not self.remove_word(w)])\n",
    "        sorted_words = self.sortFreqDict(freq_dist)[:self.vocab_max-3]\n",
    "        return set(sorted_words)\n",
    "\n",
    "    def sortFreqDict(self, freq_dist) -> list:\n",
    "        freq_dist = dict(freq_dist)\n",
    "        return sorted(freq_dist, key=freq_dist.get, reverse=True)\n",
    "\n",
    "    def fit(self, corpus: list) -> None:\n",
    "        self.vocab = self.get_vocab(corpus)\n",
    "        self.vocab.add(self.UNK)\n",
    "        self.vocab.add(self.SOS)\n",
    "        self.vocab.add(self.EOS)\n",
    "\n",
    "        self.w2id = {}\n",
    "        self.id2w = {}\n",
    "\n",
    "        if self.embedding_model is not None:\n",
    "            self.embedding_matrix = np.empty([len(self.vocab), self.embedding_model.vector_size])\n",
    "\n",
    "        ID = 0\n",
    "        for doc in corpus:\n",
    "            for word in self.tokenizer(doc):\n",
    "                word_ = word.lower()\n",
    "                if word_ in self.vocab and not word_ in self.w2id:\n",
    "                    self.w2id[word_] = ID\n",
    "                    self.id2w[ID] = word_\n",
    "                    if self.embedding_model is not None:\n",
    "                        if word_ in self.embedding_model:\n",
    "                            self.embedding_matrix[ID] = self.embedding_model[word_]\n",
    "                        else:\n",
    "                            self.embedding_matrix[ID] = np.random.rand(self.embedding_model.vector_size) \n",
    "                    ID += 1\n",
    "        #Special tokens  \n",
    "        self.w2id.update({self.UNK: ID, \n",
    "                          self.SOS: ID+1,\n",
    "                          self.EOS: ID+2})  \n",
    "        self.id2w.update({ID  : self.UNK, \n",
    "                          ID+1: self.SOS,\n",
    "                          ID+2: self.EOS})\n",
    "    \n",
    "    def replace_unk(self, doc_tokens: list) -> list: \n",
    "        for i, token in enumerate(doc_tokens):\n",
    "            if token.lower() not in self.vocab:\n",
    "                doc_tokens[i] = self.UNK\n",
    "        return doc_tokens\n",
    "\n",
    "\n",
    "    def get_ngram_doc(self, doc:str) -> list:\n",
    "        doc_tokens = self.tokenizer(doc)\n",
    "        doc_tokens = self.replace_unk(doc_tokens)\n",
    "        doc_tokens = [w.lower() for w in doc_tokens]\n",
    "        doc_tokens = [self.SOS]*(self.N - 1) + doc_tokens + [self.EOS]\n",
    "        return list(ngrams(doc_tokens, self.N))\n",
    "    \n",
    "    def transform(self, corpus: list) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        X_ngrams = []\n",
    "        y = []\n",
    "        for doc in corpus:\n",
    "            doc_ngram = self.get_ngram_doc(doc)\n",
    "            for words_window in doc_ngram:\n",
    "                words_window_ids = [self.w2id[w] for w in words_window]\n",
    "                X_ngrams.append(list(words_window_ids[:-1]))\n",
    "                y.append(words_window_ids[-1])\n",
    "        return np.array(X_ngrams), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el nuevo tokenizador a nivel de caracter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "igtUbF-UtA5e"
   },
   "outputs": [],
   "source": [
    "def CharTokenizer(doc: str) -> list:\n",
    "    l = []\n",
    "    for c in doc:\n",
    "        l.append(c)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos nivel de caracteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_level = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ukx1t0n8h12F"
   },
   "outputs": [],
   "source": [
    "if char_level:\n",
    "    tk = CharTokenizer\n",
    "else:\n",
    "    tk = TweetTokenizer()\n",
    "    tk = tk.tokenize\n",
    "    \n",
    "ngram_data = NgramData(args.N, 5000, tk)\n",
    "ngram_data.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OH2bHAWHqJ7w",
    "outputId": "1bd5b974-1b17-494c-8e8f-346fbc98c220"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 344\n"
     ]
    }
   ],
   "source": [
    "print('Vocab Size:', ngram_data.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos los datos transformados y los loader de los mismos para entrenamiento y validaci칩n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Hl5NY_wFqZPt"
   },
   "outputs": [],
   "source": [
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2PH5JSUbwX6O"
   },
   "outputs": [],
   "source": [
    "#Batch size\n",
    "args.batch_size = 64\n",
    "#Number workers\n",
    "args.num_workers = 2\n",
    "\n",
    "#Train\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = True)\n",
    "\n",
    "#Validation\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CxrtxytHyIAR",
    "outputId": "baebf0d3-f72b-4e23-e202-0fb96ad0d2ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([64, 5])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print('X shape:', batch[0].shape)\n",
    "print('y shape:', batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "l_diObstybV6"
   },
   "outputs": [],
   "source": [
    "#[[ngram_data.id2w[w] for w in tw] for tw in batch[0].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase del modelo neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "hXQa9kP8VMFU"
   },
   "outputs": [],
   "source": [
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(NeuralLM, self).__init__()\n",
    "\n",
    "        self.window_size = args.N - 1\n",
    "        self.embedding_size = args.d\n",
    "\n",
    "        self.emb = nn.Embedding(args.vocab_size, args.d)\n",
    "        self.fc1 = nn.Linear(args.d * (args.N - 1), args.d_h)\n",
    "        self.drop1 = nn.Dropout(p = args.dropout)\n",
    "        self.fc2 = nn.Linear(args.d_h, args.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1, self.window_size * self.embedding_size)\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = self.drop1(h)\n",
    "        x = self.fc2(h)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para el entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0vZZwQA1XZ8_"
   },
   "outputs": [],
   "source": [
    "def get_preds(raw_logits):\n",
    "    probs = F.softmax(raw_logits.detach(), dim=1)\n",
    "    y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    return y_pred\n",
    "\n",
    "def model_eval(data, model, gpu=False):\n",
    "    with torch.no_grad():\n",
    "        preds, tgts = [], []\n",
    "        for window_words, labels in data:\n",
    "            if gpu:\n",
    "                window_words = window_words.cuda()\n",
    "            outputs = model(window_words)\n",
    "\n",
    "            #Predictions\n",
    "            y_pred = get_preds(outputs)\n",
    "            tgt = labels.numpy()\n",
    "            tgts.append(tgt)\n",
    "            preds.append(y_pred)\n",
    "    tgts = [e for l in tgts for e in l]\n",
    "    preds = [e for l in preds for e in l]\n",
    "    return accuracy_score(tgts, preds)\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint_path, filename='checkpoint.pt'):\n",
    "    filename = os.path.join(checkpoint_path, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, os.path.join(checkpoint_path, 'model_best.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos hiperpar치metros que tendr치n que ser modificados posteriormente al usar un nuevo embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Dnk0A8MmXzRr"
   },
   "outputs": [],
   "source": [
    "#Model hyperparameters\n",
    "#Vocabulary size\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "#Word embeddings dimension\n",
    "args.d = 100\n",
    "#Hidden layer dimension\n",
    "args.d_h = 200\n",
    "#Dropout\n",
    "args.dropout = 0.1\n",
    "\n",
    "#Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "#Scheduler hyperparameters\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "#Saving hyperparameters\n",
    "args.savedir = pth + 'model'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "#Create model\n",
    "model = NeuralLM(args=args)\n",
    "\n",
    "#Send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "#Loss, Optimizer and Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer, \n",
    "                                                       mode = 'min',\n",
    "                                                       factor = args.lr_factor,\n",
    "                                                       patience = args.lr_patience,\n",
    "                                                       verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa de entrenamiento del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 902
    },
    "id": "m5w33lulaO27",
    "outputId": "86e4a066-cdcd-4533-caad-4a110bf51600"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.4386065422287932\n",
      "Epoch [1/100]: Loss = 1.9773, Val Acurracy = 0.4386, Epoch time = 31.76\n",
      "Train accuracy:  0.46004435409351324\n",
      "Epoch [2/100]: Loss = 1.8176, Val Acurracy = 0.4600, Epoch time = 30.50\n",
      "Train accuracy:  0.4607835889854001\n",
      "Epoch [3/100]: Loss = 1.7673, Val Acurracy = 0.4608, Epoch time = 30.55\n",
      "Train accuracy:  0.4825540565514692\n",
      "Epoch [4/100]: Loss = 1.7373, Val Acurracy = 0.4826, Epoch time = 30.89\n",
      "Train accuracy:  0.4729624838292367\n",
      "Epoch [5/100]: Loss = 1.7156, Val Acurracy = 0.4730, Epoch time = 31.02\n",
      "Train accuracy:  0.4506006283496581\n",
      "Epoch [6/100]: Loss = 1.7002, Val Acurracy = 0.4506, Epoch time = 30.81\n",
      "Train accuracy:  0.47444095361301053\n",
      "Epoch [7/100]: Loss = 1.6873, Val Acurracy = 0.4744, Epoch time = 30.63\n",
      "Train accuracy:  0.4743300683792275\n",
      "Epoch [8/100]: Loss = 1.6764, Val Acurracy = 0.4743, Epoch time = 30.30\n",
      "Train accuracy:  0.4886712252818333\n",
      "Epoch [9/100]: Loss = 1.6685, Val Acurracy = 0.4887, Epoch time = 32.59\n",
      "Train accuracy:  0.4830900018480872\n",
      "Epoch [10/100]: Loss = 1.6614, Val Acurracy = 0.4831, Epoch time = 33.33\n",
      "Train accuracy:  0.48443910552578084\n",
      "Epoch [11/100]: Loss = 1.6551, Val Acurracy = 0.4844, Epoch time = 32.53\n",
      "Epoch    12: reducing learning rate of group 0 to 1.1500e-01.\n",
      "Train accuracy:  0.49240436148586214\n",
      "Epoch [12/100]: Loss = 1.6499, Val Acurracy = 0.4924, Epoch time = 32.74\n",
      "Train accuracy:  0.5046386989465903\n",
      "Epoch [13/100]: Loss = 1.6107, Val Acurracy = 0.5046, Epoch time = 32.69\n",
      "Train accuracy:  0.5072445019404916\n",
      "Epoch [14/100]: Loss = 1.6055, Val Acurracy = 0.5072, Epoch time = 33.59\n",
      "Train accuracy:  0.5064683053040103\n",
      "Epoch [15/100]: Loss = 1.6023, Val Acurracy = 0.5065, Epoch time = 33.00\n",
      "Train accuracy:  0.5022916281648494\n",
      "Epoch [16/100]: Loss = 1.6014, Val Acurracy = 0.5023, Epoch time = 31.56\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-e7244224207f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtraining_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "#Training\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model.train()\n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        #Forward pass\n",
    "        outputs = model(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "\n",
    "        #Get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "\n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #Metrics in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Metrics in validation dataset\n",
    "    model.eval()\n",
    "    tuning_metric = model_eval(val_loader, model, gpu=args.use_gpu)\n",
    "    mean_epoch_metric = np.mean(tuning_metric)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "\n",
    "    #Check metric improvement\n",
    "    is_improve = tuning_metric > best_metric\n",
    "    if is_improve:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "\n",
    "    #Save best model\n",
    "    save_checkpoint({'epoch'       : epoch+1,\n",
    "                     'state_dict'  : model.state_dict(),\n",
    "                     'optimizer'   : optimizer.state_dict(),\n",
    "                     'scheduler'   : scheduler.state_dict(),\n",
    "                     'best_metric' : best_metric}, is_improve, args.savedir)\n",
    "    \n",
    "    #Early stoping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print('No improvement. Breaking out of loop.')\n",
    "        break\n",
    "    print('Train accuracy: ', mean_epoch_metric)\n",
    "    print('Epoch [{}/{}]: Loss = {:.4f}, Val Acurracy = {:.4f}, Epoch time = {:.2f}'.\n",
    "          format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time()-epoch_start_time)))\n",
    "    \n",
    "print('---------- %s seconds ---------' % time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejor modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(344, 100)\n",
       "  (fc1): Linear(in_features=500, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=344, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model with learned embeddings\n",
    "best_model = NeuralLM(args)\n",
    "best_model.load_state_dict(torch.load(pth+'model/model_best.pt')['state_dict'])\n",
    "best_model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "uChqtaZNjHkP",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def print_closest_words(embeddings, ngram_data, word, n):\n",
    "    word_id = torch.LongTensor([ngram_data.w2id[word]])\n",
    "    word_embed = embeddings(word_id)\n",
    "    dists = torch.norm(embeddings.weight - word_embed, dim=1).detach()\n",
    "    lst = sorted(enumerate(dists.numpy()), key=lambda x : x[1])\n",
    "    for idx, diff in lst[1:n+1]:\n",
    "        print(ngram_data.id2w[idx], diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "r_I150SHuyVu",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "b2f2d5f8-70d5-4c4a-e966-282d64f859b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾\n",
      "a 3.8728173\n",
      "o 4.1493793\n",
      "  4.3050637\n",
      "s 4.475555\n",
      "n 4.615959\n",
      "r 4.6823664\n",
      "i 4.7688804\n",
      "l 5.04975\n",
      "t 5.1100945\n",
      "<s> 5.1198354\n"
     ]
    }
   ],
   "source": [
    "print('Learned embeddings')\n",
    "print('춾'*20)\n",
    "print_closest_words(best_model.emb, ngram_data, 'e', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Generamos texto 3 veces con m치ximo de 300 caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "gziHZsIQv0AO"
   },
   "outputs": [],
   "source": [
    "lenght_max = 300\n",
    "\n",
    "def parse_text(text, tokenizer):\n",
    "    all_tokens = [w.lower() if w in ngram_data.w2id else '<unk>' for w in tokenizer(text)]\n",
    "    token_ids = [ngram_data.w2id[word.lower()] for word in all_tokens]\n",
    "    return all_tokens, token_ids\n",
    "\n",
    "def sample_next_word(logits, temperature=1.):\n",
    "    logits = np.asarray(logits).astype('float64')\n",
    "    preds = logits/temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probs = np.random.multinomial(1, preds)\n",
    "    return np.argmax(probs)\n",
    "\n",
    "def predict_next_token(model, token_ids):\n",
    "    words_ids_tensor = torch.LongTensor(token_ids).unsqueeze(0)\n",
    "    y_raw_pred = model(words_ids_tensor).squeeze(0).detach().numpy()\n",
    "    y_pred = sample_next_word(y_raw_pred, 1.)\n",
    "    return y_pred\n",
    "\n",
    "def generate_sentence(model, initial_text, tokenizer):\n",
    "    all_tokens, window_word_ids = parse_text(initial_text, tokenizer)\n",
    "    for i in range(lenght_max):\n",
    "        y_pred = predict_next_token(best_model, window_word_ids)\n",
    "        next_word = ngram_data.id2w[y_pred]\n",
    "        all_tokens.append(next_word)\n",
    "        if next_word == '</s>':\n",
    "            break\n",
    "        else:\n",
    "            window_word_ids.pop(0)\n",
    "            window_word_ids.append(y_pred)\n",
    "    if char_level:\n",
    "        return ''.join(all_tokens)\n",
    "    else:\n",
    "        return ' '.join(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "UbXJ642z--mB",
    "outputId": "8a33b6a5-6361-4948-dbcd-a528e9ca0708"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s><s putos perder a oierencanas anuncia un putos<unk> 游땕游낕游둣游낕</s>'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = '<s><s'\n",
    "print('Learned embeddings')\n",
    "print('춾'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'estoy que vale verga<unk></s>'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = 'estoy'\n",
    "print('Learned embeddings')\n",
    "print('춾'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yo opulidado el pubarmos cleto 游땓#pasibron wey que esputo es elevar con tantar tienen llegu칠 me dinerente nomar nada ti a habes <unk><unk> vienen como hdp sestrea</s>'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = 'yo op'\n",
    "print('Learned embeddings')\n",
    "print('춾'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que genera texto bueno, sin embargo, como es a nivel caracter, algunas palabras no tienen un significado sin embargo, el sentido se preserva. Puede que esto mejore al expandir la ventana a m치s de 6 caracteres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Escribimos 5 ejemplos de oraciones y medimos su verosimilitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "au6ek-AlDTwd"
   },
   "outputs": [],
   "source": [
    "def log_likelihood(model, text, ngram_model):\n",
    "    # Generate n-gram windows from input text and the respective label y\n",
    "    X, y = ngram_data.transform([text])\n",
    "    # Discard first two n-gram windows since they contain '<s>' tokens not necessary\n",
    "    X, y = X[2:], y[2:]\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "\n",
    "    logits = model(X).detach()\n",
    "    probs = F.softmax(logits, dim = 1).numpy()\n",
    "\n",
    "    return np.sum([np.log(probs[i][w]) for i, w in enumerate(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RHMEZCGENCL5",
    "outputId": "86aee787-4cca-45f8-8e2b-36e100eb48a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -58.841915\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model, \n",
    "                                        'La clase de lenguaje est치 muy padre', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jr4XFLiHNCOl",
    "outputId": "4f76dda4-d895-439b-832b-44a0e154bf01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -61.371037\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model,\n",
    "                                        'La clase de lenguaje est치 muy chida',\n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5c7tbaKNCRP",
    "outputId": "3f32376f-8c1f-449f-f8de-031191a56234"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -69.30573\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model,\n",
    "                                        'La clase de lenguaje est치 muy guay', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -96.00549\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model,\n",
    "                                        'La clase de procesamiento del lenguaje est치 muy padre', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -56.34829\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model,\n",
    "                                        'La clase de lenguaje est치 muy madre', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparamos que tienen sentido los resultados pues la palabra padre es m치s usada que chida y mucho m치s que guay., entre m치s palabras le pongamos menor ser치 su verosimilitud como en el ejemplo 4. Sin embargo, logra fallar en el 칰ltimo ejemplo cuando decimos que est치 muy madre. En el espa침ol normal no se usa, no obstante, al haber sido entrenado con tuits groseros, se obtiene que es m치s probable decir que est치 muy madre. Otro dato curioso, es que al cambiar la palabra 춺est치췉 por 춺esta췉, la verosimilitud disminuye, lo cual podr칤a ser 칰til para correcciones ortogr치ficas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4HQ2xLFNF8r"
   },
   "source": [
    "## 1.3. Estructuras morfol칩gicas correctas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJYUOuUKNCT0",
    "outputId": "4ffff7c7-d8b2-48a9-bb1f-a9e2bd7b4389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.246503 chingada\n",
      "-5.246503 chingada\n",
      "-14.173836 dachinga\n",
      "-14.173836 dachinga\n",
      "-15.5406885 dgachina\n",
      "--------------------------------------------------\n",
      "-66.48084 acihdnag\n",
      "-67.964966 aacgdhni\n",
      "-67.964966 aacgdhni\n",
      "-68.05988 caagdhni\n",
      "-68.05988 caagdhni\n"
     ]
    }
   ],
   "source": [
    "if char_level:\n",
    "    word_list = 'chingada'\n",
    "    perms = [''.join(perm) for perm in permutations(word_list)]\n",
    "else:\n",
    "    word_list = 'sino gano me voy a la chingada'.split(' ')\n",
    "    perms = [' '.join(perm) for perm in permutations(word_list)]\n",
    "#print(len(perms))\n",
    "\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[:5]:\n",
    "    print(p, t)\n",
    "print('-'*50)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[-5:]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que hay dos chingadas debido a que se obtiene una de otra solo intercambiando las a's y similarmente para las dem치s palabras. Los resultados son los esperados pues la m치s probable es chingada, luego dachinga que contiene la palabra chinga y despues dgachina que contiene la palabra china. Las menos probables ni se pueden leer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Perplejidad en validaci칩n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "BHbLjydkNCWU"
   },
   "outputs": [],
   "source": [
    "def perplexity(model, text, ngram_model):\n",
    "    # Generate n-gram windows from input text and the respective label y\n",
    "    X, y = ngram_data.transform([text])\n",
    "    # Discard first two n-gram windows since they contain '<s>' tokens not necessary\n",
    "    X, y = X[2:], y[2:]\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "\n",
    "    logits = model(X).detach()\n",
    "    probs = F.softmax(logits, dim = 1).numpy()\n",
    "    \n",
    "    ans = 1.\n",
    "    N = len(y)\n",
    "    print('Validation set dimension:', N)\n",
    "    probs = [(probs[i][w])**(1/N) for i, w in enumerate(y)]\n",
    "    for p in probs:\n",
    "        ans /= p\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "8wbHjIcdNCYS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set dimension: 615\n",
      "Validation set perplexity: 1.4799053402082503\n"
     ]
    }
   ],
   "source": [
    "print('Validation set perplexity:', perplexity(best_model,\n",
    "                                        X_val, \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelo de lenguaje neuronal inicializado con embedding dado.\n",
    "Leemos el embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = ''\n",
    "emb_txt = pd.read_csv(pth+'word2vec_col.txt',\n",
    "                        sep='\\r\\n', engine='python', \n",
    "                        header=None).loc[:,0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_d = int(emb_txt[0].split()[1])\n",
    "emb_N = int(emb_txt[0].split()[0])\n",
    "emb_txt = emb_txt[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el diccionario del embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict = {}\n",
    "for i in range(emb_N):\n",
    "    row_list = emb_txt[i].split()\n",
    "    emb_dict[row_list[0]] = torch.tensor(np.array(row_list[1:]).astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(windows):\n",
    "    emb = []\n",
    "    for words in windows:\n",
    "        w_emb = []\n",
    "        for w in words:\n",
    "            w_emb.append(emb_dict[ngram_data.id2w(w)])\n",
    "        emb.append(torch.tensor(w_emb))\n",
    "    return torch.tensor(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usaremos nivel de palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_level = False\n",
    "args.N = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 5000\n"
     ]
    }
   ],
   "source": [
    "if char_level:\n",
    "    tk = CharTokenizer\n",
    "else:\n",
    "    tk = TweetTokenizer()\n",
    "    tk = tk.tokenize\n",
    "    \n",
    "ngram_data = NgramData(args.N, 5000, tk)\n",
    "ngram_data.fit(X_train)\n",
    "print('Vocab Size:', ngram_data.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size\n",
    "args.batch_size = 64\n",
    "#Number workers\n",
    "args.num_workers = 2\n",
    "\n",
    "#Train\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = True)\n",
    "\n",
    "#Validation\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([64, 3])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print('X shape:', batch[0].shape)\n",
    "print('y shape:', batch[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase del modelo neuronal para el caso en que se preinicialice con un embedding dado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLM_embeding(nn.Module):\n",
    "    def __init__(self, args, emb=None):\n",
    "        super(NeuralLM_embeding, self).__init__()\n",
    "\n",
    "        self.window_size = args.N - 1\n",
    "        self.embedding_size = args.d\n",
    "\n",
    "        #self.emb = nn.Embedding(args.vocab_size, args.d)\n",
    "        self.emb = emb\n",
    "        self.fc1 = nn.Linear(args.d * (args.N - 1), args.d_h)\n",
    "        self.drop1 = nn.Dropout(p = args.dropout)\n",
    "        self.fc2 = nn.Linear(args.d_h, args.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1, self.window_size * self.embedding_size)\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = self.drop1(h)\n",
    "        x = self.fc2(h)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiamos algunos hiperpar치metros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model hyperparameters\n",
    "#Vocabulary size\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "#Word embeddings dimension\n",
    "args.d = emb_d\n",
    "#Hidden layer dimension\n",
    "args.d_h = 200 #Est치 bien puestp que emb_d es 100\n",
    "#Dropout\n",
    "args.dropout = 0.1\n",
    "\n",
    "#Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "#Scheduler hyperparameters\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "#Saving hyperparameters\n",
    "args.savedir = pth + 'model_emb'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "#Create model\n",
    "model_emb = NeuralLM_embeding(args=args, emb=embedding)\n",
    "\n",
    "#Send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "#Loss, Optimizer and Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer, \n",
    "                                                       mode = 'min',\n",
    "                                                       factor = args.lr_factor,\n",
    "                                                       patience = args.lr_patience,\n",
    "                                                       verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa de entrenamiento del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-e7244224207f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-7bb5ab4537e7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1916\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "#Training\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model.train()\n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        #Forward pass\n",
    "        outputs = model(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "\n",
    "        #Get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "\n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #Metrics in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Metrics in validation dataset\n",
    "    model.eval()\n",
    "    tuning_metric = model_eval(val_loader, model, gpu=args.use_gpu)\n",
    "    mean_epoch_metric = np.mean(tuning_metric)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "\n",
    "    #Check metric improvement\n",
    "    is_improve = tuning_metric > best_metric\n",
    "    if is_improve:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "\n",
    "    #Save best model\n",
    "    save_checkpoint({'epoch'       : epoch+1,\n",
    "                     'state_dict'  : model.state_dict(),\n",
    "                     'optimizer'   : optimizer.state_dict(),\n",
    "                     'scheduler'   : scheduler.state_dict(),\n",
    "                     'best_metric' : best_metric}, is_improve, args.savedir)\n",
    "    \n",
    "    #Early stoping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print('No improvement. Breaking out of loop.')\n",
    "        break\n",
    "    print('Train accuracy: ', mean_epoch_metric)\n",
    "    print('Epoch [{}/{}]: Loss = {:.4f}, Val Acurracy = {:.4f}, Epoch time = {:.2f}'.\n",
    "          format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time()-epoch_start_time)))\n",
    "    \n",
    "print('---------- %s seconds ---------' % time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2927, -0.5446,  0.1714],\n",
       "         [ 0.0929,  0.1798,  0.0761],\n",
       "         [-1.8047,  0.3559,  1.4479]],\n",
       "\n",
       "        [[ 0.2927, -0.5446,  0.1714],\n",
       "         [ 0.0929,  0.1798,  0.0761],\n",
       "         [-1.8047,  0.3559,  1.4479]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = nn.Embedding(10,3)\n",
    "e(torch.tensor([[1,2,3],[1,2,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([ -1.0961,  -1.0184,  -1.8950,  -1.9984,   4.2174,   6.2988,  -0.3170,\n",
       "           -1.5824,   2.4694,   2.3756,  -1.1883,  -1.4300,  -0.6092,   0.6152,\n",
       "            1.1048,   0.8866,   2.0087,  -1.8453,   3.2080,  -0.1376,  -4.1967,\n",
       "           -1.6238,   3.1863,   0.6211,   1.6738,   1.9099,   1.2532,   3.1336,\n",
       "            3.2359,  -4.5080,   1.6890,   2.0606,  -0.0722,   2.1841,   6.5726,\n",
       "           -0.1412,  -1.5676,   0.1572,   3.5281,  -0.0361,   1.6923,   0.3356,\n",
       "           -1.2028,   5.2517,  -1.0029,   2.4605,  -4.6152,   3.2214,   0.1949,\n",
       "            1.4327,   1.9920,  -0.3237,  -2.1740,   0.9460,  -3.8526,   3.9340,\n",
       "            1.6828,   4.4286,  -0.1994,   1.8463,  -2.1925, -10.4011,  -2.1528,\n",
       "            2.8122,  -3.9068,   0.2875,  -0.2148,   5.8414,   0.1621,  -1.0202,\n",
       "           -3.9861,  -3.2815,   0.9894,  -0.5335,  -1.1370,   0.7730,   0.8782,\n",
       "            1.1881,  -0.6197,   6.8253,   3.5462,   2.0978,   0.7641,  -2.7847,\n",
       "           -0.8488,   2.3285,   1.4681,   1.9173,  -0.1934,   0.4048,   0.7009,\n",
       "            0.5112,   1.3233,   1.1322,   2.9132,  -3.3316,   5.0304,   0.8276,\n",
       "           -0.1572,  -0.2303], dtype=torch.float64),\n",
       "  tensor([-1.6417,  1.4477, -2.2832, -1.9652, -0.2229,  5.1052, -0.1207, -0.1268,\n",
       "          -3.1773, -3.4544, -0.9431, -0.0945, -1.1894, -0.8121, -2.5730, -0.6139,\n",
       "          -2.3118,  1.0510,  5.6347, -5.8270,  1.2376,  1.0716,  3.8221,  2.3954,\n",
       "           0.1699,  3.2568,  2.8973,  3.2748, -2.9364,  0.2720, -1.0295, -2.6173,\n",
       "          -1.8071,  1.7376,  0.3391,  3.9329,  1.5714, -4.1001,  4.1568,  1.1624,\n",
       "          -0.5523, -0.5859, -4.7672,  0.2533, -1.1242, -0.1151, -5.6066,  2.9766,\n",
       "           4.4260,  1.0199,  3.7607, -2.2983,  4.4166, -1.3840, -1.8625,  0.3991,\n",
       "          -1.0969, -2.2860,  2.9928,  0.0440,  3.7624, -6.5231,  0.6213,  2.6418,\n",
       "          -1.9243, -1.1412, -3.8318,  0.5496,  2.2608, -1.3184, -1.1347, -3.7882,\n",
       "          -0.7750,  3.9567, -3.5794, -4.4237,  4.5057,  0.7191, -1.3996,  3.0972,\n",
       "           0.1075,  2.8299, -0.7602, -0.2772, -1.2257,  2.1579,  0.5184,  3.4380,\n",
       "          -1.0875, -1.5299,  0.3995,  0.8874, -1.9949,  2.9122,  3.2580, -2.6000,\n",
       "          -1.9951, -0.1765,  3.9469, -2.7925], dtype=torch.float64),\n",
       "  tensor([ 2.0749, -3.6219,  2.7962,  0.9734,  0.8236,  1.0387, -1.9435,  0.9390,\n",
       "           3.1465,  2.5053, -3.3736,  8.0420,  0.2890,  3.2729, -0.4777, -2.7176,\n",
       "          -3.7375,  1.4382,  3.4500, -4.6847,  3.8147, -1.8224, -0.4273,  2.7510,\n",
       "          -0.8123, -0.9013,  0.3879,  2.1081,  3.4685, -1.6470, -0.2772,  0.3224,\n",
       "           6.4958,  2.4072,  5.2072, -0.3711,  1.1508, -3.0352,  2.2160,  4.2224,\n",
       "          -0.9467,  0.7337, -1.5822,  7.8573,  0.6339, -1.4653, -5.4824,  3.3949,\n",
       "          -4.4926,  3.3945,  2.2128,  1.7211, -3.3620,  0.8859, -8.6017,  0.7469,\n",
       "           1.8434, -0.9842, -0.5781, -5.7163,  2.3754, -5.1170, -1.2832, -1.8492,\n",
       "          -3.7761,  4.3630,  3.4920,  6.0767, -0.7321,  1.6587,  2.2281, -2.8280,\n",
       "           1.1835, -0.2994, -0.1235, -1.5035,  1.7021, -2.2414,  2.5903,  3.5688,\n",
       "          -0.5472,  5.0692, -3.2299, -4.0191, -0.1603, -0.9015, -4.5494,  6.0245,\n",
       "          -0.6503,  2.4946, -1.4276,  0.1534,  3.7343,  1.0262,  7.8431,  2.1719,\n",
       "           2.4541, -1.7587,  1.0061,  0.2191], dtype=torch.float64)],\n",
       " [tensor([ -1.0961,  -1.0184,  -1.8950,  -1.9984,   4.2174,   6.2988,  -0.3170,\n",
       "           -1.5824,   2.4694,   2.3756,  -1.1883,  -1.4300,  -0.6092,   0.6152,\n",
       "            1.1048,   0.8866,   2.0087,  -1.8453,   3.2080,  -0.1376,  -4.1967,\n",
       "           -1.6238,   3.1863,   0.6211,   1.6738,   1.9099,   1.2532,   3.1336,\n",
       "            3.2359,  -4.5080,   1.6890,   2.0606,  -0.0722,   2.1841,   6.5726,\n",
       "           -0.1412,  -1.5676,   0.1572,   3.5281,  -0.0361,   1.6923,   0.3356,\n",
       "           -1.2028,   5.2517,  -1.0029,   2.4605,  -4.6152,   3.2214,   0.1949,\n",
       "            1.4327,   1.9920,  -0.3237,  -2.1740,   0.9460,  -3.8526,   3.9340,\n",
       "            1.6828,   4.4286,  -0.1994,   1.8463,  -2.1925, -10.4011,  -2.1528,\n",
       "            2.8122,  -3.9068,   0.2875,  -0.2148,   5.8414,   0.1621,  -1.0202,\n",
       "           -3.9861,  -3.2815,   0.9894,  -0.5335,  -1.1370,   0.7730,   0.8782,\n",
       "            1.1881,  -0.6197,   6.8253,   3.5462,   2.0978,   0.7641,  -2.7847,\n",
       "           -0.8488,   2.3285,   1.4681,   1.9173,  -0.1934,   0.4048,   0.7009,\n",
       "            0.5112,   1.3233,   1.1322,   2.9132,  -3.3316,   5.0304,   0.8276,\n",
       "           -0.1572,  -0.2303], dtype=torch.float64),\n",
       "  tensor([-1.6417,  1.4477, -2.2832, -1.9652, -0.2229,  5.1052, -0.1207, -0.1268,\n",
       "          -3.1773, -3.4544, -0.9431, -0.0945, -1.1894, -0.8121, -2.5730, -0.6139,\n",
       "          -2.3118,  1.0510,  5.6347, -5.8270,  1.2376,  1.0716,  3.8221,  2.3954,\n",
       "           0.1699,  3.2568,  2.8973,  3.2748, -2.9364,  0.2720, -1.0295, -2.6173,\n",
       "          -1.8071,  1.7376,  0.3391,  3.9329,  1.5714, -4.1001,  4.1568,  1.1624,\n",
       "          -0.5523, -0.5859, -4.7672,  0.2533, -1.1242, -0.1151, -5.6066,  2.9766,\n",
       "           4.4260,  1.0199,  3.7607, -2.2983,  4.4166, -1.3840, -1.8625,  0.3991,\n",
       "          -1.0969, -2.2860,  2.9928,  0.0440,  3.7624, -6.5231,  0.6213,  2.6418,\n",
       "          -1.9243, -1.1412, -3.8318,  0.5496,  2.2608, -1.3184, -1.1347, -3.7882,\n",
       "          -0.7750,  3.9567, -3.5794, -4.4237,  4.5057,  0.7191, -1.3996,  3.0972,\n",
       "           0.1075,  2.8299, -0.7602, -0.2772, -1.2257,  2.1579,  0.5184,  3.4380,\n",
       "          -1.0875, -1.5299,  0.3995,  0.8874, -1.9949,  2.9122,  3.2580, -2.6000,\n",
       "          -1.9951, -0.1765,  3.9469, -2.7925], dtype=torch.float64),\n",
       "  tensor([ 2.0749, -3.6219,  2.7962,  0.9734,  0.8236,  1.0387, -1.9435,  0.9390,\n",
       "           3.1465,  2.5053, -3.3736,  8.0420,  0.2890,  3.2729, -0.4777, -2.7176,\n",
       "          -3.7375,  1.4382,  3.4500, -4.6847,  3.8147, -1.8224, -0.4273,  2.7510,\n",
       "          -0.8123, -0.9013,  0.3879,  2.1081,  3.4685, -1.6470, -0.2772,  0.3224,\n",
       "           6.4958,  2.4072,  5.2072, -0.3711,  1.1508, -3.0352,  2.2160,  4.2224,\n",
       "          -0.9467,  0.7337, -1.5822,  7.8573,  0.6339, -1.4653, -5.4824,  3.3949,\n",
       "          -4.4926,  3.3945,  2.2128,  1.7211, -3.3620,  0.8859, -8.6017,  0.7469,\n",
       "           1.8434, -0.9842, -0.5781, -5.7163,  2.3754, -5.1170, -1.2832, -1.8492,\n",
       "          -3.7761,  4.3630,  3.4920,  6.0767, -0.7321,  1.6587,  2.2281, -2.8280,\n",
       "           1.1835, -0.2994, -0.1235, -1.5035,  1.7021, -2.2414,  2.5903,  3.5688,\n",
       "          -0.5472,  5.0692, -3.2299, -4.0191, -0.1603, -0.9015, -4.5494,  6.0245,\n",
       "          -0.6503,  2.4946, -1.4276,  0.1534,  3.7343,  1.0262,  7.8431,  2.1719,\n",
       "           2.4541, -1.7587,  1.0061,  0.2191], dtype=torch.float64)]]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(torch.tensor([[1,2,3],[1,2,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(windows):\n",
    "    emb = []\n",
    "    for words in windows:\n",
    "        w_emb = []\n",
    "        for w in words:\n",
    "            w_emb.append(emb_dict[ngram_data.id2w[w.item()]])\n",
    "        emb.append(w_emb)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
