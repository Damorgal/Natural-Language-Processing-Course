{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BRaP_x0eoI1"
   },
   "source": [
    "# NLP. Tarea 5: Modelo del Lenguaje Neuronal.\n",
    "\n",
    "**Diego Moreno**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Modelo Neuronal a nivel de caracter.\n",
    "\n",
    "Importamos librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5WWvA-6sets1"
   },
   "outputs": [],
   "source": [
    "# Tools\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from typing import Tuple\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "# Preprocesing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import  TweetTokenizer\n",
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Pytorch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Scikitlearn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bqN6c3B-f7_h"
   },
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Yb4ys6nJgPYJ"
   },
   "outputs": [],
   "source": [
    "pth = ''\n",
    "X_train = pd.read_csv(pth+'mex_train.txt', sep='\\r\\n',  engine='python', header=None).loc[:,0].values.tolist()\n",
    "X_val = pd.read_csv(pth+'mex_val.txt', sep='\\r\\n',  engine='python', header=None).loc[:,0].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nivel de caracter, tenemos que fijarnos en una ventana de 6 o más:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Jj3gCUEXhlGu"
   },
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.N = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase de N-gramas se quedará igual pues la estrategia será solamente cambiar el tokenizador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2T9J65oaiILS"
   },
   "outputs": [],
   "source": [
    "class NgramData():\n",
    "    def __init__(self, N: int, vocab_max: int=5000, tokenizer=None, embedding_model=None):\n",
    "        self.tokenizer = tokenizer if tokenizer else self.default_tokenizer\n",
    "        self.punct = set(['.',',',';',':','-','^','«','»','\"','!','¡','?','¿','\\'','...','<url>','*','@usuario'])\n",
    "        self.N = N\n",
    "        self.vocab_max = vocab_max\n",
    "        self.UNK = '<unk>'\n",
    "        self.SOS = '<s>'\n",
    "        self.EOS = '</s>'\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "    def default_tokenizer(self, doc: str) -> list:\n",
    "        return doc.split(' ')\n",
    "\n",
    "    def get_vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def remove_word(self, word: str) -> bool:\n",
    "        word = word.lower()\n",
    "        is_punct = True if word in self.punct else False\n",
    "        is_digit = word.isnumeric()\n",
    "        return is_punct or is_digit\n",
    "\n",
    "    def get_vocab(self, corpus: list) -> set:\n",
    "        freq_dist = FreqDist([w.lower() for sent in corpus \\\n",
    "                              for w in self.tokenizer(sent) \\\n",
    "                              if not self.remove_word(w)])\n",
    "        sorted_words = self.sortFreqDict(freq_dist)[:self.vocab_max-3]\n",
    "        return set(sorted_words)\n",
    "\n",
    "    def sortFreqDict(self, freq_dist) -> list:\n",
    "        freq_dist = dict(freq_dist)\n",
    "        return sorted(freq_dist, key=freq_dist.get, reverse=True)\n",
    "\n",
    "    def fit(self, corpus: list) -> None:\n",
    "        self.vocab = self.get_vocab(corpus)\n",
    "        self.vocab.add(self.UNK)\n",
    "        self.vocab.add(self.SOS)\n",
    "        self.vocab.add(self.EOS)\n",
    "\n",
    "        self.w2id = {}\n",
    "        self.id2w = {}\n",
    "\n",
    "        if self.embedding_model is not None:\n",
    "            self.embedding_matrix = np.empty([len(self.vocab), self.embedding_model.vector_size])\n",
    "\n",
    "        ID = 0\n",
    "        for doc in corpus:\n",
    "            for word in self.tokenizer(doc):\n",
    "                word_ = word.lower()\n",
    "                if word_ in self.vocab and not word_ in self.w2id:\n",
    "                    self.w2id[word_] = ID\n",
    "                    self.id2w[ID] = word_\n",
    "                    if self.embedding_model is not None:\n",
    "                        if word_ in self.embedding_model:\n",
    "                            self.embedding_matrix[ID] = self.embedding_model[word_]\n",
    "                        else:\n",
    "                            self.embedding_matrix[ID] = np.random.rand(self.embedding_model.vector_size) \n",
    "                    ID += 1\n",
    "        #Special tokens  \n",
    "        self.w2id.update({self.UNK: ID, \n",
    "                          self.SOS: ID+1,\n",
    "                          self.EOS: ID+2})  \n",
    "        self.id2w.update({ID  : self.UNK, \n",
    "                          ID+1: self.SOS,\n",
    "                          ID+2: self.EOS})\n",
    "    \n",
    "    def replace_unk(self, doc_tokens: list) -> list: \n",
    "        for i, token in enumerate(doc_tokens):\n",
    "            if token.lower() not in self.vocab:\n",
    "                doc_tokens[i] = self.UNK\n",
    "        return doc_tokens\n",
    "\n",
    "\n",
    "    def get_ngram_doc(self, doc:str) -> list:\n",
    "        doc_tokens = self.tokenizer(doc)\n",
    "        doc_tokens = self.replace_unk(doc_tokens)\n",
    "        doc_tokens = [w.lower() for w in doc_tokens]\n",
    "        doc_tokens = [self.SOS]*(self.N - 1) + doc_tokens + [self.EOS]\n",
    "        return list(ngrams(doc_tokens, self.N))\n",
    "    \n",
    "    def transform(self, corpus: list) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        X_ngrams = []\n",
    "        y = []\n",
    "        for doc in corpus:\n",
    "            doc_ngram = self.get_ngram_doc(doc)\n",
    "            for words_window in doc_ngram:\n",
    "                words_window_ids = [self.w2id[w] for w in words_window]\n",
    "                X_ngrams.append(list(words_window_ids[:-1]))\n",
    "                y.append(words_window_ids[-1])\n",
    "        return np.array(X_ngrams), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el nuevo tokenizador a nivel de caracter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "igtUbF-UtA5e"
   },
   "outputs": [],
   "source": [
    "def CharTokenizer(doc: str) -> list:\n",
    "    l = []\n",
    "    for c in doc:\n",
    "        l.append(c)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos nivel de caracteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_level = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ukx1t0n8h12F"
   },
   "outputs": [],
   "source": [
    "if char_level:\n",
    "    tk = CharTokenizer\n",
    "else:\n",
    "    tk = TweetTokenizer()\n",
    "    tk = tk.tokenize\n",
    "    \n",
    "ngram_data = NgramData(args.N, 5000, tk)\n",
    "ngram_data.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OH2bHAWHqJ7w",
    "outputId": "1bd5b974-1b17-494c-8e8f-346fbc98c220"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 344\n"
     ]
    }
   ],
   "source": [
    "print('Vocab Size:', ngram_data.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos los datos transformados y los loader de los mismos para entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Hl5NY_wFqZPt"
   },
   "outputs": [],
   "source": [
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2PH5JSUbwX6O"
   },
   "outputs": [],
   "source": [
    "#Batch size\n",
    "args.batch_size = 64\n",
    "#Number workers\n",
    "args.num_workers = 2\n",
    "\n",
    "#Train\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = True)\n",
    "\n",
    "#Validation\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CxrtxytHyIAR",
    "outputId": "baebf0d3-f72b-4e23-e202-0fb96ad0d2ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([64, 5])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print('X shape:', batch[0].shape)\n",
    "print('y shape:', batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "l_diObstybV6"
   },
   "outputs": [],
   "source": [
    "#[[ngram_data.id2w[w] for w in tw] for tw in batch[0].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase del modelo neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "hXQa9kP8VMFU"
   },
   "outputs": [],
   "source": [
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(NeuralLM, self).__init__()\n",
    "\n",
    "        self.window_size = args.N - 1\n",
    "        self.embedding_size = args.d\n",
    "\n",
    "        self.emb = nn.Embedding(args.vocab_size, args.d)\n",
    "        self.fc1 = nn.Linear(args.d * (args.N - 1), args.d_h)\n",
    "        self.drop1 = nn.Dropout(p = args.dropout)\n",
    "        self.fc2 = nn.Linear(args.d_h, args.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1, self.window_size * self.embedding_size)\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = self.drop1(h)\n",
    "        x = self.fc2(h)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para el entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0vZZwQA1XZ8_"
   },
   "outputs": [],
   "source": [
    "def get_preds(raw_logits):\n",
    "    probs = F.softmax(raw_logits.detach(), dim=1)\n",
    "    y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    return y_pred\n",
    "\n",
    "def model_eval(data, model, gpu=False):\n",
    "    with torch.no_grad():\n",
    "        preds, tgts = [], []\n",
    "        for window_words, labels in data:\n",
    "            if gpu:\n",
    "                window_words = window_words.cuda()\n",
    "            outputs = model(window_words)\n",
    "\n",
    "            #Predictions\n",
    "            y_pred = get_preds(outputs)\n",
    "            tgt = labels.numpy()\n",
    "            tgts.append(tgt)\n",
    "            preds.append(y_pred)\n",
    "    tgts = [e for l in tgts for e in l]\n",
    "    preds = [e for l in preds for e in l]\n",
    "    return accuracy_score(tgts, preds)\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint_path, filename='checkpoint.pt'):\n",
    "    filename = os.path.join(checkpoint_path, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, os.path.join(checkpoint_path, 'model_best.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos hiperparámetros que tendrán que ser modificados posteriormente al usar un nuevo embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Dnk0A8MmXzRr"
   },
   "outputs": [],
   "source": [
    "#Model hyperparameters\n",
    "#Vocabulary size\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "#Word embeddings dimension\n",
    "args.d = 100\n",
    "#Hidden layer dimension\n",
    "args.d_h = 200\n",
    "#Dropout\n",
    "args.dropout = 0.1\n",
    "\n",
    "#Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "#Scheduler hyperparameters\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "#Saving hyperparameters\n",
    "args.savedir = pth + 'model'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "#Create model\n",
    "model = NeuralLM(args=args)\n",
    "\n",
    "#Send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "#Loss, Optimizer and Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer, \n",
    "                                                       mode = 'min',\n",
    "                                                       factor = args.lr_factor,\n",
    "                                                       patience = args.lr_patience,\n",
    "                                                       verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa de entrenamiento del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 902
    },
    "id": "m5w33lulaO27",
    "outputId": "86e4a066-cdcd-4533-caad-4a110bf51600"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.4386065422287932\n",
      "Epoch [1/100]: Loss = 1.9773, Val Acurracy = 0.4386, Epoch time = 31.76\n",
      "Train accuracy:  0.46004435409351324\n",
      "Epoch [2/100]: Loss = 1.8176, Val Acurracy = 0.4600, Epoch time = 30.50\n",
      "Train accuracy:  0.4607835889854001\n",
      "Epoch [3/100]: Loss = 1.7673, Val Acurracy = 0.4608, Epoch time = 30.55\n",
      "Train accuracy:  0.4825540565514692\n",
      "Epoch [4/100]: Loss = 1.7373, Val Acurracy = 0.4826, Epoch time = 30.89\n",
      "Train accuracy:  0.4729624838292367\n",
      "Epoch [5/100]: Loss = 1.7156, Val Acurracy = 0.4730, Epoch time = 31.02\n",
      "Train accuracy:  0.4506006283496581\n",
      "Epoch [6/100]: Loss = 1.7002, Val Acurracy = 0.4506, Epoch time = 30.81\n",
      "Train accuracy:  0.47444095361301053\n",
      "Epoch [7/100]: Loss = 1.6873, Val Acurracy = 0.4744, Epoch time = 30.63\n",
      "Train accuracy:  0.4743300683792275\n",
      "Epoch [8/100]: Loss = 1.6764, Val Acurracy = 0.4743, Epoch time = 30.30\n",
      "Train accuracy:  0.4886712252818333\n",
      "Epoch [9/100]: Loss = 1.6685, Val Acurracy = 0.4887, Epoch time = 32.59\n",
      "Train accuracy:  0.4830900018480872\n",
      "Epoch [10/100]: Loss = 1.6614, Val Acurracy = 0.4831, Epoch time = 33.33\n",
      "Train accuracy:  0.48443910552578084\n",
      "Epoch [11/100]: Loss = 1.6551, Val Acurracy = 0.4844, Epoch time = 32.53\n",
      "Epoch    12: reducing learning rate of group 0 to 1.1500e-01.\n",
      "Train accuracy:  0.49240436148586214\n",
      "Epoch [12/100]: Loss = 1.6499, Val Acurracy = 0.4924, Epoch time = 32.74\n",
      "Train accuracy:  0.5046386989465903\n",
      "Epoch [13/100]: Loss = 1.6107, Val Acurracy = 0.5046, Epoch time = 32.69\n",
      "Train accuracy:  0.5072445019404916\n",
      "Epoch [14/100]: Loss = 1.6055, Val Acurracy = 0.5072, Epoch time = 33.59\n",
      "Train accuracy:  0.5064683053040103\n",
      "Epoch [15/100]: Loss = 1.6023, Val Acurracy = 0.5065, Epoch time = 33.00\n",
      "Train accuracy:  0.5022916281648494\n",
      "Epoch [16/100]: Loss = 1.6014, Val Acurracy = 0.5023, Epoch time = 31.56\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-e7244224207f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtraining_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "#Training\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model.train()\n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        #Forward pass\n",
    "        outputs = model(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "\n",
    "        #Get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "\n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #Metrics in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Metrics in validation dataset\n",
    "    model.eval()\n",
    "    tuning_metric = model_eval(val_loader, model, gpu=args.use_gpu)\n",
    "    mean_epoch_metric = np.mean(tuning_metric)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "\n",
    "    #Check metric improvement\n",
    "    is_improve = tuning_metric > best_metric\n",
    "    if is_improve:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "\n",
    "    #Save best model\n",
    "    save_checkpoint({'epoch'       : epoch+1,\n",
    "                     'state_dict'  : model.state_dict(),\n",
    "                     'optimizer'   : optimizer.state_dict(),\n",
    "                     'scheduler'   : scheduler.state_dict(),\n",
    "                     'best_metric' : best_metric}, is_improve, args.savedir)\n",
    "    \n",
    "    #Early stoping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print('No improvement. Breaking out of loop.')\n",
    "        break\n",
    "    print('Train accuracy: ', mean_epoch_metric)\n",
    "    print('Epoch [{}/{}]: Loss = {:.4f}, Val Acurracy = {:.4f}, Epoch time = {:.2f}'.\n",
    "          format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time()-epoch_start_time)))\n",
    "    \n",
    "print('---------- %s seconds ---------' % time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejor modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(344, 100)\n",
       "  (fc1): Linear(in_features=500, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=344, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model with learned embeddings\n",
    "best_model = NeuralLM(args)\n",
    "best_model.load_state_dict(torch.load(pth+'model/model_best.pt')['state_dict'])\n",
    "best_model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "uChqtaZNjHkP",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def print_closest_words(embeddings, ngram_data, word, n):\n",
    "    word_id = torch.LongTensor([ngram_data.w2id[word]])\n",
    "    word_embed = embeddings(word_id)\n",
    "    dists = torch.norm(embeddings.weight - word_embed, dim=1).detach()\n",
    "    lst = sorted(enumerate(dists.numpy()), key=lambda x : x[1])\n",
    "    for idx, diff in lst[1:n+1]:\n",
    "        print(ngram_data.id2w[idx], diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "r_I150SHuyVu",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "b2f2d5f8-70d5-4c4a-e966-282d64f859b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      "a 3.8728173\n",
      "o 4.1493793\n",
      "  4.3050637\n",
      "s 4.475555\n",
      "n 4.615959\n",
      "r 4.6823664\n",
      "i 4.7688804\n",
      "l 5.04975\n",
      "t 5.1100945\n",
      "<s> 5.1198354\n"
     ]
    }
   ],
   "source": [
    "print('Learned embeddings')\n",
    "print('¯'*20)\n",
    "print_closest_words(best_model.emb, ngram_data, 'e', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Generamos texto 3 veces con máximo de 300 caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "gziHZsIQv0AO"
   },
   "outputs": [],
   "source": [
    "lenght_max = 300\n",
    "\n",
    "def parse_text(text, tokenizer):\n",
    "    all_tokens = [w.lower() if w in ngram_data.w2id else '<unk>' for w in tokenizer(text)]\n",
    "    token_ids = [ngram_data.w2id[word.lower()] for word in all_tokens]\n",
    "    return all_tokens, token_ids\n",
    "\n",
    "def sample_next_word(logits, temperature=1.):\n",
    "    logits = np.asarray(logits).astype('float64')\n",
    "    preds = logits/temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probs = np.random.multinomial(1, preds)\n",
    "    return np.argmax(probs)\n",
    "\n",
    "def predict_next_token(model, token_ids):\n",
    "    words_ids_tensor = torch.LongTensor(token_ids).unsqueeze(0)\n",
    "    y_raw_pred = model(words_ids_tensor).squeeze(0).detach().numpy()\n",
    "    y_pred = sample_next_word(y_raw_pred, 1.)\n",
    "    return y_pred\n",
    "\n",
    "def generate_sentence(model, initial_text, tokenizer):\n",
    "    all_tokens, window_word_ids = parse_text(initial_text, tokenizer)\n",
    "    for i in range(lenght_max):\n",
    "        y_pred = predict_next_token(best_model, window_word_ids)\n",
    "        next_word = ngram_data.id2w[y_pred]\n",
    "        all_tokens.append(next_word)\n",
    "        if next_word == '</s>':\n",
    "            break\n",
    "        else:\n",
    "            window_word_ids.pop(0)\n",
    "            window_word_ids.append(y_pred)\n",
    "    if char_level:\n",
    "        return ''.join(all_tokens)\n",
    "    else:\n",
    "        return ' '.join(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "UbXJ642z--mB",
    "outputId": "8a33b6a5-6361-4948-dbcd-a528e9ca0708"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s><s putos perder a oierencanas anuncia un putos<unk> 😊🏻🖕🏻</s>'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = '<s><s'\n",
    "print('Learned embeddings')\n",
    "print('¯'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'estoy que vale verga<unk></s>'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = 'estoy'\n",
    "print('Learned embeddings')\n",
    "print('¯'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yo opulidado el pubarmos cleto 😈#pasibron wey que esputo es elevar con tantar tienen llegué me dinerente nomar nada ti a habes <unk><unk> vienen como hdp sestrea</s>'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = 'yo op'\n",
    "print('Learned embeddings')\n",
    "print('¯'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que genera texto bueno, sin embargo, como es a nivel caracter, algunas palabras no tienen un significado sin embargo, el sentido se preserva. Puede que esto mejore al expandir la ventana a más de 6 caracteres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Escribimos 5 ejemplos de oraciones y medimos su verosimilitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "au6ek-AlDTwd"
   },
   "outputs": [],
   "source": [
    "def log_likelihood(model, text, ngram_model):\n",
    "    # Generate n-gram windows from input text and the respective label y\n",
    "    X, y = ngram_data.transform([text])\n",
    "    # Discard first two n-gram windows since they contain '<s>' tokens not necessary\n",
    "    X, y = X[2:], y[2:]\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "\n",
    "    logits = model(X).detach()\n",
    "    probs = F.softmax(logits, dim = 1).numpy()\n",
    "\n",
    "    return np.sum([np.log(probs[i][w]) for i, w in enumerate(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RHMEZCGENCL5",
    "outputId": "86aee787-4cca-45f8-8e2b-36e100eb48a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -58.841915\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model, \n",
    "                                        'La clase de lenguaje está muy padre', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jr4XFLiHNCOl",
    "outputId": "4f76dda4-d895-439b-832b-44a0e154bf01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -61.371037\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model,\n",
    "                                        'La clase de lenguaje está muy chida',\n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5c7tbaKNCRP",
    "outputId": "3f32376f-8c1f-449f-f8de-031191a56234"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -69.30573\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model,\n",
    "                                        'La clase de lenguaje está muy guay', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -96.00549\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model,\n",
    "                                        'La clase de procesamiento del lenguaje está muy padre', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -56.34829\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model,\n",
    "                                        'La clase de lenguaje está muy madre', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparamos que tienen sentido los resultados pues la palabra padre es más usada que chida y mucho más que guay., entre más palabras le pongamos menor será su verosimilitud como en el ejemplo 4. Sin embargo, logra fallar en el último ejemplo cuando decimos que está muy madre. En el español normal no se usa, no obstante, al haber sido entrenado con tuits groseros, se obtiene que es más probable decir que está muy madre. Otro dato curioso, es que al cambiar la palabra «está» por «esta», la verosimilitud disminuye, lo cual podría ser útil para correcciones ortográficas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4HQ2xLFNF8r"
   },
   "source": [
    "## 1.3. Estructuras morfológicas correctas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJYUOuUKNCT0",
    "outputId": "4ffff7c7-d8b2-48a9-bb1f-a9e2bd7b4389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.246503 chingada\n",
      "-5.246503 chingada\n",
      "-14.173836 dachinga\n",
      "-14.173836 dachinga\n",
      "-15.5406885 dgachina\n",
      "--------------------------------------------------\n",
      "-66.48084 acihdnag\n",
      "-67.964966 aacgdhni\n",
      "-67.964966 aacgdhni\n",
      "-68.05988 caagdhni\n",
      "-68.05988 caagdhni\n"
     ]
    }
   ],
   "source": [
    "if char_level:\n",
    "    word_list = 'chingada'\n",
    "    perms = [''.join(perm) for perm in permutations(word_list)]\n",
    "else:\n",
    "    word_list = 'sino gano me voy a la chingada'.split(' ')\n",
    "    perms = [' '.join(perm) for perm in permutations(word_list)]\n",
    "#print(len(perms))\n",
    "\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[:5]:\n",
    "    print(p, t)\n",
    "print('-'*50)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[-5:]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que hay dos chingadas debido a que se obtiene una de otra solo intercambiando las a's y similarmente para las demás palabras. Los resultados son los esperados pues la más probable es chingada, luego dachinga que contiene la palabra chinga y despues dgachina que contiene la palabra china. Las menos probables ni se pueden leer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Perplejidad en validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "BHbLjydkNCWU"
   },
   "outputs": [],
   "source": [
    "def perplexity(model, text, ngram_model):\n",
    "    # Generate n-gram windows from input text and the respective label y\n",
    "    X, y = ngram_data.transform([text])\n",
    "    # Discard first two n-gram windows since they contain '<s>' tokens not necessary\n",
    "    X, y = X[2:], y[2:]\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "\n",
    "    logits = model(X).detach()\n",
    "    probs = F.softmax(logits, dim = 1).numpy()\n",
    "    \n",
    "    ans = 1.\n",
    "    N = len(y)\n",
    "    print('Validation set dimension:', N)\n",
    "    probs = [(probs[i][w])**(1/N) for i, w in enumerate(y)]\n",
    "    for p in probs:\n",
    "        ans /= p\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "8wbHjIcdNCYS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set dimension: 615\n",
      "Validation set perplexity: 1.4799053402082503\n"
     ]
    }
   ],
   "source": [
    "print('Validation set perplexity:', perplexity(best_model,\n",
    "                                        X_val, \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelo de lenguaje neuronal inicializado con embedding dado.\n",
    "Leemos el embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = ''\n",
    "emb_txt = pd.read_csv(pth+'word2vec_col.txt',\n",
    "                        sep='\\r\\n', engine='python', \n",
    "                        header=None).loc[:,0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_d = int(emb_txt[0].split()[1])\n",
    "emb_N = int(emb_txt[0].split()[0])\n",
    "emb_txt = emb_txt[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el diccionario del embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict = {}\n",
    "for i in range(emb_N):\n",
    "    row_list = emb_txt[i].split()\n",
    "    emb_dict[row_list[0]] = torch.tensor(np.array(row_list[1:]).astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(windows):\n",
    "    emb = []\n",
    "    for words in windows:\n",
    "        w_emb = []\n",
    "        for w in words:\n",
    "            w_emb.append(emb_dict[ngram_data.id2w(w)])\n",
    "        emb.append(torch.tensor(w_emb))\n",
    "    return torch.tensor(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usaremos nivel de palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_level = False\n",
    "args.N = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 5000\n"
     ]
    }
   ],
   "source": [
    "if char_level:\n",
    "    tk = CharTokenizer\n",
    "else:\n",
    "    tk = TweetTokenizer()\n",
    "    tk = tk.tokenize\n",
    "    \n",
    "ngram_data = NgramData(args.N, 5000, tk)\n",
    "ngram_data.fit(X_train)\n",
    "print('Vocab Size:', ngram_data.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size\n",
    "args.batch_size = 64\n",
    "#Number workers\n",
    "args.num_workers = 2\n",
    "\n",
    "#Train\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = True)\n",
    "\n",
    "#Validation\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([64, 3])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print('X shape:', batch[0].shape)\n",
    "print('y shape:', batch[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase del modelo neuronal para el caso en que se preinicialice con un embedding dado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLM_embeding(nn.Module):\n",
    "    def __init__(self, args, emb=None):\n",
    "        super(NeuralLM_embeding, self).__init__()\n",
    "\n",
    "        self.window_size = args.N - 1\n",
    "        self.embedding_size = args.d\n",
    "\n",
    "        #self.emb = nn.Embedding(args.vocab_size, args.d)\n",
    "        self.emb = emb\n",
    "        self.fc1 = nn.Linear(args.d * (args.N - 1), args.d_h)\n",
    "        self.drop1 = nn.Dropout(p = args.dropout)\n",
    "        self.fc2 = nn.Linear(args.d_h, args.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1, self.window_size * self.embedding_size)\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = self.drop1(h)\n",
    "        x = self.fc2(h)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiamos algunos hiperparámetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model hyperparameters\n",
    "#Vocabulary size\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "#Word embeddings dimension\n",
    "args.d = emb_d\n",
    "#Hidden layer dimension\n",
    "args.d_h = 200 #Está bien puestp que emb_d es 100\n",
    "#Dropout\n",
    "args.dropout = 0.1\n",
    "\n",
    "#Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "#Scheduler hyperparameters\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "#Saving hyperparameters\n",
    "args.savedir = pth + 'model_emb'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "#Create model\n",
    "model_emb = NeuralLM_embeding(args=args, emb=embedding)\n",
    "\n",
    "#Send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "#Loss, Optimizer and Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer, \n",
    "                                                       mode = 'min',\n",
    "                                                       factor = args.lr_factor,\n",
    "                                                       patience = args.lr_patience,\n",
    "                                                       verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa de entrenamiento del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-e7244224207f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-7bb5ab4537e7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1916\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "#Training\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model.train()\n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        #Forward pass\n",
    "        outputs = model(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "\n",
    "        #Get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "\n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #Metrics in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Metrics in validation dataset\n",
    "    model.eval()\n",
    "    tuning_metric = model_eval(val_loader, model, gpu=args.use_gpu)\n",
    "    mean_epoch_metric = np.mean(tuning_metric)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "\n",
    "    #Check metric improvement\n",
    "    is_improve = tuning_metric > best_metric\n",
    "    if is_improve:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "\n",
    "    #Save best model\n",
    "    save_checkpoint({'epoch'       : epoch+1,\n",
    "                     'state_dict'  : model.state_dict(),\n",
    "                     'optimizer'   : optimizer.state_dict(),\n",
    "                     'scheduler'   : scheduler.state_dict(),\n",
    "                     'best_metric' : best_metric}, is_improve, args.savedir)\n",
    "    \n",
    "    #Early stoping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print('No improvement. Breaking out of loop.')\n",
    "        break\n",
    "    print('Train accuracy: ', mean_epoch_metric)\n",
    "    print('Epoch [{}/{}]: Loss = {:.4f}, Val Acurracy = {:.4f}, Epoch time = {:.2f}'.\n",
    "          format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time()-epoch_start_time)))\n",
    "    \n",
    "print('---------- %s seconds ---------' % time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2927, -0.5446,  0.1714],\n",
       "         [ 0.0929,  0.1798,  0.0761],\n",
       "         [-1.8047,  0.3559,  1.4479]],\n",
       "\n",
       "        [[ 0.2927, -0.5446,  0.1714],\n",
       "         [ 0.0929,  0.1798,  0.0761],\n",
       "         [-1.8047,  0.3559,  1.4479]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = nn.Embedding(10,3)\n",
    "e(torch.tensor([[1,2,3],[1,2,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([ -1.0961,  -1.0184,  -1.8950,  -1.9984,   4.2174,   6.2988,  -0.3170,\n",
       "           -1.5824,   2.4694,   2.3756,  -1.1883,  -1.4300,  -0.6092,   0.6152,\n",
       "            1.1048,   0.8866,   2.0087,  -1.8453,   3.2080,  -0.1376,  -4.1967,\n",
       "           -1.6238,   3.1863,   0.6211,   1.6738,   1.9099,   1.2532,   3.1336,\n",
       "            3.2359,  -4.5080,   1.6890,   2.0606,  -0.0722,   2.1841,   6.5726,\n",
       "           -0.1412,  -1.5676,   0.1572,   3.5281,  -0.0361,   1.6923,   0.3356,\n",
       "           -1.2028,   5.2517,  -1.0029,   2.4605,  -4.6152,   3.2214,   0.1949,\n",
       "            1.4327,   1.9920,  -0.3237,  -2.1740,   0.9460,  -3.8526,   3.9340,\n",
       "            1.6828,   4.4286,  -0.1994,   1.8463,  -2.1925, -10.4011,  -2.1528,\n",
       "            2.8122,  -3.9068,   0.2875,  -0.2148,   5.8414,   0.1621,  -1.0202,\n",
       "           -3.9861,  -3.2815,   0.9894,  -0.5335,  -1.1370,   0.7730,   0.8782,\n",
       "            1.1881,  -0.6197,   6.8253,   3.5462,   2.0978,   0.7641,  -2.7847,\n",
       "           -0.8488,   2.3285,   1.4681,   1.9173,  -0.1934,   0.4048,   0.7009,\n",
       "            0.5112,   1.3233,   1.1322,   2.9132,  -3.3316,   5.0304,   0.8276,\n",
       "           -0.1572,  -0.2303], dtype=torch.float64),\n",
       "  tensor([-1.6417,  1.4477, -2.2832, -1.9652, -0.2229,  5.1052, -0.1207, -0.1268,\n",
       "          -3.1773, -3.4544, -0.9431, -0.0945, -1.1894, -0.8121, -2.5730, -0.6139,\n",
       "          -2.3118,  1.0510,  5.6347, -5.8270,  1.2376,  1.0716,  3.8221,  2.3954,\n",
       "           0.1699,  3.2568,  2.8973,  3.2748, -2.9364,  0.2720, -1.0295, -2.6173,\n",
       "          -1.8071,  1.7376,  0.3391,  3.9329,  1.5714, -4.1001,  4.1568,  1.1624,\n",
       "          -0.5523, -0.5859, -4.7672,  0.2533, -1.1242, -0.1151, -5.6066,  2.9766,\n",
       "           4.4260,  1.0199,  3.7607, -2.2983,  4.4166, -1.3840, -1.8625,  0.3991,\n",
       "          -1.0969, -2.2860,  2.9928,  0.0440,  3.7624, -6.5231,  0.6213,  2.6418,\n",
       "          -1.9243, -1.1412, -3.8318,  0.5496,  2.2608, -1.3184, -1.1347, -3.7882,\n",
       "          -0.7750,  3.9567, -3.5794, -4.4237,  4.5057,  0.7191, -1.3996,  3.0972,\n",
       "           0.1075,  2.8299, -0.7602, -0.2772, -1.2257,  2.1579,  0.5184,  3.4380,\n",
       "          -1.0875, -1.5299,  0.3995,  0.8874, -1.9949,  2.9122,  3.2580, -2.6000,\n",
       "          -1.9951, -0.1765,  3.9469, -2.7925], dtype=torch.float64),\n",
       "  tensor([ 2.0749, -3.6219,  2.7962,  0.9734,  0.8236,  1.0387, -1.9435,  0.9390,\n",
       "           3.1465,  2.5053, -3.3736,  8.0420,  0.2890,  3.2729, -0.4777, -2.7176,\n",
       "          -3.7375,  1.4382,  3.4500, -4.6847,  3.8147, -1.8224, -0.4273,  2.7510,\n",
       "          -0.8123, -0.9013,  0.3879,  2.1081,  3.4685, -1.6470, -0.2772,  0.3224,\n",
       "           6.4958,  2.4072,  5.2072, -0.3711,  1.1508, -3.0352,  2.2160,  4.2224,\n",
       "          -0.9467,  0.7337, -1.5822,  7.8573,  0.6339, -1.4653, -5.4824,  3.3949,\n",
       "          -4.4926,  3.3945,  2.2128,  1.7211, -3.3620,  0.8859, -8.6017,  0.7469,\n",
       "           1.8434, -0.9842, -0.5781, -5.7163,  2.3754, -5.1170, -1.2832, -1.8492,\n",
       "          -3.7761,  4.3630,  3.4920,  6.0767, -0.7321,  1.6587,  2.2281, -2.8280,\n",
       "           1.1835, -0.2994, -0.1235, -1.5035,  1.7021, -2.2414,  2.5903,  3.5688,\n",
       "          -0.5472,  5.0692, -3.2299, -4.0191, -0.1603, -0.9015, -4.5494,  6.0245,\n",
       "          -0.6503,  2.4946, -1.4276,  0.1534,  3.7343,  1.0262,  7.8431,  2.1719,\n",
       "           2.4541, -1.7587,  1.0061,  0.2191], dtype=torch.float64)],\n",
       " [tensor([ -1.0961,  -1.0184,  -1.8950,  -1.9984,   4.2174,   6.2988,  -0.3170,\n",
       "           -1.5824,   2.4694,   2.3756,  -1.1883,  -1.4300,  -0.6092,   0.6152,\n",
       "            1.1048,   0.8866,   2.0087,  -1.8453,   3.2080,  -0.1376,  -4.1967,\n",
       "           -1.6238,   3.1863,   0.6211,   1.6738,   1.9099,   1.2532,   3.1336,\n",
       "            3.2359,  -4.5080,   1.6890,   2.0606,  -0.0722,   2.1841,   6.5726,\n",
       "           -0.1412,  -1.5676,   0.1572,   3.5281,  -0.0361,   1.6923,   0.3356,\n",
       "           -1.2028,   5.2517,  -1.0029,   2.4605,  -4.6152,   3.2214,   0.1949,\n",
       "            1.4327,   1.9920,  -0.3237,  -2.1740,   0.9460,  -3.8526,   3.9340,\n",
       "            1.6828,   4.4286,  -0.1994,   1.8463,  -2.1925, -10.4011,  -2.1528,\n",
       "            2.8122,  -3.9068,   0.2875,  -0.2148,   5.8414,   0.1621,  -1.0202,\n",
       "           -3.9861,  -3.2815,   0.9894,  -0.5335,  -1.1370,   0.7730,   0.8782,\n",
       "            1.1881,  -0.6197,   6.8253,   3.5462,   2.0978,   0.7641,  -2.7847,\n",
       "           -0.8488,   2.3285,   1.4681,   1.9173,  -0.1934,   0.4048,   0.7009,\n",
       "            0.5112,   1.3233,   1.1322,   2.9132,  -3.3316,   5.0304,   0.8276,\n",
       "           -0.1572,  -0.2303], dtype=torch.float64),\n",
       "  tensor([-1.6417,  1.4477, -2.2832, -1.9652, -0.2229,  5.1052, -0.1207, -0.1268,\n",
       "          -3.1773, -3.4544, -0.9431, -0.0945, -1.1894, -0.8121, -2.5730, -0.6139,\n",
       "          -2.3118,  1.0510,  5.6347, -5.8270,  1.2376,  1.0716,  3.8221,  2.3954,\n",
       "           0.1699,  3.2568,  2.8973,  3.2748, -2.9364,  0.2720, -1.0295, -2.6173,\n",
       "          -1.8071,  1.7376,  0.3391,  3.9329,  1.5714, -4.1001,  4.1568,  1.1624,\n",
       "          -0.5523, -0.5859, -4.7672,  0.2533, -1.1242, -0.1151, -5.6066,  2.9766,\n",
       "           4.4260,  1.0199,  3.7607, -2.2983,  4.4166, -1.3840, -1.8625,  0.3991,\n",
       "          -1.0969, -2.2860,  2.9928,  0.0440,  3.7624, -6.5231,  0.6213,  2.6418,\n",
       "          -1.9243, -1.1412, -3.8318,  0.5496,  2.2608, -1.3184, -1.1347, -3.7882,\n",
       "          -0.7750,  3.9567, -3.5794, -4.4237,  4.5057,  0.7191, -1.3996,  3.0972,\n",
       "           0.1075,  2.8299, -0.7602, -0.2772, -1.2257,  2.1579,  0.5184,  3.4380,\n",
       "          -1.0875, -1.5299,  0.3995,  0.8874, -1.9949,  2.9122,  3.2580, -2.6000,\n",
       "          -1.9951, -0.1765,  3.9469, -2.7925], dtype=torch.float64),\n",
       "  tensor([ 2.0749, -3.6219,  2.7962,  0.9734,  0.8236,  1.0387, -1.9435,  0.9390,\n",
       "           3.1465,  2.5053, -3.3736,  8.0420,  0.2890,  3.2729, -0.4777, -2.7176,\n",
       "          -3.7375,  1.4382,  3.4500, -4.6847,  3.8147, -1.8224, -0.4273,  2.7510,\n",
       "          -0.8123, -0.9013,  0.3879,  2.1081,  3.4685, -1.6470, -0.2772,  0.3224,\n",
       "           6.4958,  2.4072,  5.2072, -0.3711,  1.1508, -3.0352,  2.2160,  4.2224,\n",
       "          -0.9467,  0.7337, -1.5822,  7.8573,  0.6339, -1.4653, -5.4824,  3.3949,\n",
       "          -4.4926,  3.3945,  2.2128,  1.7211, -3.3620,  0.8859, -8.6017,  0.7469,\n",
       "           1.8434, -0.9842, -0.5781, -5.7163,  2.3754, -5.1170, -1.2832, -1.8492,\n",
       "          -3.7761,  4.3630,  3.4920,  6.0767, -0.7321,  1.6587,  2.2281, -2.8280,\n",
       "           1.1835, -0.2994, -0.1235, -1.5035,  1.7021, -2.2414,  2.5903,  3.5688,\n",
       "          -0.5472,  5.0692, -3.2299, -4.0191, -0.1603, -0.9015, -4.5494,  6.0245,\n",
       "          -0.6503,  2.4946, -1.4276,  0.1534,  3.7343,  1.0262,  7.8431,  2.1719,\n",
       "           2.4541, -1.7587,  1.0061,  0.2191], dtype=torch.float64)]]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(torch.tensor([[1,2,3],[1,2,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(windows):\n",
    "    emb = []\n",
    "    for words in windows:\n",
    "        w_emb = []\n",
    "        for w in words:\n",
    "            w_emb.append(emb_dict[ngram_data.id2w[w.item()]])\n",
    "        emb.append(w_emb)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
