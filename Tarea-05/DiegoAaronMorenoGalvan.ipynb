{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BRaP_x0eoI1"
   },
   "source": [
    "# NLP. Tarea 5: Modelo del Lenguaje Neuronal.\n",
    "\n",
    "**Diego Moreno**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Modelo Neuronal a nivel de caracter.\n",
    "\n",
    "Importamos librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5WWvA-6sets1"
   },
   "outputs": [],
   "source": [
    "# Tools\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from typing import Tuple\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "# Preprocesing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import  TweetTokenizer\n",
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Pytorch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Scikitlearn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bqN6c3B-f7_h"
   },
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Yb4ys6nJgPYJ"
   },
   "outputs": [],
   "source": [
    "pth = ''\n",
    "X_train = pd.read_csv(pth+'mex_train.txt', sep='\\r\\n',  engine='python', header=None).loc[:,0].values.tolist()\n",
    "X_val = pd.read_csv(pth+'mex_val.txt', sep='\\r\\n',  engine='python', header=None).loc[:,0].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nivel de caracter, tenemos que fijarnos en una ventana de 6 o más:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Jj3gCUEXhlGu"
   },
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.N = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase de N-gramas se quedará igual pues la estrategia será solamente cambiar el tokenizador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2T9J65oaiILS"
   },
   "outputs": [],
   "source": [
    "class NgramData():\n",
    "    def __init__(self, N: int, vocab_max: int=5000, tokenizer=None, embedding_model=None):\n",
    "        self.tokenizer = tokenizer if tokenizer else self.default_tokenizer\n",
    "        self.punct = set(['.',',',';',':','-','^','«','»','\"','!','¡','?','¿','\\'','...','<url>','*','@usuario'])\n",
    "        self.N = N\n",
    "        self.vocab_max = vocab_max\n",
    "        self.UNK = '<unk>'\n",
    "        self.SOS = '<s>'\n",
    "        self.EOS = '</s>'\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "    def default_tokenizer(self, doc: str) -> list:\n",
    "        return doc.split(' ')\n",
    "\n",
    "    def get_vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def remove_word(self, word: str) -> bool:\n",
    "        word = word.lower()\n",
    "        is_punct = True if word in self.punct else False\n",
    "        is_digit = word.isnumeric()\n",
    "        return is_punct or is_digit\n",
    "\n",
    "    def get_vocab(self, corpus: list) -> set:\n",
    "        freq_dist = FreqDist([w.lower() for sent in corpus \\\n",
    "                              for w in self.tokenizer(sent) \\\n",
    "                              if not self.remove_word(w)])\n",
    "        sorted_words = self.sortFreqDict(freq_dist)[:self.vocab_max-3]\n",
    "        return set(sorted_words)\n",
    "\n",
    "    def sortFreqDict(self, freq_dist) -> list:\n",
    "        freq_dist = dict(freq_dist)\n",
    "        return sorted(freq_dist, key=freq_dist.get, reverse=True)\n",
    "\n",
    "    def fit(self, corpus: list) -> None:\n",
    "        self.vocab = self.get_vocab(corpus)\n",
    "        self.vocab.add(self.UNK)\n",
    "        self.vocab.add(self.SOS)\n",
    "        self.vocab.add(self.EOS)\n",
    "\n",
    "        self.w2id = {}\n",
    "        self.id2w = {}\n",
    "\n",
    "        if self.embedding_model is not None:\n",
    "            self.embedding_matrix = np.empty([len(self.vocab), self.embedding_model.vector_size])\n",
    "\n",
    "        ID = 0\n",
    "        for doc in corpus:\n",
    "            for word in self.tokenizer(doc):\n",
    "                word_ = word.lower()\n",
    "                if word_ in self.vocab and not word_ in self.w2id:\n",
    "                    self.w2id[word_] = ID\n",
    "                    self.id2w[ID] = word_\n",
    "                    if self.embedding_model is not None:\n",
    "                        if word_ in self.embedding_model.emb_dict:\n",
    "                            self.embedding_matrix[ID] = self.embedding_model.emb_dict[word_]\n",
    "                        else:\n",
    "                            self.embedding_matrix[ID] = np.random.rand(self.embedding_model.vector_size) \n",
    "                    ID += 1\n",
    "        #Special tokens  \n",
    "        self.w2id.update({self.UNK: ID, \n",
    "                          self.SOS: ID+1,\n",
    "                          self.EOS: ID+2})  \n",
    "        self.id2w.update({ID  : self.UNK, \n",
    "                          ID+1: self.SOS,\n",
    "                          ID+2: self.EOS})\n",
    "    \n",
    "    def replace_unk(self, doc_tokens: list) -> list: \n",
    "        for i, token in enumerate(doc_tokens):\n",
    "            if token.lower() not in self.vocab:\n",
    "                doc_tokens[i] = self.UNK\n",
    "        return doc_tokens\n",
    "\n",
    "\n",
    "    def get_ngram_doc(self, doc:str) -> list:\n",
    "        doc_tokens = self.tokenizer(doc)\n",
    "        doc_tokens = self.replace_unk(doc_tokens)\n",
    "        doc_tokens = [w.lower() for w in doc_tokens]\n",
    "        doc_tokens = [self.SOS]*(self.N - 1) + doc_tokens + [self.EOS]\n",
    "        return list(ngrams(doc_tokens, self.N))\n",
    "    \n",
    "    def transform(self, corpus: list) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        X_ngrams = []\n",
    "        y = []\n",
    "        for doc in corpus:\n",
    "            doc_ngram = self.get_ngram_doc(doc)\n",
    "            for words_window in doc_ngram:\n",
    "                words_window_ids = [self.w2id[w] for w in words_window]\n",
    "                X_ngrams.append(list(words_window_ids[:-1]))\n",
    "                y.append(words_window_ids[-1])\n",
    "        return np.array(X_ngrams), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el nuevo tokenizador a nivel de caracter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "igtUbF-UtA5e"
   },
   "outputs": [],
   "source": [
    "def CharTokenizer(doc: str) -> list:\n",
    "    l = []\n",
    "    for c in doc:\n",
    "        l.append(c)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos nivel de caracteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_level = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ukx1t0n8h12F"
   },
   "outputs": [],
   "source": [
    "if char_level:\n",
    "    tk = CharTokenizer\n",
    "else:\n",
    "    tk = TweetTokenizer()\n",
    "    tk = tk.tokenize\n",
    "    \n",
    "ngram_data = NgramData(args.N, 5000, tk)\n",
    "ngram_data.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OH2bHAWHqJ7w",
    "outputId": "1bd5b974-1b17-494c-8e8f-346fbc98c220"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 344\n"
     ]
    }
   ],
   "source": [
    "print('Vocab Size:', ngram_data.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos los datos transformados y los loader de los mismos para entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Hl5NY_wFqZPt"
   },
   "outputs": [],
   "source": [
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2PH5JSUbwX6O"
   },
   "outputs": [],
   "source": [
    "#Batch size\n",
    "args.batch_size = 64\n",
    "#Number workers\n",
    "args.num_workers = 2\n",
    "\n",
    "#Train\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = True)\n",
    "\n",
    "#Validation\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CxrtxytHyIAR",
    "outputId": "baebf0d3-f72b-4e23-e202-0fb96ad0d2ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([64, 5])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print('X shape:', batch[0].shape)\n",
    "print('y shape:', batch[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase del modelo neuronal, también se incluye modificación para el caso de inicializar con un embedding preentrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "hXQa9kP8VMFU"
   },
   "outputs": [],
   "source": [
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, args, emb=None):\n",
    "        super(NeuralLM, self).__init__()\n",
    "\n",
    "        self.window_size = args.N - 1\n",
    "        self.embedding_size = args.d\n",
    "        \n",
    "        if emb == None:\n",
    "            self.emb = nn.Embedding(args.vocab_size, args.d)\n",
    "        else:\n",
    "            self.emb = nn.Embedding(emb.vocab_size,# emb.vector_size,\n",
    "                                    #args.vocab_size, \n",
    "                                    args.d,\n",
    "                                    _weight=torch.Tensor(emb.embedding_matrix))\n",
    "        self.fc1 = nn.Linear(args.d * (args.N - 1), args.d_h)\n",
    "        self.drop1 = nn.Dropout(p = args.dropout)\n",
    "        self.fc2 = nn.Linear(args.d_h, args.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1, self.window_size * self.embedding_size)\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = self.drop1(h)\n",
    "        x = self.fc2(h)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para el entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0vZZwQA1XZ8_"
   },
   "outputs": [],
   "source": [
    "def get_preds(raw_logits):\n",
    "    probs = F.softmax(raw_logits.detach(), dim=1)\n",
    "    y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    return y_pred\n",
    "\n",
    "def model_eval(data, model, gpu=False):\n",
    "    with torch.no_grad():\n",
    "        preds, tgts = [], []\n",
    "        for window_words, labels in data:\n",
    "            if gpu:\n",
    "                window_words = window_words.cuda()\n",
    "            outputs = model(window_words)\n",
    "\n",
    "            #Predictions\n",
    "            y_pred = get_preds(outputs)\n",
    "            tgt = labels.numpy()\n",
    "            tgts.append(tgt)\n",
    "            preds.append(y_pred)\n",
    "    tgts = [e for l in tgts for e in l]\n",
    "    preds = [e for l in preds for e in l]\n",
    "    return accuracy_score(tgts, preds)\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint_path, filename='checkpoint.pt'):\n",
    "    filename = os.path.join(checkpoint_path, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, os.path.join(checkpoint_path, 'model_best.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos hiperparámetros que tendrán que ser modificados posteriormente al usar un nuevo embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Dnk0A8MmXzRr"
   },
   "outputs": [],
   "source": [
    "#Model hyperparameters\n",
    "#Vocabulary size\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "#Word embeddings dimension\n",
    "args.d = 100\n",
    "#Hidden layer dimension\n",
    "args.d_h = 200\n",
    "#Dropout\n",
    "args.dropout = 0.1\n",
    "\n",
    "#Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "#Scheduler hyperparameters\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "#Saving hyperparameters\n",
    "args.savedir = pth + 'model'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "#Create model\n",
    "model = NeuralLM(args=args)\n",
    "\n",
    "#Send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "#Loss, Optimizer and Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer, \n",
    "                                                       mode = 'min',\n",
    "                                                       factor = args.lr_factor,\n",
    "                                                       patience = args.lr_patience,\n",
    "                                                       verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa de entrenamiento del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 902
    },
    "id": "m5w33lulaO27",
    "outputId": "86e4a066-cdcd-4533-caad-4a110bf51600"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.4386065422287932\n",
      "Epoch [1/100]: Loss = 1.9773, Val Acurracy = 0.4386, Epoch time = 31.76\n",
      "Train accuracy:  0.46004435409351324\n",
      "Epoch [2/100]: Loss = 1.8176, Val Acurracy = 0.4600, Epoch time = 30.50\n",
      "Train accuracy:  0.4607835889854001\n",
      "Epoch [3/100]: Loss = 1.7673, Val Acurracy = 0.4608, Epoch time = 30.55\n",
      "Train accuracy:  0.4825540565514692\n",
      "Epoch [4/100]: Loss = 1.7373, Val Acurracy = 0.4826, Epoch time = 30.89\n",
      "Train accuracy:  0.4729624838292367\n",
      "Epoch [5/100]: Loss = 1.7156, Val Acurracy = 0.4730, Epoch time = 31.02\n",
      "Train accuracy:  0.4506006283496581\n",
      "Epoch [6/100]: Loss = 1.7002, Val Acurracy = 0.4506, Epoch time = 30.81\n",
      "Train accuracy:  0.47444095361301053\n",
      "Epoch [7/100]: Loss = 1.6873, Val Acurracy = 0.4744, Epoch time = 30.63\n",
      "Train accuracy:  0.4743300683792275\n",
      "Epoch [8/100]: Loss = 1.6764, Val Acurracy = 0.4743, Epoch time = 30.30\n",
      "Train accuracy:  0.4886712252818333\n",
      "Epoch [9/100]: Loss = 1.6685, Val Acurracy = 0.4887, Epoch time = 32.59\n",
      "Train accuracy:  0.4830900018480872\n",
      "Epoch [10/100]: Loss = 1.6614, Val Acurracy = 0.4831, Epoch time = 33.33\n",
      "Train accuracy:  0.48443910552578084\n",
      "Epoch [11/100]: Loss = 1.6551, Val Acurracy = 0.4844, Epoch time = 32.53\n",
      "Epoch    12: reducing learning rate of group 0 to 1.1500e-01.\n",
      "Train accuracy:  0.49240436148586214\n",
      "Epoch [12/100]: Loss = 1.6499, Val Acurracy = 0.4924, Epoch time = 32.74\n",
      "Train accuracy:  0.5046386989465903\n",
      "Epoch [13/100]: Loss = 1.6107, Val Acurracy = 0.5046, Epoch time = 32.69\n",
      "Train accuracy:  0.5072445019404916\n",
      "Epoch [14/100]: Loss = 1.6055, Val Acurracy = 0.5072, Epoch time = 33.59\n",
      "Train accuracy:  0.5064683053040103\n",
      "Epoch [15/100]: Loss = 1.6023, Val Acurracy = 0.5065, Epoch time = 33.00\n",
      "Train accuracy:  0.5022916281648494\n",
      "Epoch [16/100]: Loss = 1.6014, Val Acurracy = 0.5023, Epoch time = 31.56\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-e7244224207f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtraining_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "#Training\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model.train()\n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        #Forward pass\n",
    "        outputs = model(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "\n",
    "        #Get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "\n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #Metrics in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Metrics in validation dataset\n",
    "    model.eval()\n",
    "    tuning_metric = model_eval(val_loader, model, gpu=args.use_gpu)\n",
    "    mean_epoch_metric = np.mean(tuning_metric)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "\n",
    "    #Check metric improvement\n",
    "    is_improve = tuning_metric > best_metric\n",
    "    if is_improve:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "\n",
    "    #Save best model\n",
    "    save_checkpoint({'epoch'       : epoch+1,\n",
    "                     'state_dict'  : model.state_dict(),\n",
    "                     'optimizer'   : optimizer.state_dict(),\n",
    "                     'scheduler'   : scheduler.state_dict(),\n",
    "                     'best_metric' : best_metric}, is_improve, args.savedir)\n",
    "    \n",
    "    #Early stoping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print('No improvement. Breaking out of loop.')\n",
    "        break\n",
    "    print('Train accuracy: ', mean_epoch_metric)\n",
    "    print('Epoch [{}/{}]: Loss = {:.4f}, Val Acurracy = {:.4f}, Epoch time = {:.2f}'.\n",
    "          format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time()-epoch_start_time)))\n",
    "    \n",
    "print('---------- %s seconds ---------' % time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejor modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(344, 100)\n",
       "  (fc1): Linear(in_features=500, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=344, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model with learned embeddings\n",
    "best_model = NeuralLM(args)\n",
    "best_model.load_state_dict(torch.load(pth+'model/model_best.pt')['state_dict'])\n",
    "best_model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Generamos texto 3 veces con máximo de 300 caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "gziHZsIQv0AO"
   },
   "outputs": [],
   "source": [
    "lenght_max = 300\n",
    "\n",
    "def parse_text(text, tokenizer):\n",
    "    all_tokens = [w.lower() if w in ngram_data.w2id else '<unk>' for w in tokenizer(text)]\n",
    "    token_ids = [ngram_data.w2id[word.lower()] for word in all_tokens]\n",
    "    return all_tokens, token_ids\n",
    "\n",
    "def sample_next_word(logits, temperature=1.):\n",
    "    logits = np.asarray(logits).astype('float64')\n",
    "    preds = logits/temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probs = np.random.multinomial(1, preds)\n",
    "    return np.argmax(probs)\n",
    "\n",
    "def predict_next_token(model, token_ids):\n",
    "    words_ids_tensor = torch.LongTensor(token_ids).unsqueeze(0)\n",
    "    y_raw_pred = model(words_ids_tensor).squeeze(0).detach().numpy()\n",
    "    y_pred = sample_next_word(y_raw_pred, 1.)\n",
    "    return y_pred\n",
    "\n",
    "def generate_sentence(model, initial_text, tokenizer):\n",
    "    all_tokens, window_word_ids = parse_text(initial_text, tokenizer)\n",
    "    for i in range(lenght_max):\n",
    "        y_pred = predict_next_token(best_model, window_word_ids)\n",
    "        next_word = ngram_data.id2w[y_pred]\n",
    "        all_tokens.append(next_word)\n",
    "        if next_word == '</s>':\n",
    "            break\n",
    "        else:\n",
    "            window_word_ids.pop(0)\n",
    "            window_word_ids.append(y_pred)\n",
    "    if char_level:\n",
    "        return ''.join(all_tokens)\n",
    "    else:\n",
    "        return ' '.join(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "UbXJ642z--mB",
    "outputId": "8a33b6a5-6361-4948-dbcd-a528e9ca0708"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s><s putos perder a oierencanas anuncia un putos<unk> 😊🏻🖕🏻</s>'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = '<s><s'\n",
    "print('Learned embeddings')\n",
    "print('¯'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'estoy que vale verga<unk></s>'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = 'estoy'\n",
    "print('Learned embeddings')\n",
    "print('¯'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yo opulidado el pubarmos cleto 😈#pasibron wey que esputo es elevar con tantar tienen llegué me dinerente nomar nada ti a habes <unk><unk> vienen como hdp sestrea</s>'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = 'yo op'\n",
    "print('Learned embeddings')\n",
    "print('¯'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que genera texto bueno, sin embargo, como es a nivel caracter, algunas palabras no tienen un significado sin embargo, el sentido se preserva. Puede que esto mejore al expandir la ventana a más de 6 caracteres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Escribimos 5 ejemplos de oraciones y medimos su verosimilitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "id": "au6ek-AlDTwd"
   },
   "outputs": [],
   "source": [
    "def log_likelihood(model, text, ngram_model):\n",
    "    # Generate n-gram windows from input text and the respective label y\n",
    "    X, y = ngram_data.transform([text])\n",
    "    # Discard first two n-gram windows since they contain '<s>' tokens not necessary\n",
    "    X, y = X[2:], y[2:]\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "\n",
    "    logits = model(X).detach()\n",
    "    probs = F.softmax(logits, dim = 1).numpy()\n",
    "\n",
    "    return np.sum([np.log(probs[i][w]) for i, w in enumerate(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RHMEZCGENCL5",
    "outputId": "86aee787-4cca-45f8-8e2b-36e100eb48a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -58.841915\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model, \n",
    "                                        'La clase de lenguaje está muy padre', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jr4XFLiHNCOl",
    "outputId": "4f76dda4-d895-439b-832b-44a0e154bf01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -61.371037\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model,\n",
    "                                        'La clase de lenguaje está muy chida',\n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5c7tbaKNCRP",
    "outputId": "3f32376f-8c1f-449f-f8de-031191a56234"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -69.30573\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model,\n",
    "                                        'La clase de lenguaje está muy guay', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -96.00549\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model,\n",
    "                                        'La clase de procesamiento del lenguaje está muy padre', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -56.34829\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model,\n",
    "                                        'La clase de lenguaje está muy madre', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparamos que tienen sentido los resultados pues la palabra padre es más usada que chida y mucho más que guay., entre más palabras le pongamos menor será su verosimilitud como en el ejemplo 4. Sin embargo, logra fallar en el último ejemplo cuando decimos que está muy madre. En el español normal no se usa, no obstante, al haber sido entrenado con tuits groseros, se obtiene que es más probable decir que está muy madre. Otro dato curioso, es que al cambiar la palabra «está» por «esta», la verosimilitud disminuye, lo cual podría ser útil para correcciones ortográficas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4HQ2xLFNF8r"
   },
   "source": [
    "## 1.3. Estructuras morfológicas correctas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJYUOuUKNCT0",
    "outputId": "4ffff7c7-d8b2-48a9-bb1f-a9e2bd7b4389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.246503 chingada\n",
      "-5.246503 chingada\n",
      "-14.173836 dachinga\n",
      "-14.173836 dachinga\n",
      "-15.5406885 dgachina\n",
      "--------------------------------------------------\n",
      "-66.48084 acihdnag\n",
      "-67.964966 aacgdhni\n",
      "-67.964966 aacgdhni\n",
      "-68.05988 caagdhni\n",
      "-68.05988 caagdhni\n"
     ]
    }
   ],
   "source": [
    "if char_level:\n",
    "    word_list = 'chingada'\n",
    "    perms = [''.join(perm) for perm in permutations(word_list)]\n",
    "else:\n",
    "    word_list = 'sino gano me voy a la chingada'.split(' ')\n",
    "    perms = [' '.join(perm) for perm in permutations(word_list)]\n",
    "#print(len(perms))\n",
    "\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[:5]:\n",
    "    print(p, t)\n",
    "print('-'*50)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[-5:]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que hay dos chingadas debido a que se obtiene una de otra solo intercambiando las a's y similarmente para las demás palabras. Los resultados son los esperados pues la más probable es chingada, luego dachinga que contiene la palabra chinga y despues dgachina que contiene la palabra china. Las menos probables ni se pueden leer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Perplejidad en validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "BHbLjydkNCWU"
   },
   "outputs": [],
   "source": [
    "def perplexity(model, text, ngram_model):\n",
    "    # Generate n-gram windows from input text and the respective label y\n",
    "    X, y = ngram_data.transform([text])\n",
    "    # Discard first two n-gram windows since they contain '<s>' tokens not necessary\n",
    "    X, y = X[2:], y[2:]\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "\n",
    "    logits = model(X).detach()\n",
    "    probs = F.softmax(logits, dim = 1).numpy()\n",
    "    \n",
    "    ans = 1.\n",
    "    N = len(y)\n",
    "    print('Validation set dimension:', N)\n",
    "    probs = [(probs[i][w])**(1/N) for i, w in enumerate(y)]\n",
    "    for p in probs:\n",
    "        ans /= p\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "8wbHjIcdNCYS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set dimension: 615\n",
      "Validation set perplexity: 1.4799053402082503\n"
     ]
    }
   ],
   "source": [
    "print('Validation set perplexity:', perplexity(best_model,\n",
    "                                               X_val, \n",
    "                                               ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Modelo de lenguaje neuronal inicializado con embedding dado.\n",
    "Leemos el embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = ''\n",
    "emb_txt = pd.read_csv(pth+'word2vec_col.txt',\n",
    "                        sep='\\r\\n', engine='python', \n",
    "                        header=None).loc[:,0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_d = int(emb_txt[0].split()[1])\n",
    "emb_N = int(emb_txt[0].split()[0])\n",
    "emb_txt = emb_txt[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el diccionario del embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict = {}\n",
    "for i in range(emb_N):\n",
    "    row_list = emb_txt[i].split()\n",
    "    emb_dict[row_list[0]] = np.array(row_list[1:]).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase del embedding nuevo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class embedding():\n",
    "    def __init__(self, emb_d, emb_dict):\n",
    "        self.emb_dict = emb_dict\n",
    "        self.id2w = {}\n",
    "        self.w2id = {}\n",
    "        self.embedding_matrix = np.empty([len(emb_dict), emb_d])\n",
    "        self.vector_size = emb_d\n",
    "        self.vocab_size = len(emb_dict)\n",
    "        for i, word in enumerate(emb_dict):\n",
    "            self.embedding_matrix[i,:] = emb_dict[word]\n",
    "            self.id2w[i] = word\n",
    "            self.w2id[word] = i\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        emb = []\n",
    "        for words in x:\n",
    "            w_emb = []\n",
    "            for w in words:\n",
    "                w_emb.append(self.embedding_matrix[w])\n",
    "            emb.append(w_emb)\n",
    "        return torch.Tensor(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usaremos nivel de palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_level = False\n",
    "args.N = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 5000\n"
     ]
    }
   ],
   "source": [
    "if char_level:\n",
    "    tk = CharTokenizer\n",
    "else:\n",
    "    tk = TweetTokenizer()\n",
    "    tk = tk.tokenize\n",
    "\n",
    "emb_model = embedding(emb_d, emb_dict)\n",
    "ngram_data = NgramData(args.N, 5000, tk, emb_model)\n",
    "ngram_data.fit(X_train)\n",
    "print('Vocab Size:', ngram_data.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el nuevo ngram_data con el embedding preinicializado:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a transformar los datos crear los loaders: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size\n",
    "args.batch_size = 64\n",
    "#Number workers\n",
    "args.num_workers = 2\n",
    "\n",
    "#Train\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = True)\n",
    "\n",
    "#Validation\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([64, 3])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print('X shape:', batch[0].shape)\n",
    "print('y shape:', batch[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiamos algunos hiperparámetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model hyperparameters\n",
    "#Vocabulary size\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "#Word embeddings dimension\n",
    "args.d = emb_d\n",
    "#Hidden layer dimension\n",
    "args.d_h = 200 #Está bien puestp que emb_d es 100\n",
    "#Dropout\n",
    "args.dropout = 0.1\n",
    "\n",
    "#Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "#Scheduler hyperparameters\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "#Saving hyperparameters\n",
    "args.savedir = pth + 'model_emb2'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "#Create model\n",
    "model_emb = NeuralLM(args=args, emb=emb_model)\n",
    "\n",
    "#Send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model_emb.cuda()\n",
    "\n",
    "#Loss, Optimizer and Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_emb.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer, \n",
    "                                                       mode = 'min',\n",
    "                                                       factor = args.lr_factor,\n",
    "                                                       patience = args.lr_patience,\n",
    "                                                       verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa de entrenamiento del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.16385976971988314\n",
      "Epoch [1/100]: Loss = 5.8367, Val Acurracy = 0.1639, Epoch time = 263.77\n",
      "Train accuracy:  0.17253823681044853\n",
      "Epoch [2/100]: Loss = 5.3963, Val Acurracy = 0.1725, Epoch time = 265.16\n",
      "Train accuracy:  0.1827633614023028\n",
      "Epoch [3/100]: Loss = 5.1734, Val Acurracy = 0.1828, Epoch time = 263.57\n",
      "Train accuracy:  0.20836913559030762\n",
      "Epoch [4/100]: Loss = 4.9939, Val Acurracy = 0.2084, Epoch time = 255.22\n",
      "Train accuracy:  0.16523457638769548\n",
      "Epoch [5/100]: Loss = 4.8421, Val Acurracy = 0.1652, Epoch time = 256.90\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-601dcf5586ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "#Training\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model_emb.train()\n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        #Forward pass\n",
    "        outputs = model_emb(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "\n",
    "        #Get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "\n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #Metrics in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Metrics in validation dataset\n",
    "    model_emb.eval()\n",
    "    tuning_metric = model_eval(val_loader, model_emb, gpu=args.use_gpu)\n",
    "    mean_epoch_metric = np.mean(tuning_metric)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "\n",
    "    #Check metric improvement\n",
    "    is_improve = tuning_metric > best_metric\n",
    "    if is_improve:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "\n",
    "    #Save best model\n",
    "    save_checkpoint({'epoch'       : epoch+1,\n",
    "                     'state_dict'  : model_emb.state_dict(),\n",
    "                     'optimizer'   : optimizer.state_dict(),\n",
    "                     'scheduler'   : scheduler.state_dict(),\n",
    "                     'best_metric' : best_metric}, is_improve, args.savedir)\n",
    "    \n",
    "    #Early stoping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print('No improvement. Breaking out of loop.')\n",
    "        break\n",
    "    print('Train accuracy: ', mean_epoch_metric)\n",
    "    print('Epoch [{}/{}]: Loss = {:.4f}, Val Acurracy = {:.4f}, Epoch time = {:.2f}'.\n",
    "          format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time()-epoch_start_time)))\n",
    "    \n",
    "print('---------- %s seconds ---------' % time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(973265, 100)\n",
       "  (fc1): Linear(in_features=300, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=5000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model with learned embeddings\n",
    "best_model = NeuralLM(args, emb_model)\n",
    "best_model.load_state_dict(torch.load(pth+'model_emb2/model_best.pt')['state_dict'])\n",
    "best_model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Palabras más similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_words(embeddings, ngram_data, word, n):\n",
    "    word_id = torch.LongTensor([ngram_data.w2id[word]])\n",
    "    word_embed = embeddings(word_id)\n",
    "    dists = torch.norm(embeddings.weight - word_embed, dim=1).detach()\n",
    "    lst = sorted(enumerate(dists.numpy()), key=lambda x : x[1])\n",
    "    for idx, diff in lst[1:n+1]:\n",
    "        print(emb_model.id2w[idx], diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      "favoritopic 19.523092\n",
      "preferido 19.648405\n",
      "favoritooo 22.36418\n",
      "favoritoo 22.412514\n",
      "predilecto 22.511961\n",
      "faborito 23.558441\n",
      "fabuloso 23.960974\n",
      "elmejor 23.98476\n",
      "favorite 24.176588\n",
      "favoritoooo 24.35437\n"
     ]
    }
   ],
   "source": [
    "print('Learned embeddings')\n",
    "print('¯'*20)\n",
    "print_closest_words(best_model.emb, ngram_data, 'lugar', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      "habrá 25.64563\n",
      "existe 25.68292\n",
      "tenemos 26.000948\n",
      "tienen 26.04076\n",
      "tendrán 26.042225\n",
      "sirven 26.308668\n",
      "existen 26.376514\n",
      "exista 26.764889\n",
      "tendríamos 26.82992\n",
      "sirve 27.166117\n"
     ]
    }
   ],
   "source": [
    "print('Learned embeddings')\n",
    "print('¯'*20)\n",
    "print_closest_words(best_model.emb, ngram_data, 'madre', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      "jente 27.215136\n",
      "gnte 27.30129\n",
      "gent 27.690762\n",
      "gnt 28.113308\n",
      "gentuza 28.855957\n",
      "gentee 29.477058\n",
      "hipocresía 29.829397\n",
      "tipa 30.148119\n",
      "gentr 30.306572\n",
      "gentepic 30.612246\n"
     ]
    }
   ],
   "source": [
    "print('Learned embeddings')\n",
    "print('¯'*20)\n",
    "print_closest_words(best_model.emb, ngram_data, 'chingada', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que en algunas palabras es buena la relación como chingada-gente-gentuza-tipa-hipocrecía, lugar-favorito-predilecto-preferido. Sin embargo en madre no existen muy buenas realiciones pues solo parece ser que se relacionan con servir para pura madre o tener madre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Generación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> <s> <s> <unk> dar <unk> chinga tu madre <unk> </s>'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = '<s> <s> <s>'\n",
    "print('Learned embeddings')\n",
    "print('¯'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> que no saben entiendo ya valen verga pero 😂 😂 😂 ❤ ya le traigo de la <unk> <unk> aqui de cagada <unk> <unk> </s>'"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = '<s> que no'\n",
    "print('Learned embeddings')\n",
    "print('¯'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> que pedo chingó 💜 en la mañana <unk> </s>'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = '<s> que pedo'\n",
    "print('Learned embeddings')\n",
    "print('¯'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que genera buena secuencia de palabras con la ventaja de que las palabras existen (ventaja sobre el modelo por caracteres). algunas tienen sentido, sin embargo, en muchas ocasiones se pierde el significado. Tener en cuenta que se entrenaron pocass epocas también porque se tardaba demasiado con el embeding gigantesco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. 5 ejemplos de oraciones y medimos su verosimilitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -37.39764\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model, \n",
    "                                        'La clase de lenguaje está muy padre', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -39.85654\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model, \n",
    "                                        'La clase de lenguaje esta muy padre', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -39.408936\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model, \n",
    "                                        'La clase de lenguaje está muy chida', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -15.099327\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model, \n",
    "                                        'Vamos para allá', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -18.496256\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model, \n",
    "                                        'Vamos para haya', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colocamos ejemplos en los que hay faltas de ortografía para notar si en realidad puede servir para corregir ortográficamente pues la parabra correcta es la que mejor debería quedar en el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Estructura sintácticas correctas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-20.249084 nosotros buscamos a una mujer\n",
      "-21.911526 buscamos nosotros a una mujer\n",
      "-22.509857 buscamos mujer a una nosotros\n",
      "-22.755405 buscamos una mujer a nosotros\n",
      "-22.98527 buscamos a una mujer nosotros\n",
      "--------------------------------------------------\n",
      "-34.871372 a buscamos nosotros mujer una\n",
      "-34.946426 mujer una nosotros buscamos a\n",
      "-35.016426 una mujer nosotros buscamos a\n",
      "-36.017612 a una nosotros buscamos mujer\n",
      "-36.558258 a una nosotros mujer buscamos\n"
     ]
    }
   ],
   "source": [
    "if char_level:\n",
    "    word_list = 'chingada'\n",
    "    perms = [''.join(perm) for perm in permutations(word_list)]\n",
    "else:\n",
    "    word_list = 'buscamos a una nosotros mujer'.split(' ')\n",
    "    perms = [' '.join(perm) for perm in permutations(word_list)]\n",
    "#print(len(perms))\n",
    "\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[:5]:\n",
    "    print(p, t)\n",
    "print('-'*50)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[-5:]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ejemplo propuesto fue puesto con la intención de que la palabra nosotros se relacionara con buscamos lo la conjugación y la palabra una con mujer. Lo cual fue satisfecho por el ordenamiento con verosimilitud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Perplejidad para los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, text, ngram_model):\n",
    "    # Generate n-gram windows from input text and the respective label y\n",
    "    X, y = ngram_data.transform(text)\n",
    "    # Discard first two n-gram windows since they contain '<s>' tokens not necessary\n",
    "    X, y = X[2:], y[2:]\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "\n",
    "    logits = model(X).detach()\n",
    "    probs = F.softmax(logits, dim = 1).numpy()\n",
    "    \n",
    "    ans = 1.\n",
    "    N = len(y)\n",
    "    print('Validation set dimension:', N)\n",
    "    probs = [(probs[i][w])**(1/N) for i, w in enumerate(y)]\n",
    "    for p in probs:\n",
    "        ans /= p\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set dimension: 11636\n",
      "Validation set perplexity: 197.58976210196224\n"
     ]
    }
   ],
   "source": [
    "print('Validation set perplexity:', perplexity(best_model,\n",
    "                                               X_val, \n",
    "                                               ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Comparamos con el modelo de clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 5000\n"
     ]
    }
   ],
   "source": [
    "if char_level:\n",
    "    tk = CharTokenizer\n",
    "else:\n",
    "    tk = TweetTokenizer()\n",
    "    tk = tk.tokenize\n",
    "\n",
    "emb_model = None\n",
    "ngram_data = NgramData(args.N, 5000, tk, emb_model)\n",
    "ngram_data.fit(X_train)\n",
    "print('Vocab Size:', ngram_data.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size\n",
    "args.batch_size = 64\n",
    "#Number workers\n",
    "args.num_workers = 2\n",
    "\n",
    "#Train\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = True)\n",
    "\n",
    "#Validation\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([64, 3])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print('X shape:', batch[0].shape)\n",
    "print('y shape:', batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model hyperparameters\n",
    "#Vocabulary size\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "#Word embeddings dimension\n",
    "args.d = 100\n",
    "#Hidden layer dimension\n",
    "args.d_h = 200\n",
    "#Dropout\n",
    "args.dropout = 0.1\n",
    "\n",
    "#Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "#Scheduler hyperparameters\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "#Saving hyperparameters\n",
    "args.savedir = pth + 'model_normal'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "#Create model\n",
    "model_normal = NeuralLM(args=args)\n",
    "\n",
    "#Send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model_emb.cuda()\n",
    "\n",
    "#Loss, Optimizer and Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_normal.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer, \n",
    "                                                       mode = 'min',\n",
    "                                                       factor = args.lr_factor,\n",
    "                                                       patience = args.lr_patience,\n",
    "                                                       verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.22194535143495445\n",
      "Epoch [1/100]: Loss = 5.5247, Val Acurracy = 0.2219, Epoch time = 29.94\n",
      "Train accuracy:  0.20295583433579653\n",
      "Epoch [2/100]: Loss = 5.0686, Val Acurracy = 0.2030, Epoch time = 29.92\n",
      "Train accuracy:  0.21876611101563842\n",
      "Epoch [3/100]: Loss = 4.8570, Val Acurracy = 0.2188, Epoch time = 30.31\n",
      "Train accuracy:  0.23002234060835194\n",
      "Epoch [4/100]: Loss = 4.6880, Val Acurracy = 0.2300, Epoch time = 30.61\n",
      "Train accuracy:  0.22675717477229765\n",
      "Epoch [5/100]: Loss = 4.5403, Val Acurracy = 0.2268, Epoch time = 30.57\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-220-20e06e13afa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m#Metrics in training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    108\u001b[0m                         \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             F.sgd(params_with_grad,\n\u001b[0m\u001b[1;32m    111\u001b[0m                   \u001b[0md_p_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                   \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "#Training\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model_normal.train()\n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        #Forward pass\n",
    "        outputs = model_normal(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "\n",
    "        #Get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "\n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #Metrics in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Metrics in validation dataset\n",
    "    model_normal.eval()\n",
    "    tuning_metric = model_eval(val_loader, model_normal, gpu=args.use_gpu)\n",
    "    mean_epoch_metric = np.mean(tuning_metric)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "\n",
    "    #Check metric improvement\n",
    "    is_improve = tuning_metric > best_metric\n",
    "    if is_improve:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "\n",
    "    #Save best model\n",
    "    save_checkpoint({'epoch'       : epoch+1,\n",
    "                     'state_dict'  : model_normal.state_dict(),\n",
    "                     'optimizer'   : optimizer.state_dict(),\n",
    "                     'scheduler'   : scheduler.state_dict(),\n",
    "                     'best_metric' : best_metric}, is_improve, args.savedir)\n",
    "    \n",
    "    #Early stoping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print('No improvement. Breaking out of loop.')\n",
    "        break\n",
    "    print('Train accuracy: ', mean_epoch_metric)\n",
    "    print('Epoch [{}/{}]: Loss = {:.4f}, Val Acurracy = {:.4f}, Epoch time = {:.2f}'.\n",
    "          format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time()-epoch_start_time)))\n",
    "    \n",
    "print('---------- %s seconds ---------' % time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo paramos de igual manera a las 5 epocas debido a que el caso anterior se tardaba mucho y solo se lograron entrenar 5 de ellas. Para hacer el ejercicio más comparable en perplejidad se realizó lo anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(5000, 100)\n",
       "  (fc1): Linear(in_features=300, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=5000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model with learned embeddings\n",
    "best_model = NeuralLM(args)\n",
    "best_model.load_state_dict(torch.load(pth+'model_normal/model_best.pt')['state_dict'])\n",
    "best_model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set dimension: 11636\n",
      "Validation set perplexity: 114.87373333780658\n"
     ]
    }
   ],
   "source": [
    "print('Validation set perplexity:', perplexity(best_model,\n",
    "                                               X_val, \n",
    "                                               ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuenta con una perplejidad menor el modelo simple, es decir, se ajusta mejor a datos de test. Esto puede ser debido a que con el embedding preinicializado se consideraron más palabras del vocabulario (todas las del embedding), esto agrega más complejidad y puede que sea más sobreajustado y por consiguiente menos ajustado a datos no vistos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Modelo con conexión directa entre embeddings y capa final.\n",
    "Modificamos la clase del modelo neuronal:\n",
    "\n",
    "La diferencia será en el forward. Recordar que el modelo de bengio está dado por $y=b+Wx+U\\tanh(d+Hx)$, lo que teníamos antes con el modelo del profesor era solamente la parte $U\\tanh(d+Hx)$, es por ello que en el forward debemos de agregar $Wx$ donde $W$ es otra capa entrenable para el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLM_conexion(nn.Module):\n",
    "    def __init__(self, args, emb=None):\n",
    "        super(NeuralLM, self).__init__()\n",
    "\n",
    "        self.window_size = args.N - 1\n",
    "        self.embedding_size = args.d\n",
    "        \n",
    "        if emb == None:\n",
    "            self.emb = nn.Embedding(args.vocab_size, args.d)\n",
    "        else:\n",
    "            self.emb = nn.Embedding(emb.vocab_size, args.d,\n",
    "                                    _weight=torch.Tensor(emb.embedding_matrix))\n",
    "        self.fc1 = nn.Linear(args.d * (args.N - 1), args.d_h)\n",
    "        self.drop1 = nn.Dropout(p = args.dropout)\n",
    "        self.fc2 = nn.Linear(args.d_h, args.vocab_size, bias=False)\n",
    "        self.fc3 = nn.Linear(args.d * (args.N - 1), args.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1, self.window_size * self.embedding_size)\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = self.drop1(h)\n",
    "        h = self.fc2(h)\n",
    "        w = self.fc3(x)\n",
    "        return w + h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos ahora este modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 5000\n"
     ]
    }
   ],
   "source": [
    "if char_level:\n",
    "    tk = CharTokenizer\n",
    "else:\n",
    "    tk = TweetTokenizer()\n",
    "    tk = tk.tokenize\n",
    "\n",
    "emb_model = None\n",
    "ngram_data = NgramData(args.N, 5000, tk, emb_model)\n",
    "ngram_data.fit(X_train)\n",
    "print('Vocab Size:', ngram_data.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size\n",
    "args.batch_size = 64\n",
    "#Number workers\n",
    "args.num_workers = 2\n",
    "\n",
    "#Train\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = True)\n",
    "\n",
    "#Validation\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([64, 3])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print('X shape:', batch[0].shape)\n",
    "print('y shape:', batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model hyperparameters\n",
    "#Vocabulary size\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "#Word embeddings dimension\n",
    "args.d = 100\n",
    "#Hidden layer dimension\n",
    "args.d_h = 200\n",
    "#Dropout\n",
    "args.dropout = 0.1\n",
    "\n",
    "#Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "#Scheduler hyperparameters\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "#Saving hyperparameters\n",
    "args.savedir = pth + 'model_conexion'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "#Create model\n",
    "model_conexion = NeuralLM(args=args)\n",
    "\n",
    "#Send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model_conexion.cuda()\n",
    "\n",
    "#Loss, Optimizer and Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_conexion.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer, \n",
    "                                                       mode = 'min',\n",
    "                                                       factor = args.lr_factor,\n",
    "                                                       patience = args.lr_patience,\n",
    "                                                       verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.20115140058429284\n",
      "Epoch [1/100]: Loss = 5.5270, Val Acurracy = 0.2012, Epoch time = 29.13\n",
      "Train accuracy:  0.21266540642722118\n",
      "Epoch [2/100]: Loss = 5.0725, Val Acurracy = 0.2127, Epoch time = 29.22\n",
      "Train accuracy:  0.191699604743083\n",
      "Epoch [3/100]: Loss = 4.8623, Val Acurracy = 0.1917, Epoch time = 29.36\n",
      "Train accuracy:  0.19805808558171506\n",
      "Epoch [4/100]: Loss = 4.6918, Val Acurracy = 0.1981, Epoch time = 29.29\n",
      "Train accuracy:  0.2263275476886063\n",
      "Epoch [5/100]: Loss = 4.5458, Val Acurracy = 0.2263, Epoch time = 29.27\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-75c535f7ccb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "#Training\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model_conexion.train()\n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        #Forward pass\n",
    "        outputs = model_conexion(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "\n",
    "        #Get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "\n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #Metrics in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Metrics in validation dataset\n",
    "    model_conexion.eval()\n",
    "    tuning_metric = model_eval(val_loader, model_conexion, gpu=args.use_gpu)\n",
    "    mean_epoch_metric = np.mean(tuning_metric)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "\n",
    "    #Check metric improvement\n",
    "    is_improve = tuning_metric > best_metric\n",
    "    if is_improve:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "\n",
    "    #Save best model\n",
    "    save_checkpoint({'epoch'       : epoch+1,\n",
    "                     'state_dict'  : model_conexion.state_dict(),\n",
    "                     'optimizer'   : optimizer.state_dict(),\n",
    "                     'scheduler'   : scheduler.state_dict(),\n",
    "                     'best_metric' : best_metric}, is_improve, args.savedir)\n",
    "    \n",
    "    #Early stoping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print('No improvement. Breaking out of loop.')\n",
    "        break\n",
    "    print('Train accuracy: ', mean_epoch_metric)\n",
    "    print('Epoch [{}/{}]: Loss = {:.4f}, Val Acurracy = {:.4f}, Epoch time = {:.2f}'.\n",
    "          format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time()-epoch_start_time)))\n",
    "    \n",
    "print('---------- %s seconds ---------' % time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(5000, 100)\n",
       "  (fc1): Linear(in_features=300, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=5000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model with learned embeddings\n",
    "best_model = NeuralLM(args)\n",
    "best_model.load_state_dict(torch.load(pth+'model_conexion/model_best.pt')['state_dict'])\n",
    "best_model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set dimension: 11636\n",
      "Validation set perplexity: 116.10106576906372\n"
     ]
    }
   ],
   "source": [
    "print('Validation set perplexity:', perplexity(best_model,\n",
    "                                               X_val, \n",
    "                                               ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que la perplejidad del nuevo modelo es un poco mayor que la perplejidad del modelo simple. Cabe resaltar que en accuracy estaba teniendo mejor desempeño este modelo debido a la modificacion, sin embargo en pocas iteraciones, dicho modelo es de mayor complejidad y tal vez por eso se ajustó un poco más a los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
