{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BRaP_x0eoI1"
   },
   "source": [
    "# NLP. Tarea 5: Modelo del Lenguaje Neuronal.\n",
    "\n",
    "**Diego Moreno**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Modelo Neuronal a nivel de caracter.\n",
    "\n",
    "Importamos librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5WWvA-6sets1"
   },
   "outputs": [],
   "source": [
    "# Tools\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from typing import Tuple\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "# Preprocesing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import  TweetTokenizer\n",
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Pytorch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Scikitlearn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bqN6c3B-f7_h"
   },
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Yb4ys6nJgPYJ"
   },
   "outputs": [],
   "source": [
    "pth = ''\n",
    "X_train = pd.read_csv(pth+'mex_train.txt', sep='\\r\\n',  engine='python', header=None).loc[:,0].values.tolist()\n",
    "X_val = pd.read_csv(pth+'mex_val.txt', sep='\\r\\n',  engine='python', header=None).loc[:,0].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nivel de caracter, tenemos que fijarnos en una ventana de 6 o más:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Jj3gCUEXhlGu"
   },
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.N = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase de N-gramas se quedará igual pues la estrategia será solamente cambiar el tokenizador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "id": "2T9J65oaiILS"
   },
   "outputs": [],
   "source": [
    "class NgramData():\n",
    "    def __init__(self, N: int, vocab_max: int=5000, tokenizer=None, embedding_model=None):\n",
    "        self.tokenizer = tokenizer if tokenizer else self.default_tokenizer\n",
    "        self.punct = set(['.',',',';',':','-','^','«','»','\"','!','¡','?','¿','\\'','...','<url>','*','@usuario'])\n",
    "        self.N = N\n",
    "        self.vocab_max = vocab_max\n",
    "        self.UNK = '<unk>'\n",
    "        self.SOS = '<s>'\n",
    "        self.EOS = '</s>'\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "    def default_tokenizer(self, doc: str) -> list:\n",
    "        return doc.split(' ')\n",
    "\n",
    "    def get_vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def remove_word(self, word: str) -> bool:\n",
    "        word = word.lower()\n",
    "        is_punct = True if word in self.punct else False\n",
    "        is_digit = word.isnumeric()\n",
    "        return is_punct or is_digit\n",
    "\n",
    "    def get_vocab(self, corpus: list) -> set:\n",
    "        freq_dist = FreqDist([w.lower() for sent in corpus \\\n",
    "                              for w in self.tokenizer(sent) \\\n",
    "                              if not self.remove_word(w)])\n",
    "        sorted_words = self.sortFreqDict(freq_dist)[:self.vocab_max-3]\n",
    "        return set(sorted_words)\n",
    "\n",
    "    def sortFreqDict(self, freq_dist) -> list:\n",
    "        freq_dist = dict(freq_dist)\n",
    "        return sorted(freq_dist, key=freq_dist.get, reverse=True)\n",
    "\n",
    "    def fit(self, corpus: list) -> None:\n",
    "        self.vocab = self.get_vocab(corpus)\n",
    "        self.vocab.add(self.UNK)\n",
    "        self.vocab.add(self.SOS)\n",
    "        self.vocab.add(self.EOS)\n",
    "\n",
    "        self.w2id = {}\n",
    "        self.id2w = {}\n",
    "\n",
    "        if self.embedding_model is not None:\n",
    "            self.embedding_matrix = np.empty([len(self.vocab), self.embedding_model.vector_size])\n",
    "\n",
    "        ID = 0\n",
    "        for doc in corpus:\n",
    "            for word in self.tokenizer(doc):\n",
    "                word_ = word.lower()\n",
    "                if word_ in self.vocab and not word_ in self.w2id:\n",
    "                    self.w2id[word_] = ID\n",
    "                    self.id2w[ID] = word_\n",
    "                    if self.embedding_model is not None:\n",
    "                        if word_ in self.embedding_model.emb_dict:\n",
    "                            self.embedding_matrix[ID] = self.embedding_model.emb_dict[word_]\n",
    "                        else:\n",
    "                            self.embedding_matrix[ID] = np.random.rand(self.embedding_model.vector_size) \n",
    "                    ID += 1\n",
    "        #Special tokens  \n",
    "        self.w2id.update({self.UNK: ID, \n",
    "                          self.SOS: ID+1,\n",
    "                          self.EOS: ID+2})  \n",
    "        self.id2w.update({ID  : self.UNK, \n",
    "                          ID+1: self.SOS,\n",
    "                          ID+2: self.EOS})\n",
    "    \n",
    "    def replace_unk(self, doc_tokens: list) -> list: \n",
    "        for i, token in enumerate(doc_tokens):\n",
    "            if token.lower() not in self.vocab:\n",
    "                doc_tokens[i] = self.UNK\n",
    "        return doc_tokens\n",
    "\n",
    "\n",
    "    def get_ngram_doc(self, doc:str) -> list:\n",
    "        doc_tokens = self.tokenizer(doc)\n",
    "        doc_tokens = self.replace_unk(doc_tokens)\n",
    "        doc_tokens = [w.lower() for w in doc_tokens]\n",
    "        doc_tokens = [self.SOS]*(self.N - 1) + doc_tokens + [self.EOS]\n",
    "        return list(ngrams(doc_tokens, self.N))\n",
    "    \n",
    "    def transform(self, corpus: list) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        X_ngrams = []\n",
    "        y = []\n",
    "        for doc in corpus:\n",
    "            doc_ngram = self.get_ngram_doc(doc)\n",
    "            for words_window in doc_ngram:\n",
    "                words_window_ids = [self.w2id[w] for w in words_window]\n",
    "                X_ngrams.append(list(words_window_ids[:-1]))\n",
    "                y.append(words_window_ids[-1])\n",
    "        return np.array(X_ngrams), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el nuevo tokenizador a nivel de caracter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "igtUbF-UtA5e"
   },
   "outputs": [],
   "source": [
    "def CharTokenizer(doc: str) -> list:\n",
    "    l = []\n",
    "    for c in doc:\n",
    "        l.append(c)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos nivel de caracteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_level = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ukx1t0n8h12F"
   },
   "outputs": [],
   "source": [
    "if char_level:\n",
    "    tk = CharTokenizer\n",
    "else:\n",
    "    tk = TweetTokenizer()\n",
    "    tk = tk.tokenize\n",
    "    \n",
    "ngram_data = NgramData(args.N, 5000, tk)\n",
    "ngram_data.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OH2bHAWHqJ7w",
    "outputId": "1bd5b974-1b17-494c-8e8f-346fbc98c220"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 344\n"
     ]
    }
   ],
   "source": [
    "print('Vocab Size:', ngram_data.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos los datos transformados y los loader de los mismos para entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Hl5NY_wFqZPt"
   },
   "outputs": [],
   "source": [
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2PH5JSUbwX6O"
   },
   "outputs": [],
   "source": [
    "#Batch size\n",
    "args.batch_size = 64\n",
    "#Number workers\n",
    "args.num_workers = 2\n",
    "\n",
    "#Train\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = True)\n",
    "\n",
    "#Validation\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CxrtxytHyIAR",
    "outputId": "baebf0d3-f72b-4e23-e202-0fb96ad0d2ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([64, 5])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print('X shape:', batch[0].shape)\n",
    "print('y shape:', batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "l_diObstybV6"
   },
   "outputs": [],
   "source": [
    "#[[ngram_data.id2w[w] for w in tw] for tw in batch[0].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase del modelo neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "hXQa9kP8VMFU"
   },
   "outputs": [],
   "source": [
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(NeuralLM, self).__init__()\n",
    "\n",
    "        self.window_size = args.N - 1\n",
    "        self.embedding_size = args.d\n",
    "\n",
    "        self.emb = nn.Embedding(args.vocab_size, args.d)\n",
    "        self.fc1 = nn.Linear(args.d * (args.N - 1), args.d_h)\n",
    "        self.drop1 = nn.Dropout(p = args.dropout)\n",
    "        self.fc2 = nn.Linear(args.d_h, args.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1, self.window_size * self.embedding_size)\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = self.drop1(h)\n",
    "        x = self.fc2(h)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para el entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0vZZwQA1XZ8_"
   },
   "outputs": [],
   "source": [
    "def get_preds(raw_logits):\n",
    "    probs = F.softmax(raw_logits.detach(), dim=1)\n",
    "    y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    return y_pred\n",
    "\n",
    "def model_eval(data, model, gpu=False):\n",
    "    with torch.no_grad():\n",
    "        preds, tgts = [], []\n",
    "        for window_words, labels in data:\n",
    "            if gpu:\n",
    "                window_words = window_words.cuda()\n",
    "            outputs = model(window_words)\n",
    "\n",
    "            #Predictions\n",
    "            y_pred = get_preds(outputs)\n",
    "            tgt = labels.numpy()\n",
    "            tgts.append(tgt)\n",
    "            preds.append(y_pred)\n",
    "    tgts = [e for l in tgts for e in l]\n",
    "    preds = [e for l in preds for e in l]\n",
    "    return accuracy_score(tgts, preds)\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint_path, filename='checkpoint.pt'):\n",
    "    filename = os.path.join(checkpoint_path, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, os.path.join(checkpoint_path, 'model_best.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos hiperparámetros que tendrán que ser modificados posteriormente al usar un nuevo embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Dnk0A8MmXzRr"
   },
   "outputs": [],
   "source": [
    "#Model hyperparameters\n",
    "#Vocabulary size\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "#Word embeddings dimension\n",
    "args.d = 100\n",
    "#Hidden layer dimension\n",
    "args.d_h = 200\n",
    "#Dropout\n",
    "args.dropout = 0.1\n",
    "\n",
    "#Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "#Scheduler hyperparameters\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "#Saving hyperparameters\n",
    "args.savedir = pth + 'model'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "#Create model\n",
    "model = NeuralLM(args=args)\n",
    "\n",
    "#Send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "#Loss, Optimizer and Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer, \n",
    "                                                       mode = 'min',\n",
    "                                                       factor = args.lr_factor,\n",
    "                                                       patience = args.lr_patience,\n",
    "                                                       verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa de entrenamiento del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 902
    },
    "id": "m5w33lulaO27",
    "outputId": "86e4a066-cdcd-4533-caad-4a110bf51600"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.4386065422287932\n",
      "Epoch [1/100]: Loss = 1.9773, Val Acurracy = 0.4386, Epoch time = 31.76\n",
      "Train accuracy:  0.46004435409351324\n",
      "Epoch [2/100]: Loss = 1.8176, Val Acurracy = 0.4600, Epoch time = 30.50\n",
      "Train accuracy:  0.4607835889854001\n",
      "Epoch [3/100]: Loss = 1.7673, Val Acurracy = 0.4608, Epoch time = 30.55\n",
      "Train accuracy:  0.4825540565514692\n",
      "Epoch [4/100]: Loss = 1.7373, Val Acurracy = 0.4826, Epoch time = 30.89\n",
      "Train accuracy:  0.4729624838292367\n",
      "Epoch [5/100]: Loss = 1.7156, Val Acurracy = 0.4730, Epoch time = 31.02\n",
      "Train accuracy:  0.4506006283496581\n",
      "Epoch [6/100]: Loss = 1.7002, Val Acurracy = 0.4506, Epoch time = 30.81\n",
      "Train accuracy:  0.47444095361301053\n",
      "Epoch [7/100]: Loss = 1.6873, Val Acurracy = 0.4744, Epoch time = 30.63\n",
      "Train accuracy:  0.4743300683792275\n",
      "Epoch [8/100]: Loss = 1.6764, Val Acurracy = 0.4743, Epoch time = 30.30\n",
      "Train accuracy:  0.4886712252818333\n",
      "Epoch [9/100]: Loss = 1.6685, Val Acurracy = 0.4887, Epoch time = 32.59\n",
      "Train accuracy:  0.4830900018480872\n",
      "Epoch [10/100]: Loss = 1.6614, Val Acurracy = 0.4831, Epoch time = 33.33\n",
      "Train accuracy:  0.48443910552578084\n",
      "Epoch [11/100]: Loss = 1.6551, Val Acurracy = 0.4844, Epoch time = 32.53\n",
      "Epoch    12: reducing learning rate of group 0 to 1.1500e-01.\n",
      "Train accuracy:  0.49240436148586214\n",
      "Epoch [12/100]: Loss = 1.6499, Val Acurracy = 0.4924, Epoch time = 32.74\n",
      "Train accuracy:  0.5046386989465903\n",
      "Epoch [13/100]: Loss = 1.6107, Val Acurracy = 0.5046, Epoch time = 32.69\n",
      "Train accuracy:  0.5072445019404916\n",
      "Epoch [14/100]: Loss = 1.6055, Val Acurracy = 0.5072, Epoch time = 33.59\n",
      "Train accuracy:  0.5064683053040103\n",
      "Epoch [15/100]: Loss = 1.6023, Val Acurracy = 0.5065, Epoch time = 33.00\n",
      "Train accuracy:  0.5022916281648494\n",
      "Epoch [16/100]: Loss = 1.6014, Val Acurracy = 0.5023, Epoch time = 31.56\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-e7244224207f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtraining_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "#Training\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model.train()\n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        #Forward pass\n",
    "        outputs = model(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "\n",
    "        #Get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "\n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #Metrics in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Metrics in validation dataset\n",
    "    model.eval()\n",
    "    tuning_metric = model_eval(val_loader, model, gpu=args.use_gpu)\n",
    "    mean_epoch_metric = np.mean(tuning_metric)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "\n",
    "    #Check metric improvement\n",
    "    is_improve = tuning_metric > best_metric\n",
    "    if is_improve:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "\n",
    "    #Save best model\n",
    "    save_checkpoint({'epoch'       : epoch+1,\n",
    "                     'state_dict'  : model.state_dict(),\n",
    "                     'optimizer'   : optimizer.state_dict(),\n",
    "                     'scheduler'   : scheduler.state_dict(),\n",
    "                     'best_metric' : best_metric}, is_improve, args.savedir)\n",
    "    \n",
    "    #Early stoping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print('No improvement. Breaking out of loop.')\n",
    "        break\n",
    "    print('Train accuracy: ', mean_epoch_metric)\n",
    "    print('Epoch [{}/{}]: Loss = {:.4f}, Val Acurracy = {:.4f}, Epoch time = {:.2f}'.\n",
    "          format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time()-epoch_start_time)))\n",
    "    \n",
    "print('---------- %s seconds ---------' % time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejor modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(344, 100)\n",
       "  (fc1): Linear(in_features=500, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=344, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model with learned embeddings\n",
    "best_model = NeuralLM(args)\n",
    "best_model.load_state_dict(torch.load(pth+'model/model_best.pt')['state_dict'])\n",
    "best_model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Generamos texto 3 veces con máximo de 300 caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "gziHZsIQv0AO"
   },
   "outputs": [],
   "source": [
    "lenght_max = 300\n",
    "\n",
    "def parse_text(text, tokenizer):\n",
    "    all_tokens = [w.lower() if w in ngram_data.w2id else '<unk>' for w in tokenizer(text)]\n",
    "    token_ids = [ngram_data.w2id[word.lower()] for word in all_tokens]\n",
    "    return all_tokens, token_ids\n",
    "\n",
    "def sample_next_word(logits, temperature=1.):\n",
    "    logits = np.asarray(logits).astype('float64')\n",
    "    preds = logits/temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probs = np.random.multinomial(1, preds)\n",
    "    return np.argmax(probs)\n",
    "\n",
    "def predict_next_token(model, token_ids):\n",
    "    words_ids_tensor = torch.LongTensor(token_ids).unsqueeze(0)\n",
    "    y_raw_pred = model(words_ids_tensor).squeeze(0).detach().numpy()\n",
    "    y_pred = sample_next_word(y_raw_pred, 1.)\n",
    "    return y_pred\n",
    "\n",
    "def generate_sentence(model, initial_text, tokenizer):\n",
    "    all_tokens, window_word_ids = parse_text(initial_text, tokenizer)\n",
    "    for i in range(lenght_max):\n",
    "        y_pred = predict_next_token(best_model, window_word_ids)\n",
    "        next_word = ngram_data.id2w[y_pred]\n",
    "        all_tokens.append(next_word)\n",
    "        if next_word == '</s>':\n",
    "            break\n",
    "        else:\n",
    "            window_word_ids.pop(0)\n",
    "            window_word_ids.append(y_pred)\n",
    "    if char_level:\n",
    "        return ''.join(all_tokens)\n",
    "    else:\n",
    "        return ' '.join(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "UbXJ642z--mB",
    "outputId": "8a33b6a5-6361-4948-dbcd-a528e9ca0708"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s><s putos perder a oierencanas anuncia un putos<unk> 😊🏻🖕🏻</s>'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = '<s><s'\n",
    "print('Learned embeddings')\n",
    "print('¯'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'estoy que vale verga<unk></s>'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = 'estoy'\n",
    "print('Learned embeddings')\n",
    "print('¯'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yo opulidado el pubarmos cleto 😈#pasibron wey que esputo es elevar con tantar tienen llegué me dinerente nomar nada ti a habes <unk><unk> vienen como hdp sestrea</s>'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = 'yo op'\n",
    "print('Learned embeddings')\n",
    "print('¯'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que genera texto bueno, sin embargo, como es a nivel caracter, algunas palabras no tienen un significado sin embargo, el sentido se preserva. Puede que esto mejore al expandir la ventana a más de 6 caracteres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Escribimos 5 ejemplos de oraciones y medimos su verosimilitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "au6ek-AlDTwd"
   },
   "outputs": [],
   "source": [
    "def log_likelihood(model, text, ngram_model):\n",
    "    # Generate n-gram windows from input text and the respective label y\n",
    "    X, y = ngram_data.transform([text])\n",
    "    # Discard first two n-gram windows since they contain '<s>' tokens not necessary\n",
    "    X, y = X[2:], y[2:]\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "\n",
    "    logits = model(X).detach()\n",
    "    probs = F.softmax(logits, dim = 1).numpy()\n",
    "\n",
    "    return np.sum([np.log(probs[i][w]) for i, w in enumerate(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RHMEZCGENCL5",
    "outputId": "86aee787-4cca-45f8-8e2b-36e100eb48a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -58.841915\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model, \n",
    "                                        'La clase de lenguaje está muy padre', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jr4XFLiHNCOl",
    "outputId": "4f76dda4-d895-439b-832b-44a0e154bf01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -61.371037\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model,\n",
    "                                        'La clase de lenguaje está muy chida',\n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5c7tbaKNCRP",
    "outputId": "3f32376f-8c1f-449f-f8de-031191a56234"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -69.30573\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model,\n",
    "                                        'La clase de lenguaje está muy guay', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -96.00549\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model,\n",
    "                                        'La clase de procesamiento del lenguaje está muy padre', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -56.34829\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model,\n",
    "                                        'La clase de lenguaje está muy madre', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparamos que tienen sentido los resultados pues la palabra padre es más usada que chida y mucho más que guay., entre más palabras le pongamos menor será su verosimilitud como en el ejemplo 4. Sin embargo, logra fallar en el último ejemplo cuando decimos que está muy madre. En el español normal no se usa, no obstante, al haber sido entrenado con tuits groseros, se obtiene que es más probable decir que está muy madre. Otro dato curioso, es que al cambiar la palabra «está» por «esta», la verosimilitud disminuye, lo cual podría ser útil para correcciones ortográficas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4HQ2xLFNF8r"
   },
   "source": [
    "## 1.3. Estructuras morfológicas correctas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJYUOuUKNCT0",
    "outputId": "4ffff7c7-d8b2-48a9-bb1f-a9e2bd7b4389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.246503 chingada\n",
      "-5.246503 chingada\n",
      "-14.173836 dachinga\n",
      "-14.173836 dachinga\n",
      "-15.5406885 dgachina\n",
      "--------------------------------------------------\n",
      "-66.48084 acihdnag\n",
      "-67.964966 aacgdhni\n",
      "-67.964966 aacgdhni\n",
      "-68.05988 caagdhni\n",
      "-68.05988 caagdhni\n"
     ]
    }
   ],
   "source": [
    "if char_level:\n",
    "    word_list = 'chingada'\n",
    "    perms = [''.join(perm) for perm in permutations(word_list)]\n",
    "else:\n",
    "    word_list = 'sino gano me voy a la chingada'.split(' ')\n",
    "    perms = [' '.join(perm) for perm in permutations(word_list)]\n",
    "#print(len(perms))\n",
    "\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[:5]:\n",
    "    print(p, t)\n",
    "print('-'*50)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[-5:]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que hay dos chingadas debido a que se obtiene una de otra solo intercambiando las a's y similarmente para las demás palabras. Los resultados son los esperados pues la más probable es chingada, luego dachinga que contiene la palabra chinga y despues dgachina que contiene la palabra china. Las menos probables ni se pueden leer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Perplejidad en validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "BHbLjydkNCWU"
   },
   "outputs": [],
   "source": [
    "def perplexity(model, text, ngram_model):\n",
    "    # Generate n-gram windows from input text and the respective label y\n",
    "    X, y = ngram_data.transform([text])\n",
    "    # Discard first two n-gram windows since they contain '<s>' tokens not necessary\n",
    "    X, y = X[2:], y[2:]\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "\n",
    "    logits = model(X).detach()\n",
    "    probs = F.softmax(logits, dim = 1).numpy()\n",
    "    \n",
    "    ans = 1.\n",
    "    N = len(y)\n",
    "    print('Validation set dimension:', N)\n",
    "    probs = [(probs[i][w])**(1/N) for i, w in enumerate(y)]\n",
    "    for p in probs:\n",
    "        ans /= p\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "8wbHjIcdNCYS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set dimension: 615\n",
      "Validation set perplexity: 1.4799053402082503\n"
     ]
    }
   ],
   "source": [
    "print('Validation set perplexity:', perplexity(best_model,\n",
    "                                               X_val, \n",
    "                                               ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Modelo de lenguaje neuronal inicializado con embedding dado.\n",
    "Leemos el embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = ''\n",
    "emb_txt = pd.read_csv(pth+'word2vec_col.txt',\n",
    "                        sep='\\r\\n', engine='python', \n",
    "                        header=None).loc[:,0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_d = int(emb_txt[0].split()[1])\n",
    "emb_N = int(emb_txt[0].split()[0])\n",
    "emb_txt = emb_txt[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el diccionario del embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict = {}\n",
    "for i in range(emb_N):\n",
    "    row_list = emb_txt[i].split()\n",
    "    emb_dict[row_list[0]] = np.array(row_list[1:]).astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase del embedding nuevo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "class embedding():\n",
    "    def __init__(self, emb_N, emb_dict):\n",
    "        self.emb_dict = emb_dict\n",
    "        self.embedding_matrix = np.empty([len(emb_dict), emb_N])\n",
    "        self.vector_size = emb_N\n",
    "        for i, word in enumerate(emb_dict):\n",
    "            self.embedding_matrix[i,:] = emb_dict[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usaremos nivel de palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_level = False\n",
    "args.N = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el nuevo ngram_data con el embedding preinicializado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 5000\n"
     ]
    }
   ],
   "source": [
    "if char_level:\n",
    "    tk = CharTokenizer\n",
    "else:\n",
    "    tk = TweetTokenizer()\n",
    "    tk = tk.tokenize\n",
    "\n",
    "emb_model = embedding(emb_d, emb_dict)\n",
    "ngram_data = NgramData(args.N, 5000, tk, emb_model)\n",
    "ngram_data.fit(X_train)\n",
    "print('Vocab Size:', ngram_data.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a transformar los datos crear los loaders: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size\n",
    "args.batch_size = 64\n",
    "#Number workers\n",
    "args.num_workers = 2\n",
    "\n",
    "#Train\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = True)\n",
    "\n",
    "#Validation\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([64, 3])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print('X shape:', batch[0].shape)\n",
    "print('y shape:', batch[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiamos algunos hiperparámetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model hyperparameters\n",
    "#Vocabulary size\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "#Word embeddings dimension\n",
    "args.d = emb_d\n",
    "#Hidden layer dimension\n",
    "args.d_h = 200 #Está bien puestp que emb_d es 100\n",
    "#Dropout\n",
    "args.dropout = 0.1\n",
    "\n",
    "#Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "#Scheduler hyperparameters\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "#Saving hyperparameters\n",
    "args.savedir = pth + 'model_emb'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "#Create model\n",
    "model_emb = NeuralLM(args=args)\n",
    "\n",
    "#Send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model_emb.cuda()\n",
    "\n",
    "#Loss, Optimizer and Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_emb.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer, \n",
    "                                                       mode = 'min',\n",
    "                                                       factor = args.lr_factor,\n",
    "                                                       patience = args.lr_patience,\n",
    "                                                       verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa de entrenamiento del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.21567279601306066\n",
      "Epoch [1/100]: Loss = 5.5244, Val Acurracy = 0.2157, Epoch time = 29.42\n",
      "Train accuracy:  0.2209142464340952\n",
      "Epoch [2/100]: Loss = 5.0727, Val Acurracy = 0.2209, Epoch time = 29.34\n",
      "Train accuracy:  0.22555421893796185\n",
      "Epoch [3/100]: Loss = 4.8611, Val Acurracy = 0.2256, Epoch time = 29.37\n",
      "Train accuracy:  0.18654407973878673\n",
      "Epoch [4/100]: Loss = 4.6914, Val Acurracy = 0.1865, Epoch time = 28.97\n",
      "Train accuracy:  0.21979721601649768\n",
      "Epoch [5/100]: Loss = 4.5472, Val Acurracy = 0.2198, Epoch time = 30.31\n",
      "Train accuracy:  0.21214985392679153\n",
      "Epoch [6/100]: Loss = 4.4175, Val Acurracy = 0.2121, Epoch time = 29.64\n",
      "Train accuracy:  0.19041072349200894\n",
      "Epoch [7/100]: Loss = 4.2921, Val Acurracy = 0.1904, Epoch time = 29.55\n",
      "Train accuracy:  0.21747722976456435\n",
      "Epoch [8/100]: Loss = 4.1725, Val Acurracy = 0.2175, Epoch time = 29.97\n",
      "Train accuracy:  0.21146245059288538\n",
      "Epoch [9/100]: Loss = 4.0719, Val Acurracy = 0.2115, Epoch time = 30.27\n",
      "Train accuracy:  0.16772641347310535\n",
      "Epoch [10/100]: Loss = 3.9669, Val Acurracy = 0.1677, Epoch time = 29.11\n",
      "Train accuracy:  0.1788967176490806\n",
      "Epoch [11/100]: Loss = 3.8664, Val Acurracy = 0.1789, Epoch time = 29.10\n",
      "Train accuracy:  0.16282866471902388\n",
      "Epoch [12/100]: Loss = 3.7822, Val Acurracy = 0.1628, Epoch time = 28.96\n",
      "Train accuracy:  0.1660938305550782\n",
      "Epoch [13/100]: Loss = 3.6934, Val Acurracy = 0.1661, Epoch time = 28.96\n",
      "Train accuracy:  0.21816463309847053\n",
      "Epoch [14/100]: Loss = 3.6161, Val Acurracy = 0.2182, Epoch time = 29.04\n",
      "Train accuracy:  0.1588760955490634\n",
      "Epoch [15/100]: Loss = 3.5516, Val Acurracy = 0.1589, Epoch time = 28.91\n",
      "Train accuracy:  0.18912184224093487\n",
      "Epoch [16/100]: Loss = 3.4916, Val Acurracy = 0.1891, Epoch time = 28.82\n",
      "Train accuracy:  0.19006702182505586\n",
      "Epoch [17/100]: Loss = 3.4196, Val Acurracy = 0.1901, Epoch time = 29.29\n",
      "Train accuracy:  0.2096580168413817\n",
      "Epoch [18/100]: Loss = 3.3747, Val Acurracy = 0.2097, Epoch time = 29.17\n",
      "Train accuracy:  0.15672796013060664\n",
      "Epoch [19/100]: Loss = 3.3261, Val Acurracy = 0.1567, Epoch time = 28.84\n",
      "Train accuracy:  0.1994328922495274\n",
      "Epoch [20/100]: Loss = 3.2785, Val Acurracy = 0.1994, Epoch time = 29.19\n",
      "Train accuracy:  0.17580340264650285\n",
      "Epoch [21/100]: Loss = 3.2324, Val Acurracy = 0.1758, Epoch time = 28.82\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-324-601dcf5586ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "#Training\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model_emb.train()\n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        #Forward pass\n",
    "        outputs = model_emb(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "\n",
    "        #Get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "\n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #Metrics in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Metrics in validation dataset\n",
    "    model_emb.eval()\n",
    "    tuning_metric = model_eval(val_loader, model_emb, gpu=args.use_gpu)\n",
    "    mean_epoch_metric = np.mean(tuning_metric)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "\n",
    "    #Check metric improvement\n",
    "    is_improve = tuning_metric > best_metric\n",
    "    if is_improve:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "\n",
    "    #Save best model\n",
    "    save_checkpoint({'epoch'       : epoch+1,\n",
    "                     'state_dict'  : model_emb.state_dict(),\n",
    "                     'optimizer'   : optimizer.state_dict(),\n",
    "                     'scheduler'   : scheduler.state_dict(),\n",
    "                     'best_metric' : best_metric}, is_improve, args.savedir)\n",
    "    \n",
    "    #Early stoping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print('No improvement. Breaking out of loop.')\n",
    "        break\n",
    "    print('Train accuracy: ', mean_epoch_metric)\n",
    "    print('Epoch [{}/{}]: Loss = {:.4f}, Val Acurracy = {:.4f}, Epoch time = {:.2f}'.\n",
    "          format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time()-epoch_start_time)))\n",
    "    \n",
    "print('---------- %s seconds ---------' % time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(5000, 100)\n",
       "  (fc1): Linear(in_features=300, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=5000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model with learned embeddings\n",
    "best_model = NeuralLM(args)\n",
    "best_model.load_state_dict(torch.load(pth+'model_emb/model_best.pt')['state_dict'])\n",
    "best_model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Palabras más similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_words(embeddings, ngram_data, word, n):\n",
    "    word_id = torch.LongTensor([[ngram_data.w2id[word]]])\n",
    "    word_embed = embeddings(word_id).view(-1).numpy()\n",
    "    aux = torch.tensor(embeddings.embedding_matrix - word_embed)\n",
    "    dists = torch.norm(aux, dim=1).detach()\n",
    "    lst = sorted(enumerate(dists.numpy()), key=lambda x : x[1])\n",
    "    for idx, diff in lst[1:n+1]:\n",
    "        print(embeddings.id2w[idx], diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_words(embeddings, ngram_data, word, n):\n",
    "    word_id = torch.LongTensor([ngram_data.w2id[word]])\n",
    "    word_embed = embeddings(word_id)\n",
    "    dists = torch.norm(embeddings.weight - word_embed, dim=1).detach()\n",
    "    lst = sorted(enumerate(dists.numpy()), key=lambda x : x[1])\n",
    "    for idx, diff in lst[1:n+1]:\n",
    "        print(ngram_data.id2w[idx], diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      "estuviera 11.158217\n",
      "falta 11.536218\n",
      "gol 11.661474\n",
      "🍆 11.697962\n",
      "plata 11.7053995\n",
      "escribir 11.808645\n",
      "🐞 11.84848\n",
      "chacal 11.848602\n",
      "peleas 11.894655\n",
      "aquello 11.899229\n"
     ]
    }
   ],
   "source": [
    "print('Learned embeddings')\n",
    "print('¯'*20)\n",
    "print_closest_words(best_model.emb, ngram_data, 'lugar', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      "acerca 10.808623\n",
      "happy 10.836503\n",
      "regalar 10.970969\n",
      "4to 10.990195\n",
      "venir 11.020816\n",
      "jajajajajjaja 11.024045\n",
      "mejoran 11.116126\n",
      "puros 11.134831\n",
      "tragan 11.146174\n",
      "jajajajajajajaja 11.151438\n"
     ]
    }
   ],
   "source": [
    "print('Learned embeddings')\n",
    "print('¯'*20)\n",
    "print_closest_words(best_model.emb, ngram_data, 'madre', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      "<unk> 10.303227\n",
      "quedan 10.429716\n",
      "primer 10.556313\n",
      "bonitos 10.570142\n",
      "mamo 10.769064\n",
      "periodistas 10.787671\n",
      "chingar 10.793026\n",
      "tenían 10.830661\n",
      "gallardo 10.853344\n",
      "grupo 10.94077\n"
     ]
    }
   ],
   "source": [
    "print('Learned embeddings')\n",
    "print('¯'*20)\n",
    "print_closest_words(best_model.emb, ngram_data, 'chingada', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que en algunas palabras es buena la relación como chingada-chingar-mamo, lugar-estuviera-falta-gol en ámbitos de fuera de lugar en el futbol. Sin embargo en madre no existen muy buenas realiciones a menos que sean los jaja's pues ambos aparecen relacionados, tal vez porque un insulto con madre siempre acabara en risas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Generación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> <s> <s> llevo estos de mil putas <unk> bloquee se oye <unk> <unk> dolor chingona programas cosa sueños <unk> <unk> para momentos estaría verga dime te digan <unk> para ver si no quiere … </s>'"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = '<s> <s> <s>'\n",
    "print('Learned embeddings')\n",
    "print('¯'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> que no saben entiendo ya valen verga pero 😂 😂 😂 ❤ ya le traigo de la <unk> <unk> aqui de cagada <unk> <unk> </s>'"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = '<s> que no'\n",
    "print('Learned embeddings')\n",
    "print('¯'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> que pedo con el mismo <unk> <unk> enfermos que poca madre tienen del mamar <unk> </s>'"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = '<s> que pedo'\n",
    "print('Learned embeddings')\n",
    "print('¯'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que genera buena secuencia de palabras con la ventaja de que las palabras existen (ventaja sobre el modelo por caracteres). algunas tienen sentido, sin embargo, en muchas ocasiones se pierde el significado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. 5 ejemplos de oraciones y medimos su verosimilitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -28.252762\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model, \n",
    "                                        'La clase de lenguaje está muy padre', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -30.028719\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model, \n",
    "                                        'La clase de lenguaje esta muy padre', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -29.552502\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model, \n",
    "                                        'La clase de lenguaje está muy chida', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -13.44938\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model, \n",
    "                                        'Vamos para allá', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -16.710388\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model, \n",
    "                                        'Vamos para haya', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colocamos ejemplos en los que hay faltas de ortografía para notar si en realidad puede servir para corregir ortográficamente pues la parabra correcta es la que mejor debería quedar en el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Estructura sintácticas correctas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-14.370728 buscamos nosotros a una mujer\n",
      "-16.224253 nosotros buscamos a una mujer\n",
      "-19.5894 nosotros buscamos mujer a una\n",
      "-19.754845 buscamos una mujer a nosotros\n",
      "-20.257294 nosotros buscamos una mujer a\n",
      "--------------------------------------------------\n",
      "-33.993004 a una nosotros mujer buscamos\n",
      "-34.36071 una mujer nosotros buscamos a\n",
      "-35.04878 una mujer nosotros a buscamos\n",
      "-35.06093 a una nosotros buscamos mujer\n",
      "-35.988052 mujer una nosotros buscamos a\n"
     ]
    }
   ],
   "source": [
    "if char_level:\n",
    "    word_list = 'chingada'\n",
    "    perms = [''.join(perm) for perm in permutations(word_list)]\n",
    "else:\n",
    "    word_list = 'buscamos a una nosotros mujer'.split(' ')\n",
    "    perms = [' '.join(perm) for perm in permutations(word_list)]\n",
    "#print(len(perms))\n",
    "\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[:5]:\n",
    "    print(p, t)\n",
    "print('-'*50)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[-5:]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ejemplo propuesto fue puesto con la intención de que la palabra nosotros se relacionara con buscamos lo la conjugación y la palabra una con mujer. Lo cual fue satisfecho por el ordenamiento con verosimilitud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Perplejidad para los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, text, ngram_model):\n",
    "    # Generate n-gram windows from input text and the respective label y\n",
    "    X, y = ngram_data.transform(text)\n",
    "    # Discard first two n-gram windows since they contain '<s>' tokens not necessary\n",
    "    X, y = X[2:], y[2:]\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "\n",
    "    logits = model(X).detach()\n",
    "    probs = F.softmax(logits, dim = 1).numpy()\n",
    "    \n",
    "    ans = 1.\n",
    "    N = len(y)\n",
    "    print('Validation set dimension:', N)\n",
    "    probs = [(probs[i][w])**(1/N) for i, w in enumerate(y)]\n",
    "    for p in probs:\n",
    "        ans /= p\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set dimension: 11636\n",
      "Validation set perplexity: 114.71186832388392\n"
     ]
    }
   ],
   "source": [
    "print('Validation set perplexity:', perplexity(best_model,\n",
    "                                               X_val, \n",
    "                                               ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Comparamos con el modelo de clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 5000\n"
     ]
    }
   ],
   "source": [
    "if char_level:\n",
    "    tk = CharTokenizer\n",
    "else:\n",
    "    tk = TweetTokenizer()\n",
    "    tk = tk.tokenize\n",
    "\n",
    "emb_model = None\n",
    "ngram_data = NgramData(args.N, 5000, tk, emb_model)\n",
    "ngram_data.fit(X_train)\n",
    "print('Vocab Size:', ngram_data.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size\n",
    "args.batch_size = 64\n",
    "#Number workers\n",
    "args.num_workers = 2\n",
    "\n",
    "#Train\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = True)\n",
    "\n",
    "#Validation\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([64, 3])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print('X shape:', batch[0].shape)\n",
    "print('y shape:', batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model hyperparameters\n",
    "#Vocabulary size\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "#Word embeddings dimension\n",
    "args.d = 100\n",
    "#Hidden layer dimension\n",
    "args.d_h = 200\n",
    "#Dropout\n",
    "args.dropout = 0.1\n",
    "\n",
    "#Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "#Scheduler hyperparameters\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "#Saving hyperparameters\n",
    "args.savedir = pth + 'model_normal'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "#Create model\n",
    "model_normal = NeuralLM(args=args)\n",
    "\n",
    "#Send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model_emb.cuda()\n",
    "\n",
    "#Loss, Optimizer and Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_normal.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer, \n",
    "                                                       mode = 'min',\n",
    "                                                       factor = args.lr_factor,\n",
    "                                                       patience = args.lr_patience,\n",
    "                                                       verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.217992782264994\n",
      "Epoch [1/100]: Loss = 5.5172, Val Acurracy = 0.2180, Epoch time = 30.00\n",
      "Train accuracy:  0.20940024059116688\n",
      "Epoch [2/100]: Loss = 5.0656, Val Acurracy = 0.2094, Epoch time = 29.30\n",
      "Train accuracy:  0.19994844474995704\n",
      "Epoch [3/100]: Loss = 4.8504, Val Acurracy = 0.1999, Epoch time = 29.76\n",
      "Train accuracy:  0.22082832101735694\n",
      "Epoch [4/100]: Loss = 4.6843, Val Acurracy = 0.2208, Epoch time = 28.87\n",
      "Train accuracy:  0.22787420518989518\n",
      "Epoch [5/100]: Loss = 4.5378, Val Acurracy = 0.2279, Epoch time = 29.15\n",
      "Train accuracy:  0.20012029558343358\n",
      "Epoch [6/100]: Loss = 4.4101, Val Acurracy = 0.2001, Epoch time = 28.82\n",
      "Train accuracy:  0.20115140058429284\n",
      "Epoch [7/100]: Loss = 4.2836, Val Acurracy = 0.2012, Epoch time = 29.18\n",
      "Train accuracy:  0.2123217047602681\n",
      "Epoch [8/100]: Loss = 4.1711, Val Acurracy = 0.2123, Epoch time = 28.73\n",
      "Train accuracy:  0.19719883141433237\n",
      "Epoch [9/100]: Loss = 4.0630, Val Acurracy = 0.1972, Epoch time = 28.66\n",
      "Train accuracy:  0.21129059975940884\n",
      "Epoch [10/100]: Loss = 3.9541, Val Acurracy = 0.2113, Epoch time = 29.37\n",
      "Train accuracy:  0.14890874720742395\n",
      "Epoch [11/100]: Loss = 3.8610, Val Acurracy = 0.1489, Epoch time = 29.99\n",
      "Train accuracy:  0.16703901013919917\n",
      "Epoch [12/100]: Loss = 3.7624, Val Acurracy = 0.1670, Epoch time = 29.15\n",
      "Train accuracy:  0.19316033682763362\n",
      "Epoch [13/100]: Loss = 3.6939, Val Acurracy = 0.1932, Epoch time = 29.43\n",
      "Train accuracy:  0.2236638597697199\n",
      "Epoch [14/100]: Loss = 3.6111, Val Acurracy = 0.2237, Epoch time = 28.98\n",
      "Train accuracy:  0.16849974222374978\n",
      "Epoch [15/100]: Loss = 3.5345, Val Acurracy = 0.1685, Epoch time = 28.74\n",
      "Train accuracy:  0.1666953084722461\n",
      "Epoch [16/100]: Loss = 3.4706, Val Acurracy = 0.1667, Epoch time = 28.76\n",
      "Train accuracy:  0.20261213266884345\n",
      "Epoch [17/100]: Loss = 3.4111, Val Acurracy = 0.2026, Epoch time = 29.70\n",
      "Train accuracy:  0.17271008764392506\n",
      "Epoch [18/100]: Loss = 3.3549, Val Acurracy = 0.1727, Epoch time = 29.71\n",
      "Train accuracy:  0.17563155181302628\n",
      "Epoch [19/100]: Loss = 3.3127, Val Acurracy = 0.1756, Epoch time = 29.06\n",
      "Train accuracy:  0.19384774016153977\n",
      "Epoch [20/100]: Loss = 3.2657, Val Acurracy = 0.1938, Epoch time = 28.72\n",
      "Train accuracy:  0.19427736724523115\n",
      "Epoch [21/100]: Loss = 3.2260, Val Acurracy = 0.1943, Epoch time = 28.69\n",
      "Epoch    22: reducing learning rate of group 0 to 1.1500e-01.\n",
      "Train accuracy:  0.20759580683966317\n",
      "Epoch [22/100]: Loss = 3.1797, Val Acurracy = 0.2076, Epoch time = 29.39\n",
      "Train accuracy:  0.19487884516239903\n",
      "Epoch [23/100]: Loss = 2.8759, Val Acurracy = 0.1949, Epoch time = 29.04\n",
      "Train accuracy:  0.2096580168413817\n",
      "Epoch [24/100]: Loss = 2.8265, Val Acurracy = 0.2097, Epoch time = 28.94\n",
      "No improvement. Breaking out of loop.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-394-20e06e13afa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m           format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time()-epoch_start_time)))\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'---------- %s seconds ---------'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'float'"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "#Training\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model_normal.train()\n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        #Forward pass\n",
    "        outputs = model_normal(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "\n",
    "        #Get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "\n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #Metrics in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Metrics in validation dataset\n",
    "    model_normal.eval()\n",
    "    tuning_metric = model_eval(val_loader, model_normal, gpu=args.use_gpu)\n",
    "    mean_epoch_metric = np.mean(tuning_metric)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "\n",
    "    #Check metric improvement\n",
    "    is_improve = tuning_metric > best_metric\n",
    "    if is_improve:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "\n",
    "    #Save best model\n",
    "    save_checkpoint({'epoch'       : epoch+1,\n",
    "                     'state_dict'  : model_normal.state_dict(),\n",
    "                     'optimizer'   : optimizer.state_dict(),\n",
    "                     'scheduler'   : scheduler.state_dict(),\n",
    "                     'best_metric' : best_metric}, is_improve, args.savedir)\n",
    "    \n",
    "    #Early stoping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print('No improvement. Breaking out of loop.')\n",
    "        break\n",
    "    print('Train accuracy: ', mean_epoch_metric)\n",
    "    print('Epoch [{}/{}]: Loss = {:.4f}, Val Acurracy = {:.4f}, Epoch time = {:.2f}'.\n",
    "          format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time()-epoch_start_time)))\n",
    "    \n",
    "print('---------- %s seconds ---------' % time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(5000, 100)\n",
       "  (fc1): Linear(in_features=300, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=5000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model with learned embeddings\n",
    "best_model = NeuralLM(args)\n",
    "best_model.load_state_dict(torch.load(pth+'model_normal/model_best.pt')['state_dict'])\n",
    "best_model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set dimension: 11636\n",
      "Validation set perplexity: 118.35818891012026\n"
     ]
    }
   ],
   "source": [
    "print('Validation set perplexity:', perplexity(best_model,\n",
    "                                               X_val, \n",
    "                                               ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
