{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BRaP_x0eoI1"
   },
   "source": [
    "# NLP. Tarea 5: Modelo del Lenguaje Neuronal.\n",
    "\n",
    "**Diego Moreno**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Modelo Neuronal a nivel de caracter.\n",
    "\n",
    "Importamos librer칤as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5WWvA-6sets1"
   },
   "outputs": [],
   "source": [
    "# Tools\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from typing import Tuple\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "# Preprocesing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import  TweetTokenizer\n",
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Pytorch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Scikitlearn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bqN6c3B-f7_h"
   },
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Yb4ys6nJgPYJ"
   },
   "outputs": [],
   "source": [
    "pth = ''\n",
    "X_train = pd.read_csv(pth+'mex_train.txt', sep='\\r\\n',  engine='python', header=None).loc[:,0].values.tolist()\n",
    "X_val = pd.read_csv(pth+'mex_val.txt', sep='\\r\\n',  engine='python', header=None).loc[:,0].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nivel de caracter, tenemos que fijarnos en una ventana de 6 o m치s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Jj3gCUEXhlGu"
   },
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.N = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase de N-gramas se quedar치 igual pues la estrategia ser치 solamente cambiar el tokenizador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "id": "2T9J65oaiILS"
   },
   "outputs": [],
   "source": [
    "class NgramData():\n",
    "    def __init__(self, N: int, vocab_max: int=5000, tokenizer=None, embedding_model=None):\n",
    "        self.tokenizer = tokenizer if tokenizer else self.default_tokenizer\n",
    "        self.punct = set(['.',',',';',':','-','^','춺','췉','\"','!','춰','?','','\\'','...','<url>','*','@usuario'])\n",
    "        self.N = N\n",
    "        self.vocab_max = vocab_max\n",
    "        self.UNK = '<unk>'\n",
    "        self.SOS = '<s>'\n",
    "        self.EOS = '</s>'\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "    def default_tokenizer(self, doc: str) -> list:\n",
    "        return doc.split(' ')\n",
    "\n",
    "    def get_vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def remove_word(self, word: str) -> bool:\n",
    "        word = word.lower()\n",
    "        is_punct = True if word in self.punct else False\n",
    "        is_digit = word.isnumeric()\n",
    "        return is_punct or is_digit\n",
    "\n",
    "    def get_vocab(self, corpus: list) -> set:\n",
    "        freq_dist = FreqDist([w.lower() for sent in corpus \\\n",
    "                              for w in self.tokenizer(sent) \\\n",
    "                              if not self.remove_word(w)])\n",
    "        sorted_words = self.sortFreqDict(freq_dist)[:self.vocab_max-3]\n",
    "        return set(sorted_words)\n",
    "\n",
    "    def sortFreqDict(self, freq_dist) -> list:\n",
    "        freq_dist = dict(freq_dist)\n",
    "        return sorted(freq_dist, key=freq_dist.get, reverse=True)\n",
    "\n",
    "    def fit(self, corpus: list) -> None:\n",
    "        self.vocab = self.get_vocab(corpus)\n",
    "        self.vocab.add(self.UNK)\n",
    "        self.vocab.add(self.SOS)\n",
    "        self.vocab.add(self.EOS)\n",
    "\n",
    "        self.w2id = {}\n",
    "        self.id2w = {}\n",
    "\n",
    "        if self.embedding_model is not None:\n",
    "            self.embedding_matrix = np.empty([len(self.vocab), self.embedding_model.vector_size])\n",
    "\n",
    "        ID = 0\n",
    "        for doc in corpus:\n",
    "            for word in self.tokenizer(doc):\n",
    "                word_ = word.lower()\n",
    "                if word_ in self.vocab and not word_ in self.w2id:\n",
    "                    self.w2id[word_] = ID\n",
    "                    self.id2w[ID] = word_\n",
    "                    if self.embedding_model is not None:\n",
    "                        if word_ in self.embedding_model.emb_dict:\n",
    "                            self.embedding_matrix[ID] = self.embedding_model.emb_dict[word_]\n",
    "                        else:\n",
    "                            self.embedding_matrix[ID] = np.random.rand(self.embedding_model.vector_size) \n",
    "                    ID += 1\n",
    "        #Special tokens  \n",
    "        self.w2id.update({self.UNK: ID, \n",
    "                          self.SOS: ID+1,\n",
    "                          self.EOS: ID+2})  \n",
    "        self.id2w.update({ID  : self.UNK, \n",
    "                          ID+1: self.SOS,\n",
    "                          ID+2: self.EOS})\n",
    "    \n",
    "    def replace_unk(self, doc_tokens: list) -> list: \n",
    "        for i, token in enumerate(doc_tokens):\n",
    "            if token.lower() not in self.vocab:\n",
    "                doc_tokens[i] = self.UNK\n",
    "        return doc_tokens\n",
    "\n",
    "\n",
    "    def get_ngram_doc(self, doc:str) -> list:\n",
    "        doc_tokens = self.tokenizer(doc)\n",
    "        doc_tokens = self.replace_unk(doc_tokens)\n",
    "        doc_tokens = [w.lower() for w in doc_tokens]\n",
    "        doc_tokens = [self.SOS]*(self.N - 1) + doc_tokens + [self.EOS]\n",
    "        return list(ngrams(doc_tokens, self.N))\n",
    "    \n",
    "    def transform(self, corpus: list) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        X_ngrams = []\n",
    "        y = []\n",
    "        for doc in corpus:\n",
    "            doc_ngram = self.get_ngram_doc(doc)\n",
    "            for words_window in doc_ngram:\n",
    "                words_window_ids = [self.w2id[w] for w in words_window]\n",
    "                X_ngrams.append(list(words_window_ids[:-1]))\n",
    "                y.append(words_window_ids[-1])\n",
    "        return np.array(X_ngrams), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el nuevo tokenizador a nivel de caracter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "igtUbF-UtA5e"
   },
   "outputs": [],
   "source": [
    "def CharTokenizer(doc: str) -> list:\n",
    "    l = []\n",
    "    for c in doc:\n",
    "        l.append(c)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos nivel de caracteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_level = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ukx1t0n8h12F"
   },
   "outputs": [],
   "source": [
    "if char_level:\n",
    "    tk = CharTokenizer\n",
    "else:\n",
    "    tk = TweetTokenizer()\n",
    "    tk = tk.tokenize\n",
    "    \n",
    "ngram_data = NgramData(args.N, 5000, tk)\n",
    "ngram_data.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OH2bHAWHqJ7w",
    "outputId": "1bd5b974-1b17-494c-8e8f-346fbc98c220"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 344\n"
     ]
    }
   ],
   "source": [
    "print('Vocab Size:', ngram_data.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos los datos transformados y los loader de los mismos para entrenamiento y validaci칩n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Hl5NY_wFqZPt"
   },
   "outputs": [],
   "source": [
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2PH5JSUbwX6O"
   },
   "outputs": [],
   "source": [
    "#Batch size\n",
    "args.batch_size = 64\n",
    "#Number workers\n",
    "args.num_workers = 2\n",
    "\n",
    "#Train\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = True)\n",
    "\n",
    "#Validation\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CxrtxytHyIAR",
    "outputId": "baebf0d3-f72b-4e23-e202-0fb96ad0d2ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([64, 5])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print('X shape:', batch[0].shape)\n",
    "print('y shape:', batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "l_diObstybV6"
   },
   "outputs": [],
   "source": [
    "#[[ngram_data.id2w[w] for w in tw] for tw in batch[0].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase del modelo neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "hXQa9kP8VMFU"
   },
   "outputs": [],
   "source": [
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(NeuralLM, self).__init__()\n",
    "\n",
    "        self.window_size = args.N - 1\n",
    "        self.embedding_size = args.d\n",
    "\n",
    "        self.emb = nn.Embedding(args.vocab_size, args.d)\n",
    "        self.fc1 = nn.Linear(args.d * (args.N - 1), args.d_h)\n",
    "        self.drop1 = nn.Dropout(p = args.dropout)\n",
    "        self.fc2 = nn.Linear(args.d_h, args.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1, self.window_size * self.embedding_size)\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = self.drop1(h)\n",
    "        x = self.fc2(h)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para el entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0vZZwQA1XZ8_"
   },
   "outputs": [],
   "source": [
    "def get_preds(raw_logits):\n",
    "    probs = F.softmax(raw_logits.detach(), dim=1)\n",
    "    y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    return y_pred\n",
    "\n",
    "def model_eval(data, model, gpu=False):\n",
    "    with torch.no_grad():\n",
    "        preds, tgts = [], []\n",
    "        for window_words, labels in data:\n",
    "            if gpu:\n",
    "                window_words = window_words.cuda()\n",
    "            outputs = model(window_words)\n",
    "\n",
    "            #Predictions\n",
    "            y_pred = get_preds(outputs)\n",
    "            tgt = labels.numpy()\n",
    "            tgts.append(tgt)\n",
    "            preds.append(y_pred)\n",
    "    tgts = [e for l in tgts for e in l]\n",
    "    preds = [e for l in preds for e in l]\n",
    "    return accuracy_score(tgts, preds)\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint_path, filename='checkpoint.pt'):\n",
    "    filename = os.path.join(checkpoint_path, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, os.path.join(checkpoint_path, 'model_best.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos hiperpar치metros que tendr치n que ser modificados posteriormente al usar un nuevo embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Dnk0A8MmXzRr"
   },
   "outputs": [],
   "source": [
    "#Model hyperparameters\n",
    "#Vocabulary size\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "#Word embeddings dimension\n",
    "args.d = 100\n",
    "#Hidden layer dimension\n",
    "args.d_h = 200\n",
    "#Dropout\n",
    "args.dropout = 0.1\n",
    "\n",
    "#Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "#Scheduler hyperparameters\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "#Saving hyperparameters\n",
    "args.savedir = pth + 'model'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "#Create model\n",
    "model = NeuralLM(args=args)\n",
    "\n",
    "#Send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "#Loss, Optimizer and Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer, \n",
    "                                                       mode = 'min',\n",
    "                                                       factor = args.lr_factor,\n",
    "                                                       patience = args.lr_patience,\n",
    "                                                       verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa de entrenamiento del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 902
    },
    "id": "m5w33lulaO27",
    "outputId": "86e4a066-cdcd-4533-caad-4a110bf51600"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.4386065422287932\n",
      "Epoch [1/100]: Loss = 1.9773, Val Acurracy = 0.4386, Epoch time = 31.76\n",
      "Train accuracy:  0.46004435409351324\n",
      "Epoch [2/100]: Loss = 1.8176, Val Acurracy = 0.4600, Epoch time = 30.50\n",
      "Train accuracy:  0.4607835889854001\n",
      "Epoch [3/100]: Loss = 1.7673, Val Acurracy = 0.4608, Epoch time = 30.55\n",
      "Train accuracy:  0.4825540565514692\n",
      "Epoch [4/100]: Loss = 1.7373, Val Acurracy = 0.4826, Epoch time = 30.89\n",
      "Train accuracy:  0.4729624838292367\n",
      "Epoch [5/100]: Loss = 1.7156, Val Acurracy = 0.4730, Epoch time = 31.02\n",
      "Train accuracy:  0.4506006283496581\n",
      "Epoch [6/100]: Loss = 1.7002, Val Acurracy = 0.4506, Epoch time = 30.81\n",
      "Train accuracy:  0.47444095361301053\n",
      "Epoch [7/100]: Loss = 1.6873, Val Acurracy = 0.4744, Epoch time = 30.63\n",
      "Train accuracy:  0.4743300683792275\n",
      "Epoch [8/100]: Loss = 1.6764, Val Acurracy = 0.4743, Epoch time = 30.30\n",
      "Train accuracy:  0.4886712252818333\n",
      "Epoch [9/100]: Loss = 1.6685, Val Acurracy = 0.4887, Epoch time = 32.59\n",
      "Train accuracy:  0.4830900018480872\n",
      "Epoch [10/100]: Loss = 1.6614, Val Acurracy = 0.4831, Epoch time = 33.33\n",
      "Train accuracy:  0.48443910552578084\n",
      "Epoch [11/100]: Loss = 1.6551, Val Acurracy = 0.4844, Epoch time = 32.53\n",
      "Epoch    12: reducing learning rate of group 0 to 1.1500e-01.\n",
      "Train accuracy:  0.49240436148586214\n",
      "Epoch [12/100]: Loss = 1.6499, Val Acurracy = 0.4924, Epoch time = 32.74\n",
      "Train accuracy:  0.5046386989465903\n",
      "Epoch [13/100]: Loss = 1.6107, Val Acurracy = 0.5046, Epoch time = 32.69\n",
      "Train accuracy:  0.5072445019404916\n",
      "Epoch [14/100]: Loss = 1.6055, Val Acurracy = 0.5072, Epoch time = 33.59\n",
      "Train accuracy:  0.5064683053040103\n",
      "Epoch [15/100]: Loss = 1.6023, Val Acurracy = 0.5065, Epoch time = 33.00\n",
      "Train accuracy:  0.5022916281648494\n",
      "Epoch [16/100]: Loss = 1.6014, Val Acurracy = 0.5023, Epoch time = 31.56\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-e7244224207f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtraining_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "#Training\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model.train()\n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        #Forward pass\n",
    "        outputs = model(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "\n",
    "        #Get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "\n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #Metrics in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Metrics in validation dataset\n",
    "    model.eval()\n",
    "    tuning_metric = model_eval(val_loader, model, gpu=args.use_gpu)\n",
    "    mean_epoch_metric = np.mean(tuning_metric)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "\n",
    "    #Check metric improvement\n",
    "    is_improve = tuning_metric > best_metric\n",
    "    if is_improve:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "\n",
    "    #Save best model\n",
    "    save_checkpoint({'epoch'       : epoch+1,\n",
    "                     'state_dict'  : model.state_dict(),\n",
    "                     'optimizer'   : optimizer.state_dict(),\n",
    "                     'scheduler'   : scheduler.state_dict(),\n",
    "                     'best_metric' : best_metric}, is_improve, args.savedir)\n",
    "    \n",
    "    #Early stoping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print('No improvement. Breaking out of loop.')\n",
    "        break\n",
    "    print('Train accuracy: ', mean_epoch_metric)\n",
    "    print('Epoch [{}/{}]: Loss = {:.4f}, Val Acurracy = {:.4f}, Epoch time = {:.2f}'.\n",
    "          format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time()-epoch_start_time)))\n",
    "    \n",
    "print('---------- %s seconds ---------' % time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejor modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(344, 100)\n",
       "  (fc1): Linear(in_features=500, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=344, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model with learned embeddings\n",
    "best_model = NeuralLM(args)\n",
    "best_model.load_state_dict(torch.load(pth+'model/model_best.pt')['state_dict'])\n",
    "best_model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Generamos texto 3 veces con m치ximo de 300 caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "gziHZsIQv0AO"
   },
   "outputs": [],
   "source": [
    "lenght_max = 300\n",
    "\n",
    "def parse_text(text, tokenizer):\n",
    "    all_tokens = [w.lower() if w in ngram_data.w2id else '<unk>' for w in tokenizer(text)]\n",
    "    token_ids = [ngram_data.w2id[word.lower()] for word in all_tokens]\n",
    "    return all_tokens, token_ids\n",
    "\n",
    "def sample_next_word(logits, temperature=1.):\n",
    "    logits = np.asarray(logits).astype('float64')\n",
    "    preds = logits/temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probs = np.random.multinomial(1, preds)\n",
    "    return np.argmax(probs)\n",
    "\n",
    "def predict_next_token(model, token_ids):\n",
    "    words_ids_tensor = torch.LongTensor(token_ids).unsqueeze(0)\n",
    "    y_raw_pred = model(words_ids_tensor).squeeze(0).detach().numpy()\n",
    "    y_pred = sample_next_word(y_raw_pred, 1.)\n",
    "    return y_pred\n",
    "\n",
    "def generate_sentence(model, initial_text, tokenizer):\n",
    "    all_tokens, window_word_ids = parse_text(initial_text, tokenizer)\n",
    "    for i in range(lenght_max):\n",
    "        y_pred = predict_next_token(best_model, window_word_ids)\n",
    "        next_word = ngram_data.id2w[y_pred]\n",
    "        all_tokens.append(next_word)\n",
    "        if next_word == '</s>':\n",
    "            break\n",
    "        else:\n",
    "            window_word_ids.pop(0)\n",
    "            window_word_ids.append(y_pred)\n",
    "    if char_level:\n",
    "        return ''.join(all_tokens)\n",
    "    else:\n",
    "        return ' '.join(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "UbXJ642z--mB",
    "outputId": "8a33b6a5-6361-4948-dbcd-a528e9ca0708"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s><s putos perder a oierencanas anuncia un putos<unk> 游땕游낕游둣游낕</s>'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = '<s><s'\n",
    "print('Learned embeddings')\n",
    "print('춾'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'estoy que vale verga<unk></s>'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = 'estoy'\n",
    "print('Learned embeddings')\n",
    "print('춾'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yo opulidado el pubarmos cleto 游땓#pasibron wey que esputo es elevar con tantar tienen llegu칠 me dinerente nomar nada ti a habes <unk><unk> vienen como hdp sestrea</s>'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = 'yo op'\n",
    "print('Learned embeddings')\n",
    "print('춾'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que genera texto bueno, sin embargo, como es a nivel caracter, algunas palabras no tienen un significado sin embargo, el sentido se preserva. Puede que esto mejore al expandir la ventana a m치s de 6 caracteres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Escribimos 5 ejemplos de oraciones y medimos su verosimilitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "au6ek-AlDTwd"
   },
   "outputs": [],
   "source": [
    "def log_likelihood(model, text, ngram_model):\n",
    "    # Generate n-gram windows from input text and the respective label y\n",
    "    X, y = ngram_data.transform([text])\n",
    "    # Discard first two n-gram windows since they contain '<s>' tokens not necessary\n",
    "    X, y = X[2:], y[2:]\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "\n",
    "    logits = model(X).detach()\n",
    "    probs = F.softmax(logits, dim = 1).numpy()\n",
    "\n",
    "    return np.sum([np.log(probs[i][w]) for i, w in enumerate(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RHMEZCGENCL5",
    "outputId": "86aee787-4cca-45f8-8e2b-36e100eb48a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -58.841915\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model, \n",
    "                                        'La clase de lenguaje est치 muy padre', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jr4XFLiHNCOl",
    "outputId": "4f76dda4-d895-439b-832b-44a0e154bf01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -61.371037\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model,\n",
    "                                        'La clase de lenguaje est치 muy chida',\n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5c7tbaKNCRP",
    "outputId": "3f32376f-8c1f-449f-f8de-031191a56234"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -69.30573\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model,\n",
    "                                        'La clase de lenguaje est치 muy guay', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -96.00549\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model,\n",
    "                                        'La clase de procesamiento del lenguaje est치 muy padre', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -56.34829\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model,\n",
    "                                        'La clase de lenguaje est치 muy madre', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparamos que tienen sentido los resultados pues la palabra padre es m치s usada que chida y mucho m치s que guay., entre m치s palabras le pongamos menor ser치 su verosimilitud como en el ejemplo 4. Sin embargo, logra fallar en el 칰ltimo ejemplo cuando decimos que est치 muy madre. En el espa침ol normal no se usa, no obstante, al haber sido entrenado con tuits groseros, se obtiene que es m치s probable decir que est치 muy madre. Otro dato curioso, es que al cambiar la palabra 춺est치췉 por 춺esta췉, la verosimilitud disminuye, lo cual podr칤a ser 칰til para correcciones ortogr치ficas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4HQ2xLFNF8r"
   },
   "source": [
    "## 1.3. Estructuras morfol칩gicas correctas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJYUOuUKNCT0",
    "outputId": "4ffff7c7-d8b2-48a9-bb1f-a9e2bd7b4389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.246503 chingada\n",
      "-5.246503 chingada\n",
      "-14.173836 dachinga\n",
      "-14.173836 dachinga\n",
      "-15.5406885 dgachina\n",
      "--------------------------------------------------\n",
      "-66.48084 acihdnag\n",
      "-67.964966 aacgdhni\n",
      "-67.964966 aacgdhni\n",
      "-68.05988 caagdhni\n",
      "-68.05988 caagdhni\n"
     ]
    }
   ],
   "source": [
    "if char_level:\n",
    "    word_list = 'chingada'\n",
    "    perms = [''.join(perm) for perm in permutations(word_list)]\n",
    "else:\n",
    "    word_list = 'sino gano me voy a la chingada'.split(' ')\n",
    "    perms = [' '.join(perm) for perm in permutations(word_list)]\n",
    "#print(len(perms))\n",
    "\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[:5]:\n",
    "    print(p, t)\n",
    "print('-'*50)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[-5:]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que hay dos chingadas debido a que se obtiene una de otra solo intercambiando las a's y similarmente para las dem치s palabras. Los resultados son los esperados pues la m치s probable es chingada, luego dachinga que contiene la palabra chinga y despues dgachina que contiene la palabra china. Las menos probables ni se pueden leer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Perplejidad en validaci칩n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "BHbLjydkNCWU"
   },
   "outputs": [],
   "source": [
    "def perplexity(model, text, ngram_model):\n",
    "    # Generate n-gram windows from input text and the respective label y\n",
    "    X, y = ngram_data.transform([text])\n",
    "    # Discard first two n-gram windows since they contain '<s>' tokens not necessary\n",
    "    X, y = X[2:], y[2:]\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "\n",
    "    logits = model(X).detach()\n",
    "    probs = F.softmax(logits, dim = 1).numpy()\n",
    "    \n",
    "    ans = 1.\n",
    "    N = len(y)\n",
    "    print('Validation set dimension:', N)\n",
    "    probs = [(probs[i][w])**(1/N) for i, w in enumerate(y)]\n",
    "    for p in probs:\n",
    "        ans /= p\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "8wbHjIcdNCYS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set dimension: 615\n",
      "Validation set perplexity: 1.4799053402082503\n"
     ]
    }
   ],
   "source": [
    "print('Validation set perplexity:', perplexity(best_model,\n",
    "                                               X_val, \n",
    "                                               ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Modelo de lenguaje neuronal inicializado con embedding dado.\n",
    "Leemos el embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = ''\n",
    "emb_txt = pd.read_csv(pth+'word2vec_col.txt',\n",
    "                        sep='\\r\\n', engine='python', \n",
    "                        header=None).loc[:,0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_d = int(emb_txt[0].split()[1])\n",
    "emb_N = int(emb_txt[0].split()[0])\n",
    "emb_txt = emb_txt[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el diccionario del embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict = {}\n",
    "for i in range(emb_N):\n",
    "    row_list = emb_txt[i].split()\n",
    "    emb_dict[row_list[0]] = np.array(row_list[1:]).astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase del embedding nuevo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "class embedding():\n",
    "    def __init__(self, emb_N, emb_dict):\n",
    "        self.emb_dict = emb_dict\n",
    "        self.embedding_matrix = np.empty([len(emb_dict), emb_N])\n",
    "        self.vector_size = emb_N\n",
    "        for i, word in enumerate(emb_dict):\n",
    "            self.embedding_matrix[i,:] = emb_dict[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usaremos nivel de palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_level = False\n",
    "args.N = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el nuevo ngram_data con el embedding preinicializado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 5000\n"
     ]
    }
   ],
   "source": [
    "if char_level:\n",
    "    tk = CharTokenizer\n",
    "else:\n",
    "    tk = TweetTokenizer()\n",
    "    tk = tk.tokenize\n",
    "\n",
    "emb_model = embedding(emb_d, emb_dict)\n",
    "ngram_data = NgramData(args.N, 5000, tk, emb_model)\n",
    "ngram_data.fit(X_train)\n",
    "print('Vocab Size:', ngram_data.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a transformar los datos crear los loaders: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size\n",
    "args.batch_size = 64\n",
    "#Number workers\n",
    "args.num_workers = 2\n",
    "\n",
    "#Train\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = True)\n",
    "\n",
    "#Validation\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([64, 3])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print('X shape:', batch[0].shape)\n",
    "print('y shape:', batch[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiamos algunos hiperpar치metros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model hyperparameters\n",
    "#Vocabulary size\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "#Word embeddings dimension\n",
    "args.d = emb_d\n",
    "#Hidden layer dimension\n",
    "args.d_h = 200 #Est치 bien puestp que emb_d es 100\n",
    "#Dropout\n",
    "args.dropout = 0.1\n",
    "\n",
    "#Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "#Scheduler hyperparameters\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "#Saving hyperparameters\n",
    "args.savedir = pth + 'model_emb'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "#Create model\n",
    "model_emb = NeuralLM(args=args)\n",
    "\n",
    "#Send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model_emb.cuda()\n",
    "\n",
    "#Loss, Optimizer and Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_emb.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer, \n",
    "                                                       mode = 'min',\n",
    "                                                       factor = args.lr_factor,\n",
    "                                                       patience = args.lr_patience,\n",
    "                                                       verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa de entrenamiento del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.21567279601306066\n",
      "Epoch [1/100]: Loss = 5.5244, Val Acurracy = 0.2157, Epoch time = 29.42\n",
      "Train accuracy:  0.2209142464340952\n",
      "Epoch [2/100]: Loss = 5.0727, Val Acurracy = 0.2209, Epoch time = 29.34\n",
      "Train accuracy:  0.22555421893796185\n",
      "Epoch [3/100]: Loss = 4.8611, Val Acurracy = 0.2256, Epoch time = 29.37\n",
      "Train accuracy:  0.18654407973878673\n",
      "Epoch [4/100]: Loss = 4.6914, Val Acurracy = 0.1865, Epoch time = 28.97\n",
      "Train accuracy:  0.21979721601649768\n",
      "Epoch [5/100]: Loss = 4.5472, Val Acurracy = 0.2198, Epoch time = 30.31\n",
      "Train accuracy:  0.21214985392679153\n",
      "Epoch [6/100]: Loss = 4.4175, Val Acurracy = 0.2121, Epoch time = 29.64\n",
      "Train accuracy:  0.19041072349200894\n",
      "Epoch [7/100]: Loss = 4.2921, Val Acurracy = 0.1904, Epoch time = 29.55\n",
      "Train accuracy:  0.21747722976456435\n",
      "Epoch [8/100]: Loss = 4.1725, Val Acurracy = 0.2175, Epoch time = 29.97\n",
      "Train accuracy:  0.21146245059288538\n",
      "Epoch [9/100]: Loss = 4.0719, Val Acurracy = 0.2115, Epoch time = 30.27\n",
      "Train accuracy:  0.16772641347310535\n",
      "Epoch [10/100]: Loss = 3.9669, Val Acurracy = 0.1677, Epoch time = 29.11\n",
      "Train accuracy:  0.1788967176490806\n",
      "Epoch [11/100]: Loss = 3.8664, Val Acurracy = 0.1789, Epoch time = 29.10\n",
      "Train accuracy:  0.16282866471902388\n",
      "Epoch [12/100]: Loss = 3.7822, Val Acurracy = 0.1628, Epoch time = 28.96\n",
      "Train accuracy:  0.1660938305550782\n",
      "Epoch [13/100]: Loss = 3.6934, Val Acurracy = 0.1661, Epoch time = 28.96\n",
      "Train accuracy:  0.21816463309847053\n",
      "Epoch [14/100]: Loss = 3.6161, Val Acurracy = 0.2182, Epoch time = 29.04\n",
      "Train accuracy:  0.1588760955490634\n",
      "Epoch [15/100]: Loss = 3.5516, Val Acurracy = 0.1589, Epoch time = 28.91\n",
      "Train accuracy:  0.18912184224093487\n",
      "Epoch [16/100]: Loss = 3.4916, Val Acurracy = 0.1891, Epoch time = 28.82\n",
      "Train accuracy:  0.19006702182505586\n",
      "Epoch [17/100]: Loss = 3.4196, Val Acurracy = 0.1901, Epoch time = 29.29\n",
      "Train accuracy:  0.2096580168413817\n",
      "Epoch [18/100]: Loss = 3.3747, Val Acurracy = 0.2097, Epoch time = 29.17\n",
      "Train accuracy:  0.15672796013060664\n",
      "Epoch [19/100]: Loss = 3.3261, Val Acurracy = 0.1567, Epoch time = 28.84\n",
      "Train accuracy:  0.1994328922495274\n",
      "Epoch [20/100]: Loss = 3.2785, Val Acurracy = 0.1994, Epoch time = 29.19\n",
      "Train accuracy:  0.17580340264650285\n",
      "Epoch [21/100]: Loss = 3.2324, Val Acurracy = 0.1758, Epoch time = 28.82\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-324-601dcf5586ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "#Training\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model_emb.train()\n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        #Forward pass\n",
    "        outputs = model_emb(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "\n",
    "        #Get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "\n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #Metrics in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Metrics in validation dataset\n",
    "    model_emb.eval()\n",
    "    tuning_metric = model_eval(val_loader, model_emb, gpu=args.use_gpu)\n",
    "    mean_epoch_metric = np.mean(tuning_metric)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "\n",
    "    #Check metric improvement\n",
    "    is_improve = tuning_metric > best_metric\n",
    "    if is_improve:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "\n",
    "    #Save best model\n",
    "    save_checkpoint({'epoch'       : epoch+1,\n",
    "                     'state_dict'  : model_emb.state_dict(),\n",
    "                     'optimizer'   : optimizer.state_dict(),\n",
    "                     'scheduler'   : scheduler.state_dict(),\n",
    "                     'best_metric' : best_metric}, is_improve, args.savedir)\n",
    "    \n",
    "    #Early stoping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print('No improvement. Breaking out of loop.')\n",
    "        break\n",
    "    print('Train accuracy: ', mean_epoch_metric)\n",
    "    print('Epoch [{}/{}]: Loss = {:.4f}, Val Acurracy = {:.4f}, Epoch time = {:.2f}'.\n",
    "          format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time()-epoch_start_time)))\n",
    "    \n",
    "print('---------- %s seconds ---------' % time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(5000, 100)\n",
       "  (fc1): Linear(in_features=300, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=5000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model with learned embeddings\n",
    "best_model = NeuralLM(args)\n",
    "best_model.load_state_dict(torch.load(pth+'model_emb/model_best.pt')['state_dict'])\n",
    "best_model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Palabras m치s similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_words(embeddings, ngram_data, word, n):\n",
    "    word_id = torch.LongTensor([[ngram_data.w2id[word]]])\n",
    "    word_embed = embeddings(word_id).view(-1).numpy()\n",
    "    aux = torch.tensor(embeddings.embedding_matrix - word_embed)\n",
    "    dists = torch.norm(aux, dim=1).detach()\n",
    "    lst = sorted(enumerate(dists.numpy()), key=lambda x : x[1])\n",
    "    for idx, diff in lst[1:n+1]:\n",
    "        print(embeddings.id2w[idx], diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_words(embeddings, ngram_data, word, n):\n",
    "    word_id = torch.LongTensor([ngram_data.w2id[word]])\n",
    "    word_embed = embeddings(word_id)\n",
    "    dists = torch.norm(embeddings.weight - word_embed, dim=1).detach()\n",
    "    lst = sorted(enumerate(dists.numpy()), key=lambda x : x[1])\n",
    "    for idx, diff in lst[1:n+1]:\n",
    "        print(ngram_data.id2w[idx], diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾\n",
      "estuviera 11.158217\n",
      "falta 11.536218\n",
      "gol 11.661474\n",
      "游꼕 11.697962\n",
      "plata 11.7053995\n",
      "escribir 11.808645\n",
      "游 11.84848\n",
      "chacal 11.848602\n",
      "peleas 11.894655\n",
      "aquello 11.899229\n"
     ]
    }
   ],
   "source": [
    "print('Learned embeddings')\n",
    "print('춾'*20)\n",
    "print_closest_words(best_model.emb, ngram_data, 'lugar', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾\n",
      "acerca 10.808623\n",
      "happy 10.836503\n",
      "regalar 10.970969\n",
      "4to 10.990195\n",
      "venir 11.020816\n",
      "jajajajajjaja 11.024045\n",
      "mejoran 11.116126\n",
      "puros 11.134831\n",
      "tragan 11.146174\n",
      "jajajajajajajaja 11.151438\n"
     ]
    }
   ],
   "source": [
    "print('Learned embeddings')\n",
    "print('춾'*20)\n",
    "print_closest_words(best_model.emb, ngram_data, 'madre', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾\n",
      "<unk> 10.303227\n",
      "quedan 10.429716\n",
      "primer 10.556313\n",
      "bonitos 10.570142\n",
      "mamo 10.769064\n",
      "periodistas 10.787671\n",
      "chingar 10.793026\n",
      "ten칤an 10.830661\n",
      "gallardo 10.853344\n",
      "grupo 10.94077\n"
     ]
    }
   ],
   "source": [
    "print('Learned embeddings')\n",
    "print('춾'*20)\n",
    "print_closest_words(best_model.emb, ngram_data, 'chingada', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que en algunas palabras es buena la relaci칩n como chingada-chingar-mamo, lugar-estuviera-falta-gol en 치mbitos de fuera de lugar en el futbol. Sin embargo en madre no existen muy buenas realiciones a menos que sean los jaja's pues ambos aparecen relacionados, tal vez porque un insulto con madre siempre acabara en risas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Generaci칩n de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> <s> <s> llevo estos de mil putas <unk> bloquee se oye <unk> <unk> dolor chingona programas cosa sue침os <unk> <unk> para momentos estar칤a verga dime te digan <unk> para ver si no quiere  </s>'"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = '<s> <s> <s>'\n",
    "print('Learned embeddings')\n",
    "print('춾'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> que no saben entiendo ya valen verga pero 游땍 游땍 游땍 仇 ya le traigo de la <unk> <unk> aqui de cagada <unk> <unk> </s>'"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = '<s> que no'\n",
    "print('Learned embeddings')\n",
    "print('춾'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned embeddings\n",
      "춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾춾\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> que pedo con el mismo <unk> <unk> enfermos que poca madre tienen del mamar <unk> </s>'"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tokens = '<s> que pedo'\n",
    "print('Learned embeddings')\n",
    "print('춾'*20)\n",
    "generate_sentence(best_model, initial_tokens, tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que genera buena secuencia de palabras con la ventaja de que las palabras existen (ventaja sobre el modelo por caracteres). algunas tienen sentido, sin embargo, en muchas ocasiones se pierde el significado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. 5 ejemplos de oraciones y medimos su verosimilitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -28.252762\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model, \n",
    "                                        'La clase de lenguaje est치 muy padre', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -30.028719\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model, \n",
    "                                        'La clase de lenguaje esta muy padre', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -29.552502\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model, \n",
    "                                        'La clase de lenguaje est치 muy chida', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -13.44938\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model, \n",
    "                                        'Vamos para all치', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -16.710388\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood:', log_likelihood(best_model, \n",
    "                                        'Vamos para haya', \n",
    "                                        ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colocamos ejemplos en los que hay faltas de ortograf칤a para notar si en realidad puede servir para corregir ortogr치ficamente pues la parabra correcta es la que mejor deber칤a quedar en el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Estructura sint치cticas correctas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-14.370728 buscamos nosotros a una mujer\n",
      "-16.224253 nosotros buscamos a una mujer\n",
      "-19.5894 nosotros buscamos mujer a una\n",
      "-19.754845 buscamos una mujer a nosotros\n",
      "-20.257294 nosotros buscamos una mujer a\n",
      "--------------------------------------------------\n",
      "-33.993004 a una nosotros mujer buscamos\n",
      "-34.36071 una mujer nosotros buscamos a\n",
      "-35.04878 una mujer nosotros a buscamos\n",
      "-35.06093 a una nosotros buscamos mujer\n",
      "-35.988052 mujer una nosotros buscamos a\n"
     ]
    }
   ],
   "source": [
    "if char_level:\n",
    "    word_list = 'chingada'\n",
    "    perms = [''.join(perm) for perm in permutations(word_list)]\n",
    "else:\n",
    "    word_list = 'buscamos a una nosotros mujer'.split(' ')\n",
    "    perms = [' '.join(perm) for perm in permutations(word_list)]\n",
    "#print(len(perms))\n",
    "\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[:5]:\n",
    "    print(p, t)\n",
    "print('-'*50)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[-5:]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ejemplo propuesto fue puesto con la intenci칩n de que la palabra nosotros se relacionara con buscamos lo la conjugaci칩n y la palabra una con mujer. Lo cual fue satisfecho por el ordenamiento con verosimilitud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Perplejidad para los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, text, ngram_model):\n",
    "    # Generate n-gram windows from input text and the respective label y\n",
    "    X, y = ngram_data.transform(text)\n",
    "    # Discard first two n-gram windows since they contain '<s>' tokens not necessary\n",
    "    X, y = X[2:], y[2:]\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "\n",
    "    logits = model(X).detach()\n",
    "    probs = F.softmax(logits, dim = 1).numpy()\n",
    "    \n",
    "    ans = 1.\n",
    "    N = len(y)\n",
    "    print('Validation set dimension:', N)\n",
    "    probs = [(probs[i][w])**(1/N) for i, w in enumerate(y)]\n",
    "    for p in probs:\n",
    "        ans /= p\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set dimension: 11636\n",
      "Validation set perplexity: 114.71186832388392\n"
     ]
    }
   ],
   "source": [
    "print('Validation set perplexity:', perplexity(best_model,\n",
    "                                               X_val, \n",
    "                                               ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Comparamos con el modelo de clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 5000\n"
     ]
    }
   ],
   "source": [
    "if char_level:\n",
    "    tk = CharTokenizer\n",
    "else:\n",
    "    tk = TweetTokenizer()\n",
    "    tk = tk.tokenize\n",
    "\n",
    "emb_model = None\n",
    "ngram_data = NgramData(args.N, 5000, tk, emb_model)\n",
    "ngram_data.fit(X_train)\n",
    "print('Vocab Size:', ngram_data.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size\n",
    "args.batch_size = 64\n",
    "#Number workers\n",
    "args.num_workers = 2\n",
    "\n",
    "#Train\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = True)\n",
    "\n",
    "#Validation\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype=torch.int64),\n",
    "                 torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size = args.batch_size,\n",
    "                          num_workers = args.num_workers,\n",
    "                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([64, 3])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print('X shape:', batch[0].shape)\n",
    "print('y shape:', batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model hyperparameters\n",
    "#Vocabulary size\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "#Word embeddings dimension\n",
    "args.d = 100\n",
    "#Hidden layer dimension\n",
    "args.d_h = 200\n",
    "#Dropout\n",
    "args.dropout = 0.1\n",
    "\n",
    "#Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "#Scheduler hyperparameters\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "#Saving hyperparameters\n",
    "args.savedir = pth + 'model_normal'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "#Create model\n",
    "model_normal = NeuralLM(args=args)\n",
    "\n",
    "#Send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model_emb.cuda()\n",
    "\n",
    "#Loss, Optimizer and Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_normal.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer, \n",
    "                                                       mode = 'min',\n",
    "                                                       factor = args.lr_factor,\n",
    "                                                       patience = args.lr_patience,\n",
    "                                                       verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.217992782264994\n",
      "Epoch [1/100]: Loss = 5.5172, Val Acurracy = 0.2180, Epoch time = 30.00\n",
      "Train accuracy:  0.20940024059116688\n",
      "Epoch [2/100]: Loss = 5.0656, Val Acurracy = 0.2094, Epoch time = 29.30\n",
      "Train accuracy:  0.19994844474995704\n",
      "Epoch [3/100]: Loss = 4.8504, Val Acurracy = 0.1999, Epoch time = 29.76\n",
      "Train accuracy:  0.22082832101735694\n",
      "Epoch [4/100]: Loss = 4.6843, Val Acurracy = 0.2208, Epoch time = 28.87\n",
      "Train accuracy:  0.22787420518989518\n",
      "Epoch [5/100]: Loss = 4.5378, Val Acurracy = 0.2279, Epoch time = 29.15\n",
      "Train accuracy:  0.20012029558343358\n",
      "Epoch [6/100]: Loss = 4.4101, Val Acurracy = 0.2001, Epoch time = 28.82\n",
      "Train accuracy:  0.20115140058429284\n",
      "Epoch [7/100]: Loss = 4.2836, Val Acurracy = 0.2012, Epoch time = 29.18\n",
      "Train accuracy:  0.2123217047602681\n",
      "Epoch [8/100]: Loss = 4.1711, Val Acurracy = 0.2123, Epoch time = 28.73\n",
      "Train accuracy:  0.19719883141433237\n",
      "Epoch [9/100]: Loss = 4.0630, Val Acurracy = 0.1972, Epoch time = 28.66\n",
      "Train accuracy:  0.21129059975940884\n",
      "Epoch [10/100]: Loss = 3.9541, Val Acurracy = 0.2113, Epoch time = 29.37\n",
      "Train accuracy:  0.14890874720742395\n",
      "Epoch [11/100]: Loss = 3.8610, Val Acurracy = 0.1489, Epoch time = 29.99\n",
      "Train accuracy:  0.16703901013919917\n",
      "Epoch [12/100]: Loss = 3.7624, Val Acurracy = 0.1670, Epoch time = 29.15\n",
      "Train accuracy:  0.19316033682763362\n",
      "Epoch [13/100]: Loss = 3.6939, Val Acurracy = 0.1932, Epoch time = 29.43\n",
      "Train accuracy:  0.2236638597697199\n",
      "Epoch [14/100]: Loss = 3.6111, Val Acurracy = 0.2237, Epoch time = 28.98\n",
      "Train accuracy:  0.16849974222374978\n",
      "Epoch [15/100]: Loss = 3.5345, Val Acurracy = 0.1685, Epoch time = 28.74\n",
      "Train accuracy:  0.1666953084722461\n",
      "Epoch [16/100]: Loss = 3.4706, Val Acurracy = 0.1667, Epoch time = 28.76\n",
      "Train accuracy:  0.20261213266884345\n",
      "Epoch [17/100]: Loss = 3.4111, Val Acurracy = 0.2026, Epoch time = 29.70\n",
      "Train accuracy:  0.17271008764392506\n",
      "Epoch [18/100]: Loss = 3.3549, Val Acurracy = 0.1727, Epoch time = 29.71\n",
      "Train accuracy:  0.17563155181302628\n",
      "Epoch [19/100]: Loss = 3.3127, Val Acurracy = 0.1756, Epoch time = 29.06\n",
      "Train accuracy:  0.19384774016153977\n",
      "Epoch [20/100]: Loss = 3.2657, Val Acurracy = 0.1938, Epoch time = 28.72\n",
      "Train accuracy:  0.19427736724523115\n",
      "Epoch [21/100]: Loss = 3.2260, Val Acurracy = 0.1943, Epoch time = 28.69\n",
      "Epoch    22: reducing learning rate of group 0 to 1.1500e-01.\n",
      "Train accuracy:  0.20759580683966317\n",
      "Epoch [22/100]: Loss = 3.1797, Val Acurracy = 0.2076, Epoch time = 29.39\n",
      "Train accuracy:  0.19487884516239903\n",
      "Epoch [23/100]: Loss = 2.8759, Val Acurracy = 0.1949, Epoch time = 29.04\n",
      "Train accuracy:  0.2096580168413817\n",
      "Epoch [24/100]: Loss = 2.8265, Val Acurracy = 0.2097, Epoch time = 28.94\n",
      "No improvement. Breaking out of loop.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-394-20e06e13afa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m           format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time()-epoch_start_time)))\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'---------- %s seconds ---------'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'float'"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "#Training\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model_normal.train()\n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        #Forward pass\n",
    "        outputs = model_normal(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "\n",
    "        #Get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "\n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #Metrics in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Metrics in validation dataset\n",
    "    model_normal.eval()\n",
    "    tuning_metric = model_eval(val_loader, model_normal, gpu=args.use_gpu)\n",
    "    mean_epoch_metric = np.mean(tuning_metric)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    #Update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "\n",
    "    #Check metric improvement\n",
    "    is_improve = tuning_metric > best_metric\n",
    "    if is_improve:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "\n",
    "    #Save best model\n",
    "    save_checkpoint({'epoch'       : epoch+1,\n",
    "                     'state_dict'  : model_normal.state_dict(),\n",
    "                     'optimizer'   : optimizer.state_dict(),\n",
    "                     'scheduler'   : scheduler.state_dict(),\n",
    "                     'best_metric' : best_metric}, is_improve, args.savedir)\n",
    "    \n",
    "    #Early stoping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print('No improvement. Breaking out of loop.')\n",
    "        break\n",
    "    print('Train accuracy: ', mean_epoch_metric)\n",
    "    print('Epoch [{}/{}]: Loss = {:.4f}, Val Acurracy = {:.4f}, Epoch time = {:.2f}'.\n",
    "          format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time()-epoch_start_time)))\n",
    "    \n",
    "print('---------- %s seconds ---------' % time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(5000, 100)\n",
       "  (fc1): Linear(in_features=300, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=5000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model with learned embeddings\n",
    "best_model = NeuralLM(args)\n",
    "best_model.load_state_dict(torch.load(pth+'model_normal/model_best.pt')['state_dict'])\n",
    "best_model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set dimension: 11636\n",
      "Validation set perplexity: 118.35818891012026\n"
     ]
    }
   ],
   "source": [
    "print('Validation set perplexity:', perplexity(best_model,\n",
    "                                               X_val, \n",
    "                                               ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
